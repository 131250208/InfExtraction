import json
import os
from InfExtraction.modules.preprocess import Preprocessor
from InfExtraction.modules.utils import (load_data, save_as_json_lines, merge_spans,
                                         WhiteWordTokenizer, ChineseWordTokenizer)
from InfExtraction.modules import utils
from InfExtraction.modules.metrics import MetricsCalculator
from tqdm import tqdm
import random
from tqdm import tqdm
import torch
from pprint import pprint
import copy
import re
import string
import itertools
import matplotlib.pyplot as plt
import time
import logging
import json
import copy
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import hashlib
import stanza


def convert_genia():
    data_in_dir = "../../data/ori_data/genia_bk"
    data_out_dir = "../../data/ori_data/genia"
    in_file2out_file = {
        "train_dev.genia.jsonlines": "train_data.json",
        "test.genia.jsonlines": "test_data.json",
    }

    # load data
    filename2data = {}
    for in_filename, out_filename in in_file2out_file.items():
        int_path = os.path.join(data_in_dir, in_filename)
        out_path = os.path.join(data_out_dir, out_filename)
        with open(int_path, "r", encoding="utf-8") as file_in:
            data = [json.loads(line) for line in file_in]
            out_data = []
            for batch in tqdm(data, desc="transforming"):
                ners = batch["ners"]
                sentences = batch["sentences"]
                for idx, words in enumerate(sentences):
                    text = " ".join(words)
                    tok2char_span = WhiteWordTokenizer.get_tok2char_span_map(words)
                    ent_list = []
                    for ent in ners[idx]:
                        ent_text = " ".join(words[ent[0]:ent[1] + 1])
                        char_span_list = tok2char_span[ent[0]:ent[1] + 1]
                        char_span = [char_span_list[0][0], char_span_list[-1][1]]
                        norm_ent = {"text": ent_text,
                                    "type": ent[2],
                                    "char_span": char_span}
                        assert ent_text == text[char_span[0]:char_span[1]]
                        ent_list.append(norm_ent)
                    sample = {
                        "text": text,
                        "word_list": words,
                        "entity_list": ent_list,
                    }
                    out_data.append(sample)

        json.dump(out_data, open(out_path, "w", encoding="utf-8"), ensure_ascii=False)


def clean_text(ent):
    ent = re.sub("ï¿½", "", ent)
    ent = ent.strip("\xad")
    return ent.strip()


def convert_daixiang_data(path, data_type=None):
    with open(path, "r", encoding="utf-8") as file_in:
        lines = [line.strip("\n") for line in file_in]
        data = []
        for i in range(0, len(lines), 3):
            sample = lines[i: i + 3]
            text = sample[0]
            word_list = text.split(" ")
            annstr = sample[1]
            ent_list = []
            word2char_span = WhiteWordTokenizer.get_tok2char_span_map(word_list)

            # entities
            for ann in annstr.split("|"):
                if ann == "":
                    continue
                offsets, ent_type = ann.split(" ")
                offsets = [int(idx) for idx in offsets.split(",")]
                assert len(offsets) % 2 == 0
                for idx, pos in enumerate(offsets):
                    if idx % 2 != 0:
                        offsets[idx] += 1

                extr_segs = []
                char_span = []
                tok_span = []
                for idx in range(0, len(offsets), 2):
                    wd_sp = [offsets[idx], offsets[idx + 1]]
                    ch_sp_list = word2char_span[wd_sp[0]:wd_sp[1]]
                    ch_sp = [ch_sp_list[0][0], ch_sp_list[-1][1]]

                    seg_wd = " ".join(word_list[wd_sp[0]: wd_sp[1]])
                    seg_ch = text[ch_sp[0]:ch_sp[1]]
                    assert seg_ch == seg_wd

                    char_span.extend(ch_sp)
                    tok_span.extend(wd_sp)
                    extr_segs.append(seg_ch)
                ent_txt_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(char_span, text, "en")
                ent_txt = " ".join(extr_segs)

                assert ent_txt == ent_txt_extr
                ent = {
                    "text": ent_txt,
                    "type": ent_type,
                    "char_span": char_span,
                    "tok_span": tok_span,
                }
                ent_list.append(ent)

            # merge continuous spans
            for ent in ent_list:
                ori_char_span = ent["char_span"]
                merged_span = merge_spans(ori_char_span)
                ent_ori_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(ori_char_span, text, "en")
                ent_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(merged_span, text, "en")
                ent["char_span"] = merged_span
                assert ent_ori_extr == ent_extr == ent["text"]

            new_sample = {
                "text": sample[0],
                "word_list": word_list,
                "word2char_span": word2char_span,
                "entity_list": ent_list,
            }
            if data_type is not None:
                new_sample["id"] = "{}_{}".format(data_type, len(data))

            data.append(new_sample)

    return data


def postprocess_duee():
    res_data_path = "../../data/res_data/test_data.json"
    out_path = "../../data/res_data/duee.json"
    test_data = load_data(res_data_path)
    test_data2submit = []
    for sample in test_data:
        sample2submit = {
            "id": sample["id"],
            "text": sample["text"],
            "event_list": []
        }
        for event in sample["event_list"]:
            event2submit = {
                "event_type": event["trigger_type"],
                "trigger": event["trigger"],
                "trigger_start_index": event["trigger_char_span"][0],
                "arguments": [],
            }
            for arg in event["argument_list"]:
                event2submit["arguments"].append({
                    "argument": arg["text"],
                    "role": arg["type"],
                    "argument_start_index": arg["char_span"][0],
                })
            sample2submit["event_list"].append(event2submit)
        test_data2submit.append(sample2submit)
    save_as_json_lines(test_data2submit, out_path)


def convert_oie4():
    data_in_dir = "../../data/ori_data/oie4_bk"

    train_filename = "openie4_labels"
    valid_filename = "dev.tsv"
    test_filename = "test.tsv"
    train_path = os.path.join(data_in_dir, train_filename)
    valid_path = os.path.join(data_in_dir, valid_filename)
    test_path = os.path.join(data_in_dir, test_filename)

    train_data = []
    with open(train_path, "r", encoding="utf-8") as file_in:
        text = None
        words = None
        tag_lines = []
        for line in tqdm(file_in, desc="loading data"):
            line = line.strip("\n")
            if re.search("ARG|REL|NONE", line) is not None:
                tag_lines.append(line.split(" "))
            else:
                if text is not None:
                    train_data.append({
                        "text": text,
                        "word_list": words,
                        "tag_lines": tag_lines
                    })
                    tag_lines = []
                text = line
                words = line.split(" ")
        if text is not None:
            train_data.append({
                "text": text,
                "word_list": words,
                "tag_lines": tag_lines
            })

    for sample in tqdm(train_data, desc="transforming data"):
        open_spo_list = []
        word_list = sample["word_list"]
        tok2char_span = WhiteWordTokenizer.get_tok2char_span_map(word_list)
        text = sample["text"]

        for tags in sample["tag_lines"]:
            for tag_id in range(-3, 0):
                if tags[tag_id] != "NONE":
                    assert tags[tag_id] == "REL"
                    tags[tag_id] = "ADD"
            type2indices = {}
            for idx, tag in enumerate(tags):
                if tag == "NONE":
                    continue
                if tag not in type2indices:
                    type2indices[tag] = []
                type2indices[tag].append(idx)

            spo = []
            for type_, ids in type2indices.items():
                wd_spans = []
                pre = -10
                for pos in ids:
                    if pos - 1 != pre:
                        wd_spans.append(pre + 1)
                        wd_spans.append(pos)
                    pre = pos
                wd_spans.append(pre + 1)
                wd_spans = wd_spans[1:]

                ch_spans = Preprocessor.tok_span2char_span(wd_spans, tok2char_span)
                extr_arg_text_tok = Preprocessor.extract_ent_fr_txt_by_tok_sp(wd_spans, tok2char_span, text, "en")
                extr_arg_text_ch = Preprocessor.extract_ent_fr_txt_by_char_sp(ch_spans, text, "en")

                assert extr_arg_text_ch == extr_arg_text_tok

                type_map = {
                    "REL": "predicate",
                    "ARG1": "subject",
                    "ARG2": "object",
                    # "ADD": "additional_info",
                    "TIME": "time",
                    "LOC": "location",
                }
                if type_ != "ADD":
                    spo.append({
                        "type": type_map[type_],
                        "text": extr_arg_text_tok,
                        "char_span": ch_spans,
                    })
                else:
                    add_info_map = {
                        "[unused1]": "be-none",
                        "[unused2]": "be-of",
                        "[unused3]": "be-from",
                    }
                    add_text = add_info_map[extr_arg_text_tok]

                    add_type = "predicate" if "REL" not in type2indices else "predicate_prefix"
                    spo.append({
                        "type": add_type,
                        "text": "be",
                        "char_span": [],
                    })

                    if add_text == "be-of":
                        spo.append({
                            "type": "predicate_suffix",
                            "text": "of",
                            "char_span": [],
                        })
                    elif add_text == "be-from":
                        spo.append({
                            "type": "predicate_suffix",
                            "text": "from",
                            "char_span": [],
                        })
            open_spo_list.append(spo)

        word_list = word_list[:-3]
        text = " ".join(word_list)
        sample["word2char_span"] = tok2char_span[:-3]
        sample["text"] = text
        sample["word_list"] = word_list
        sample["open_spo_list"] = open_spo_list
        for spo in open_spo_list:
            for arg in spo:
                if len(arg["char_span"]) == 0:
                    continue
                extr_arg_text_ch = Preprocessor.extract_ent_fr_txt_by_char_sp(arg["char_span"], text, "en")
                assert extr_arg_text_ch == arg["text"]
            # add [OBJ]
            # if any(arg["type"] == "object" for arg in spo):
            #     for arg in spo:
            #         if arg["type"] == "predicate" and len(arg["char_span"]) > 0:
            #             arg["text"] += " [OBJ]"
        del sample["tag_lines"]

    # valid and test
    def get_val_test_data(path):
        fix_map = {
            "the unique limestone limestone of the mountains": "the unique limestone of the mountains",
            "do n't": "don't",
            "did n't": "didn't",
        }

        with open(path, "r", encoding="utf-8") as file_in:
            lines = []
            for line in file_in:
                for key, val in fix_map.items():
                    line = re.sub(key, val, line)
                splits = line.strip("\n").split("\t")
                # puncs = re.escape(re.sub("\.", "", string.punctuation))
                new_splits = []
                # ç¬¦å·ä¸åè¯ç¨ç©ºæ ¼éå¼
                for sp in splits:
                    sp = re.sub("([A-Za-z]+|[0-9\.]+)", r" \1 ", sp)
                    sp = re.sub("\s+", " ", sp)
                    sp = re.sub("n ' t", "n 't", sp)
                    sp = re.sub("' s", "'s", sp)
                    sp = sp.strip()
                    new_splits.append(sp)
                lines.append(new_splits)

            text2anns = {}
            for line in lines:
                text = line[0]
                if text not in text2anns:
                    text2anns[text] = []
                spo = [
                    {"type": "predicate", "text": line[1]},
                    {"type": "subject", "text": line[2]}
                ]

                if len(line) >= 4:
                    if "C :" not in line[3]:
                        spo.append({"type": "object", "text": line[3]})
                if len(line) >= 5:
                    if "C :" not in line[4]:
                        arg = re.sub("T : |L : ", "", line[4])
                        spo.append({"type": "time/loc", "text": arg})
                if len(line) == 6:
                    if "C :" not in line[5]:
                        arg = re.sub("T : |L : ", "", line[5])
                        spo.append({"type": "time/loc", "text": arg})
                text2anns[text].append(spo)

        data = []
        for text, anns in text2anns.items():
            data.append({
                "text": text,
                "open_spo_list": anns,
            })
        return data

    valid_data = get_val_test_data(valid_path)
    test_data = get_val_test_data(test_path)

    return train_data, valid_data, test_data


def trans2dai_dataset():
    '''
    change our data format to daixiang data format
    :return:
    '''
    in_data_dir = "../../data/normal_data/share_14_uncbase"
    out_data_dir = "../../data/ori_data/share_14_uncbase"
    if not os.path.exists(out_data_dir):
        os.makedirs(out_data_dir)

    test_data_path = os.path.join(in_data_dir, "test_data.json")
    train_data_path = os.path.join(in_data_dir, "train_data.json")
    valid_data_path = os.path.join(in_data_dir, "valid_data.json")
    test_out_path = os.path.join(out_data_dir, "test.txt")
    valid_out_path = os.path.join(out_data_dir, "dev.txt")
    train_out_path = os.path.join(out_data_dir, "train.txt")

    def trans2daixiang_subwd(in_path, out_path):
        data = load_data(in_path)
        with open(out_path, "w", encoding="utf-8") as out_file:
            for sample in data:
                ent_list = []
                for ent in sample["entity_list"]:
                    ent_subwd_sp = [str(pos) if idx % 2 == 0 else str(pos - 1) for idx, pos in
                                    enumerate(ent["wd_span"])]
                    ent_list.append(",".join(ent_subwd_sp) + " " + ent["type"])
                text = sample["text"]
                ann_line = "|".join(ent_list)
                out_file.write("{}\n".format(text))
                out_file.write("{}\n".format(ann_line))
                out_file.write("\n")

    trans2daixiang_subwd(test_data_path, test_out_path)
    trans2daixiang_subwd(train_data_path, train_out_path)
    trans2daixiang_subwd(valid_data_path, valid_out_path)


def preprocess_saoke(data_path="../../data/ori_data/saoke_bk/saoke.json"):
    data = load_data(data_path)
    # fix data
    pred_fix_map = {
        "åæ¾X57820å": "åæ¾X157820å",
        'å°ç ç©¶é¢åççéç¹æ¾å¨': 'å°ç ç©¶é¢åçéç¹æ¾å¨',
        "NOTæXç¨å¨å¦ä½è¸¢çä¸": "éæXç¨å¨å¦ä½è¸¢çä¸",
        "è¢«Xè£ä¸ºâ ä¸çåªåY âçæ¹æ³": "è¢«Xè£ä¸ºâä¸çåªåYâçæ¹æ³",
        "å³å°å¨Xä¸¾è¡ä¸¾è¡": "å³å°å¨Xä¸¾è¡",
        "å¦é³åè¬æçæç": "å¦é³åè¬æç",
        "ä¸æ­ä¸æ­é¢ç¥": "ä¸æ­é¢ç¥",
        "æäºXè¯ºè´å°åå¹³å¥æ": "æäºXè¯ºè´å°åå¹³å¥",
        "åºå®ä½¿ç¨ä½¿ç¨": "åºå®ä½¿ç¨",
        "å¬è§Xå¬è§": "å¬è§",
        "æ¸éæ§æ§è½": "æ¸éæ§è½",
        "å°A/Væ°æ®ä¼ è¾ä¼ è¾ç»": "å°A/Væ°æ®ä¼ è¾ç»",
        'èå¥èå¥': 'èå¥',
        'å æ»æ»é¢ç§¯': 'å æ»é¢ç§¯',
        "ä¸»è¦ä»¥Xä¸»è¦ä»¥": "ä¸»è¦ä»¥",
        'æ ¹ç¶èè': 'æ ¹ç¶è',
        "å®ä¸ºä¸º": "å®ä¸º",
        'ç¬¬ä¸ç¬é£é©æèµèµé': 'ç¬¬ä¸ç¬é£é©æèµé',
        'æ éXç§ç': 'æ é¡»Xç§ç',
        "[è¸ååè¿ç­]åç­é¢": "[è¸å|è¿ç­]åç­é¢",
        "[é«ç¨³äº§åç°å°é¢ç§¯|äººåé«ç¨³äº§åç°å°é¢ç§¯": "[é«ç¨³äº§åç°å°é¢ç§¯|äººåé«ç¨³äº§åç°å°é¢ç§¯]",
        "[åå¼±ORæ¶å¤±]": "[åå¼±|æ¶å¤±]",
        "æ´ä½[è§å|è¥å»º|": "æ´ä½[è§å|è¥å»º]",
        "äºåå¥èµ°|": "äºåå¥èµ°",
        "æè®¢|ç»ç»å®æ½]": "[æè®¢|ç»ç»å®æ½]",
        "è§å®|": "è§å®",
        "ä»¥âè£æ°å¤§å®ç¥ å¾â|": "ä»¥âè£æ°å¤§å®ç¥ å¾â",
        "å¥å¨||": "å¥å¨",
        "è½¬ç§»|å¹è®­": "[è½¬ç§»|å¹è®­]",
        "è¢«å½å¡é¢ç¡®ç¡®å®ä¸º": "è¢«å½å¡é¢ç¡®å®ä¸º",
        "èä¼|": "èä¼",
        "ç±Xç±": "ç±Xåºç",
        "ä»¥Xä»¥": "ä»¥Xä¸ºç®æ ",
        "å¯ä»¥åXå¯ä»¥å": "å¯ä»¥åX",
        "å¯ä½¿Xä½ä¸|": "å¯ä½¿Xä½ä¸",
        "ä¸ç´è´åäºä¸ºXæä¾æå¡]": "ä¸ç´è´åäºä¸ºXæä¾[è®¾æ½|æå¡]",
        "ä»¥è¾¾å°çææXçææ": "ä»¥è¾¾å°Xçææ",
        "ä¸ºX[åå¦ãæ®æ]": "ä¸ºX[åå¦|æ®æ]",
        "æå©Xå¿«é[æå»ºè°æ´]Y": "æå©Xå¿«é[æå»º|è°æ´]Y",
        "ä¾æ³çç£æ£æ¥Xè´¯å½»æ§è¡å®å¨çäº§æ³è§æåµ|è®¾å¤è®¾æ½å®å¨æåµ]": "ä¾æ³çç£æ£æ¥Xè´¯å½»æ§è¡[å®å¨çäº§[æ³å¾|æ³è§]æåµ|å®å¨çäº§æ¡ä»¶|è®¾å¤è®¾æ½å®å¨æåµ]",
        "è´åäºå¸®å©Xè§£å³æè²ãèä¸ç­ç­å·¥ä½æ¹æ¹é¢é¢æéå°çé¾é¢]": "è´åäºå¸®å©Xè§£å³[è¡£|é£|ä½|è¡|å¨±ä¹|ææ|æè²|èä¸]çé¾é¢",
        "ä¸ºXæä¾ä¸ä¸ªçæ§éæ©çå¹³å°|": "ä¸ºXæä¾ä¸ä¸ª[å®¢è§è¯ä¼°|çæ§éæ©]çå¹³å°",
        "ä¼Xå¸¦æ¥[å²å»åå½±å]": "ä¼ç»Xå¸¦æ¥[å²å»|å½±å]",
        "ä»¥Xèµ¢å¾äº[Y]": "ä»¥Xèµ¢å¾äºY",
        "ä¾æ³æå®Xè´è´£Yç[ä¸å®¡|[åºå±æ³é¢|æ£å¯é¢äºå®¡æ¡ä»¶]çå®¡çåæ³å¾çç£å·¥ä½": "ä¾æ³æå®Xè´è´£Yç[ä¸å®¡|åºå±[æ³é¢|æ£å¯é¢]äºå®¡]æ¡ä»¶ç[å®¡ç|æ³å¾çç£å·¥ä½]",
        "å X sockrrlates": "åXsockrrlates",
        "ç¨Xååè¡ä¸­å¿ç³æ¥[ä¸­å¥ç­çº§|æç¥¨äºº[å§å|ä½å|èº«ä»½è¯å·ç ]": "ç¨Xååè¡ä¸­å¿ç³æ¥[ä¸­å¥ç­çº§|æç¥¨äºº[å§å|ä½å|èº«ä»½è¯å·ç ]]",
        "å¯¹Xå¯å®ç°å¨é¨[ä¸»å¤§å¤æ°çå¶çèè´[ä¸»|å¯]æåä½æç[ä¸æ¬¡ææ¾|ä¸æ¬¡å®æY]": "å¯¹Xå¯å®ç°å¨é¨[ä¸»|å¯]æåä½æç[ä¸æ¬¡ææ¾|ä¸æ¬¡å®æY]",
        "ä¸ºXå¥ç®äºäºä¸ä¸ªåä¸ä¸ªè§è²": "ä¸ºXå¥ç®äºä¸ä¸ªåä¸ä¸ªè§è²",
        "æ¯ä¿æ¤Xåé­[[å°é|æ°´ç¾|ç«ç¾]ç­ç¯å¢äºæ]|äººä¸ºæä½[å¤±è¯¯|éè¯¯]|åç§è®¡ç®æºç¯ç½ªè¡ä¸º]å¯¼è´çç ´åè¿ç¨": "æ¯ä¿æ¤Xåé­[[å°é|æ°´ç¾|ç«ç¾]ç­ç¯å¢äºæ|äººä¸ºæä½[å¤±è¯¯|éè¯¯]|åç§è®¡ç®æºç¯ç½ªè¡ä¸º]å¯¼è´çç ´åè¿ç¨",
        "ä»èåè½»ORé¿åXçåè¿«": "ä»è[åè½»|é¿å]Xç[æ©æ¦|åè¿«]",
        "è®©Xä¹è½æ¥è§¦[æéè¯æ±ORå¿ å­èä¹æäº]": "è®©Xä¹è½æ¥è§¦[æéè¯æ±|å¿ å­èä¹æäº]",
        "è´è´£å¯¹Xç[å®¡æ¹åæ¾åç®¡ç]": "è´è´£å¯¹Xç[å®¡æ¹åæ¾|ç®¡ç]",
        "åçXç[è°ä»»|è½¬ä»»|å®¡æ¹|å·¥ä½]": "åçXç[è°ä»»|è½¬ä»»|å®¡æ¹å·¥ä½]",
        "èµX[å£°å¦å¥é·ï¼å¿å¦çæ¶ï¼éµå¦è±ªæ­]": "èµX[å£°å¦å¥é·|å¿å¦çæ¶|éµå¦è±ªæ­]",
        "[æ¥èµä¸ºåè´¾|[é©¬è´©|å± å®°]ä¹ç±»]": "æ¥èµä¸º[åè´¾|é©¬è´©|å± å®°]",
        "åçXç[è°ä»»|è½¬ä»»|å®¡æ¹|å·¥ä½": "åçXç[è°ä»»|è½¬ä»»|å®¡æ¹|å·¥ä½]",
        "ä½ç°äº[ç[è±ªå|æ§æ|èé|åé æ§]æ¼ç»çæè´": "ä½ç°äºXç[è±ªå|æ§æ|èé|åé æ§]æ¼ç»çæè´",
        "å¾åäºX[ç[å±±æ°´é£å|äººæåå²|èªç¶å°ç|æ°ä¿é£æ]": "å¾åäºXç[å±±æ°´é£å|äººæåå²|èªç¶å°ç|æ°ä¿é£æ]",
        "[äº[ä¸ç¦|ä¸å¡]ä¹é´ç[çå¯¼|åå|è¯´æ]|[[å¤ºå¶å©|ä¸å¶å©|ä¸å¶å©]çå å¿å©å¯¼çæè²å·¥ä½]": "[äº[ä¸ç¦|ä¸å¡]ä¹é´ç[çå¯¼|åå|è¯´æ]|[å¤ºå¶å©|ä¸å¶å©]çå å¿å©å¯¼çæè²å·¥ä½]",
    }

    subj_fix_map = {
        "æå­8%ä»¥ä¸æå¸": "æå­98%ä»¥ä¸æå¸",
        ".æä¿äººãè¢«ä¿é©äººåå¶ä»£è¡¨ç[ææè¡ä¸º|éå¤§è¿å¤±]": "[æä¿äºº|è¢«ä¿é©äºº[|ä»£è¡¨]]ç[ææè¡ä¸º|éå¤§è¿å¤±]",
        "[0Ã25Ã16è²|80Ã25Ã16è²]": "[40Ã25Ã16è²|80Ã25Ã16è²]",
        "Codeorg": "Code.org",
        '[ç¤¾ä¼æ»éæ±|ç¤¾ä¼æ»ä¾ç»|å¨åºç¤¾ä¼èµé]': '[ç¤¾ä¼[æ»éæ±|æ»ä¾ç»]|å¨åºç¤¾ä¼èµé]',
        "_ä¸é¨æ²¿æµ·æ¸åº": "ä¸é¨æ²¿æµ·æ¸åº",
        "_å¨ç": "å¨ç",
        "[æµæ¥ä¼ åªéå¢è¡ä»½æéå¬å¸ååäº¬åå¥¥æç©ºç§æåå±æéå¬å¸]": "[æµæ¥ä¼ åªéå¢è¡ä»½æéå¬å¸|åäº¬åå¥¥æç©ºç§æåå±æéå¬å¸]",
        '[udpåtcp]åè®®å¨å®ç°æ°æ®ä¼ è¾æ¶çå¯é æ§': '[udp|tcp]åè®®å¨å®ç°æ°æ®ä¼ è¾æ¶çå¯é æ§',
        "|æ°å|é«å³°|ç´«å²«]": "[æ°å|é«å³°|ç´«å²«]",
        '[é¥¥é¥¿ç¾ç]': '[é¥¥é¥¿|ç¾ç]',
        "ThinkPad T430s 2352A31ç¬è®°æ¬ThinkPad T430s 2352A31ç¬è®°æ¬": "ThinkPad T430s 2352A31ç¬è®°æ¬",
        "å­å­": "å­",
        "ãæ­»äº¡æ¸¸è¡ããæ­»äº¡æ¸¸è¡ã": "ãæ­»äº¡æ¸¸è¡ã",
        "[åä¹¡æ®é[ä¸­å°å­¦åå­¦åæè²]å¸èµéç½®": "åä¹¡æ®é[ä¸­å°å­¦|å­¦åæè²]å¸èµéç½®",
        "ç¥åç¥å": "ç¥å",
        "å½å[æ±½æ²¹|æ´æ²¹]åæ²¹ä»·æ ¼": "å½å[æ±½|æ´æ²¹]åæ²¹ä»·æ ¼",
        "[å³å¨è¡æ¿ä¸»ç®¡é¨é¨|å³å¨è¡æ¿ä¸»ç®¡é¨é¨å§æçä¸é¨æºæ]": "å³å¨è¡æ¿ä¸»ç®¡é¨é¨[|å§æçä¸é¨æºæ]",
        "è®¸å¤[é®é¢|é®é¢è§£å³ä¹é]": "è®¸å¤é®é¢[|è§£å³ä¹é]",
        "[å½å¡é¢æ°´è¡æ¿ä¸»ç®¡é¨é¨|å½å¡é¢æ°´è¡æ¿ä¸»ç®¡é¨é¨ææçæµåç®¡çæºæ]": "å½å¡é¢æ°´è¡æ¿ä¸»ç®¡é¨é¨[|ææçæµåç®¡çæºæ]",
        "[é¿æ¯æ|é¿æ¯æç¸å³äººå]": "é¿æ¯æ[|ç¸å³äººå]",
        "[å¤©çº¿ä¸å¾è§|å¤©çº¿æ¹ä½è§|å¤©çº¿é«åº¦]": "[å¤©çº¿[ä¸å¾è§|æ¹ä½è§]|å¤©çº¿é«åº¦]",
        "[åçº§æ°´è¡æ¿ä¸»ç®¡é¨é¨|åçº§æ°´è¡æ¿ä¸»ç®¡é¨é¨ææçæå³é¨é¨|æµåæºæ]": "[åçº§æ°´è¡æ¿ä¸»ç®¡é¨é¨[|ææçæå³é¨é¨]|æµåæºæ]",
        'äºèäºè': 'äºè',
        '[èµäº§åè´åº]ç[è´§å¸ç»æåå©çç»æ]': '[èµäº§|è´åº]ç[è´§å¸ç»æ|å©çç»æ]',
        'âæ´çâ': "âæ´çâ",
        '[æ¥æ³¢|æ¥æ³¢]': 'æ¥æ³¢',
        "[åä½å¨åå¼çå­¦çå®¿è|ä¼é£èªçå¼çå­¦çå®¿è|ä¼ ç»å¼çå­¦çå®¿è|ååºå¼çç¬ç«å°é¢]": "[[åä½å¨åå¼ç|ä¼é£èªçå¼ç]å­¦çå®¿è|ä¼ ç»å¼çå­¦çå®¿è|ååºå¼çç¬ç«å°é¢]",
        "[åè¶çè´¨é|åè¿ççå¿µ|ä¼è´¨çæå¡|å¸åºçéæ±|ä¼ä¸çåç|ä¼ä¸çç¥ååº¦]": "[åè¶çè´¨é|åè¿ççå¿µ|ä¼è´¨çæå¡|å¸åºçéæ±|ä¼ä¸ç[åç|ç¥ååº¦]]",
        "[å¤§åè¿éå¼å¯¼æå¡æ³|éè¦æå®¢ç»è®°æå¡æ³|ç¹æ®éè¦é¢çº¦æå¡æ³|åè¯­å¼æå¡æ³|ç½ç»å¼çéæå¡æ³]": "[å¤§åè¿éå¼å¯¼æå¡æ³|éç¹æå®¢ç»è®°æå¡æ³|ç¹æ®éè¦é¢çº¦æå¡æ³|åè¯­å¼æå¡æ³|ç½ç»å¼çéæå¡æ³]",
        "[ææ³½æ°|ææ³½æ°]": "[æåè¾¾|ææ³½æ°]",
        "âéå¤æ±å½¢å¼èµäº§ââäººæ°å¸ââéå¤æ±å½¢å¼èµäº§ââäººæ°å¸âåæ¹": "âéå¤æ±å½¢å¼èµäº§ââäººæ°å¸âåæ¹",
        "[çæµæ°æ®|çæµèµæ|æ±¡æºèµæ]": "[çæµ[æ°æ®|èµæ]|æ±¡æºèµæ]",
        "[è¯ç¹å¿|è¯ç¹å¸|å°çº§å¸]": "[è¯ç¹[å¿|å¸]|å°çº§å¸]",
        "æ ¹ç¶èè": "æ ¹ç¶è",
        "ååºäººæ°æ¿åºæå®æºææºæ": "ååºäººæ°æ¿åºæå®æºæ",
        "å·¥å": "åå·¥",
        'ååå¼ºå¤«å¦|ç«¯æ¨æ¨±å­|åå¶ä»åæ°å®¶ææå]ä¸åä½äºº': '[ååå¼ºå¤«å¦|ç«¯æ¨æ¨±å­|å¶ä»åæ°å®¶ææå]',
        "[ä¸»é¢æ²ãåé¶ã||ææ²ãæ²éã]": "[ä¸»é¢æ²ãåé¶ã|ææ²ãæ²éã]",
        "[BodoL innhoff|BodoL innhoffåäº]": "BodoL innhoff[åäº|]",
        "[å¼ è|å¼ èä¼ä¼´ä»¬]": "å¼ è[ä¼ä¼´ä»¬|]",
        "[æ²é|æ²éçæçå·¨é­]": "æ²é[çæçå·¨é­|]",
        "[æ¾ä¸æ­¦å¨ç[åæ°å£«åµ|å¹³æ°]": "æ¾ä¸æ­¦å¨ç[åæ°å£«åµ|å¹³æ°]",
        "[è±å­æ¯|çµè¯|å¹³é¢çµè§|ä¿é©ç®±|ä¹¦æ¡|ææ°|æ´è¡£å®¤|æ²å|æ¨è´¨/é¶æ¨å°æ¿|è¡£æ/è¡£æ©±|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|æµ´ç¼¸ORæ·æµ´|å¤éæå¡]": "[è±å­æ¯|çµè¯|å¹³é¢çµè§|ä¿é©ç®±|ä¹¦æ¡|ææ°|æ´è¡£å®¤|æ²å|æ¨è´¨/é¶æ¨å°æ¿|è¡£æ/è¡£æ©±|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|æµ´ç¼¸|æ·æµ´|å¤éæå¡]",
        "[ãç·åããå¥³åã|ãè·³æ å¸¸ã]ç[ææ|åºæ¿]]": "[ãç·åã|ãå¥³åã|ãè·³æ å¸¸ã]ç[ææ|åºæ¿]",
        "[ç|å¸|å°]çº§æ°´è¡æ¿ä¸»ç®¡é¨é¨]": "[ç|å¸|å°]çº§æ°´è¡æ¿ä¸»ç®¡é¨é¨",
        "[æ²ä¸é­å ¡äº|ç»æå¨åä»¤ç³»å][æ²ä¸é­å ¡äº|ç»æå¨åä»¤ç³»å]": "[æ²ä¸é­å ¡äº|ç»æå¨åä»¤ç³»å]",
        "[å®¹é|åå¼|ç©ºè½½çµæµ|ç©ºè½½æè|ç­è·¯ï¼è´è½½ï¼æè|é»æçµåå®¹é|åå¼|ç©ºè½½çµæµ|ç©ºè½½æè|ç­è·¯ï¼è´è½½ï¼æè|é»æçµå]": "[å®¹é|åå¼|ç©ºè½½çµæµ|ç©ºè½½æè|ç­è·¯ï¼è´è½½ï¼æè|é»æçµå]",
        "[[ãæ±ç é±¼è°±ã|ãéé±¼åã|ãéé±¼é¥²å»æ³ã]": "[ãæ±ç é±¼è°±ã|ãéé±¼åã|ãéé±¼é¥²å»æ³ã]",
        "[Denise Gimpel|Lyce.jankowski|Lyce.jankowski]": "[Denise Gimpel|Dr.james S.Edgren|Lyce.jankowski]",
        "[å¼ å®¶ååç|ä¸æ°å®åç|èµµåç¶åç|æ¹ç»ªé¾åç|å®¢æ·ä»£è¡¨]|": "[å¼ å®¶ååç|ä¸æ°å®åç|èµµåç¶åç|æ¹ç»ªé¾åç|å®¢æ·ä»£è¡¨]",
        "1å[å¹´é¾è¾å¤§çå¿ç«¥|æäºº]|ä¸å¼ å åºæ¶è´¹": "1å[å¹´é¾è¾å¤§çå¿ç«¥|æäºº]ä¸å¼ å åºæ¶è´¹",
        "[[å¬å¼|çè¯]è°è®ºéè¦è¯¾é¢|ä¸æ­ææèªå·±æèè½å]]": "[[å¬å¼|çè¯]è°è®ºéè¦è¯¾é¢|ä¸æ­ææèªå·±æèè½å]",
        "[ä¸­åæå|æ±æ·®æå|ééµæå|å´æå]": "[ä¸­å|æ±æ·®|ééµ|å´]æå",
        "[éæ­£å®|éè£ç»|éå­é]ç­": "[éæ­£å®|éå­é|éè£ç»]ç­",
        "[éæ­£å®|éè£ç»|éå­é]": "[éæ­£å®|éå­é|éè£ç»]",
        "[æ|é£å ä¸ªå£«åµ]": "[é£å ä¸ªå£«åµ|æ]",
        "[ä»|ä¸­çä»ä¹å©|äºä»£åå]": "[ä»|äºä»£åå|ä¸­çä»ä¹å©]",
        '[[åºé³|å®å|åæ°´|å®å·|æ­£å®]é¾æ°]|[è£åé¾æ°]]': '[[åºé³|å®å|åæ°´|å®å·|æ­£å®]é¾æ°|è£åé¾æ°]',
        "[ææç|ä¹¡ç»ç­¹è´¹|éç»ç­¹è´¹]": "[ææç|[ä¹¡|é]ç»ç­¹è´¹]",
        "[èèå¬ä¸»|æ¨åå½é|ä»åº·ç§æ]]": "[èèå¬ä¸»|æ¨åå½é|ä»åº·ç§æ]",
        "å¶[çµæ´»|æ§ä»·æ¯|æå¡ç»éª|å¯¹æ¬åæ¶è´¹è|å¯¹æ¬åæ¶è´¹èæ´å¯]": "å¶[çµæ´»|æ§ä»·æ¯|æå¡ç»éª|å¯¹æ¬åæ¶è´¹èæ´å¯]",
        "æ²¹ç»[ãçº¢è¡£å°å¥³ã|ãç´«ç½|å°ã]|": "æ²¹ç»[ãçº¢è¡£å°å¥³ã|ãç´«ç½å°ã]",
        "[ç¢³çº¤ç»´ORç¡¼çº¤ç»´å¢å¼ºçç¯æ°§æ èåºå¤åææ|éå±åºå¤åææ]": "[[ç¢³çº¤ç»´|ç¡¼çº¤ç»´]å¢å¼ºç[ç¯æ°§æ èåºå¤åææ|éå±åºå¤åææ]]",
        "[åæå¤§å¦|å¤ªå¤æ±|æ­å§é¢": "[åæå¤§å¦|å¤ªå¤æ±|æ­å§é¢]",
        "[æä¿äºº|è¢«ä¿é©äºº|åçäºº": "[æä¿äºº|è¢«ä¿é©äºº|åçäºº]",
        "[é´è|é³èä½è´¨ä¹äºº": "[é´è|é³è]ä½è´¨ä¹äºº",
        "[å±åä¼ä¸æå|åºæ¬åå": "[å±åä¼ä¸æå|åºæ¬åå]",
        "è­æ°§å°æµå¼æ··åæèè£ç½®|ç¾å½é¶æ°å¬å¸åæ¸éè£ç½®|çº¤ç»´æ´»æ§ç­é«æå¸é]": "[è­æ°§å°æµå¼æ··åæèè£ç½®|ç¾å½é¶æ°å¬å¸åæ¸éè£ç½®|çº¤ç»´æ´»æ§ç­é«æå¸é]",
        "è®°|ä¼ é|å½æ¡£]å·¥ä½": "[æ¶åç»è®°|ä¼ é|å½æ¡£]å·¥ä½",
        "è©èé¸ç|è°ç|é¢æ¤ç|å³èç|é£æ¹¿|å¤±ç |æ°å]": "[è©èé¸ç|è°ç|é¢æ¤ç|å³èç|é£æ¹¿|å¤±ç |æ°å]",
        "ä¿é|æå¡]": "[ä¿é|æå¡]",
        "[å¡è¿ªæåæ±½è½¦åå¬å¸|åè±æ¯åå¬å¸": "[å¡è¿ªæåæ±½è½¦åå¬å¸|åè±æ¯åå¬å¸]",
        "[ç¤¾å¢|å¤§è¿é«æ ¡å¤©æåä¼èç": "[ç¤¾å¢|å¤§è¿é«æ ¡å¤©æåä¼èç]",
        "é¥®é£æåç¹è°æ¹æ³]": "[é¥®é£æå|ç¹è°æ¹æ³]",
        "[æ¬ç½ç«": "æ¬ç½ç«",
        "ãåä¼¯èç¹ç§é¦2ã|ãé¾å¤åºã]ç­": "[ãåä¼¯èç¹ç§é¦2ã|ãé¾å¤åºã]ç­",
        "å·«å¯æ²³æ¼æµ|åå¨è§é³é|åéæé¹|èå±±ç|å¤ç§æ|å«ä¸å±±|é¹ææ¥¸èªç¶ä¿æ¤åº|ä¾å®åç§ç¢|ç¦å»ºæçç©ç¾¤|åå¨æ°´ä¸åº¦ååº|ä¹ä»°|ä¹å|è¿ææ°ææ|ä¹è¸èæåè¿éå]": "[å·«å¯æ²³æ¼æµ|åå¨è§é³é|åéæé¹|èå±±ç|å¤ç§æ|å«ä¸å±±|é¹ææ¥¸èªç¶ä¿æ¤åº|ä¾å®åç§ç¢|ç¦å»ºæçç©ç¾¤|åå¨æ°´ä¸åº¦ååº|ä¹ä»°|ä¹å|è¿ææ°ææ|ä¹è¸èæåè¿éå]",
        "ãè¡æ¿ç®¡çä¸æ¿åºè¡ä¸ºç ç©¶ã|ãç¤¾ä¼ä¸»ä¹å¸åºç»æµè®ºã|ãå¯æç»­åå¸ååå±ç ç©¶--ä¸­å½åå·çå®è¯åæã]": "[ãè¡æ¿ç®¡çä¸æ¿åºè¡ä¸ºç ç©¶ã|ãç¤¾ä¼ä¸»ä¹å¸åºç»æµè®ºã|ãå¯æç»­åå¸ååå±ç ç©¶--ä¸­å½åå·çå®è¯åæã]",
        "[é»é³|èè¹|æ°´è|å°é¾è¾|æ°´èé¼ |éè|è¾è": "[é»é³|èè¹|æ°´è|å°é¾è¾|æ°´èé¼ |éè|è¾è]",
        "ãåä¼¯èç¹ç§é¦2ã|ãé¾å¤åºã]": "[ãåä¼¯èç¹ç§é¦2ã|ãé¾å¤åºã]",
        "åºæ¬ç¹è´¨|ç»éªæè®­": "[åºæ¬ç¹è´¨|ç»éªæè®­]",
        "[åçç½ãåçææ¦´å¼¹]": "[åçç½|åçææ¦´å¼¹]",
        "[åçç½ãåçææ¦´å¼¹]ç­": "[åçç½|åçææ¦´å¼¹]",
        "ä¼|": "ä¼",
        "[ç»ä¸é¨ç½²å¤§å¹åº¦è£å]": "ç»ä¸é¨ç½²å¤§å¹åº¦è£å",
        "[æ·±å³å¸åå¹¿ä¸çæè²ç£å¯¼å®¤]": "æ·±å³å¸åå¹¿ä¸çæè²ç£å¯¼å®¤",
        "_å¢å": "å¢å",
        "[ç»¼åç®¡çæ¡£æ¡è°é]å·¥ä½": "[ç»¼åç®¡ç|æ¡£æ¡è°é]å·¥ä½",
        "å¾æ­¥ç»å±±|å²©éçé|æå²©æ¢æ´[]": "[å¾æ­¥ç»å±±|å²©éçé|æå²©æ¢æ´]",
        "[è®²è§£+ç»é£+æå·§è§£æ]": "[è®²è§£|ç»é£|æå·§è§£æ]",
        "[å¬å¸åäºº]çä»·å¼": "[å¬å¸|äºº]çä»·å¼",
        "æè¿·|å¤±è´¥": "æè¿·å¤±è´¥",
        "æ´ä¸ªæ¬§æ´²äººå£èå¹´å|æ´ä¸ªç¦å©ç¤¾ä¼[]": "[æ´ä¸ªæ¬§æ´²äººå£èå¹´å|æ´ä¸ªç¦å©ç¤¾ä¼]",
        "[åºæ¿æ§åèèæ§]": "[åºæ¿æ§|èèæ§]",
        "[äººäºé¨å¨å½äººææµå¨ä¸­å¿ä¸é¦é½ç»æµè´¸æå¤§å­¦é¦ç»ææ¯å¹è®­ä¸­å¿]": "[äººäºé¨å¨å½äººææµå¨ä¸­å¿|é¦é½ç»æµè´¸æå¤§å­¦é¦ç»ææ¯å¹è®­ä¸­å¿]",
        "ãå²å­¦ç ç©¶ã|ãé©¬åæä¸»ä¹ç ç©¶ã|ãå½ä»£ä¸çä¸ç¤¾ä¼ä¸»ä¹ã|ãç§å­¦ç¤¾ä¼ä¸»ä¹ã|ãåææ¥æ¥ãï¼çè®ºçï¼|ãä¸­å½æè²æ¥ãï¼çè®ºçï¼": "[ãå²å­¦ç ç©¶ã|ãé©¬åæä¸»ä¹ç ç©¶ã|ãå½ä»£ä¸çä¸ç¤¾ä¼ä¸»ä¹ã|ãç§å­¦ç¤¾ä¼ä¸»ä¹ã|ãåææ¥æ¥ãï¼çè®ºçï¼|ãä¸­å½æè²æ¥ãï¼çè®ºçï¼]",
        "[ç¥è¯åæè½å]èªç©ºæå¡ä¸é¨äººæ": "[ç¥è¯å|æè½å]èªç©ºæå¡ä¸é¨äººæ",
        "[å¯è±¡ç­åå­¦åç»è®¡åå­¦]ççè®º": "[å¯è±¡ç­åå­¦|ç»è®¡åå­¦]ççè®º",
        "GERMARTâ|âåçç¹â": "[âGERMARTâ|âåçç¹â]",
        "æ¶å°ä½ä¸å¼ æ¬|æºæä½ä¸éè": "[æ¶å°ä½ä¸å¼ æ¬|æºæä½ä¸éè]",
        "_ä¸é¢çéåºç°çæ¡ä¾": "ä¸é¢çéåºç°çæ¡ä¾",
        "ç¤¾å¢|ä¸å¤§è¿å¶ä»é«æ ¡": "[ç¤¾å¢|å¤§è¿å¶ä»é«æ ¡]",
        "_æ¯äº²": "æ¯äº²",
        "_ç¶äº²": "ç¶äº²",
        "[å½å®å± âä¸è­¦æ¿ç½²]": "[âå½å®å± â|è­¦æ¿ç½²]",
        "[å«ééå»ç]": "[å«é|éå»ç]",
        "_æ²¿æµ·": "æ²¿æµ·",
        "è¿ªè¾¾X": "è¿ªè¾¾",
        "[å¤§ä¸­å°åå¸åå°åé]": "[å¤§ä¸­å°åå¸|å°åé]",
        "å¬å¸|": "[å¬å¸|äº§å]",
    }

    obj_fix_map = {
        "|ä¸­å½æå²åºçç¤¾": "ä¸­å½æå²åºçç¤¾",
        "945åç±³": "954åç±³",
        "[æå³è¡ä¸åä¼åå¶ä»ç¤¾ä¼ä¸­ä»ç»ç»]ç": "[æå³è¡ä¸åä¼|å¶ä»ç¤¾ä¼ä¸­ä»ç»ç»]çå·¥ä½",
        "464.ä¸äºº": "46.4ä¸äºº",
        "[å¯¹å­¦çè¿è¡ä»¥ç±å½ä¸»ä¹ä¸ºæ ¸å¿çææ³åå¾·æè²å¯¹å­¦ççææè§åè¿è¡æå¯¼]": "[å¯¹å­¦çè¿è¡ä»¥ç±å½ä¸»ä¹ä¸ºæ ¸å¿çææ³åå¾·æè²|å¯¹å­¦ççææè§åè¿è¡æå¯¼]",
        "è¡ä¸ºä¹å¼ |ä¸å¡æ­£ä¸[]": "[è¡ä¸ºä¹å¼ |ä¸å¡æ­£ä¸]",
        "æ800åé±çå¯æ¯éæ¯é": "æ3800åé±çå¯æ¯éæ¯é",
        "å¬å¸è´è´£äºº|SMGå½±è§å§ä¸­å¿ä¸»ä»»[]": "[å¬å¸è´è´£äºº|SMGå½±è§å§ä¸­å¿ä¸»ä»»]",
        "CuCl22H2O": "CuCl2 2H2O",
        "Codeorgä¸¾åçç¼ç¨å¤§ä¼": "Code.orgä¸¾åçç¼ç¨å¤§ä¼",
        "å±æ[25ä¸ªåç±»|63ä¸ªäºç±»|137ä¸ªåå±|80ä¸ªåç§]": "å±æ[25ä¸ªåç±»|63ä¸ªäºç±»|137ä¸ªåå±|380ä¸ªåç§]",
        "[ç ç¦å|å»ºçª1]4ä¸ª": "[ç ç¦å|å»ºçª]14ä¸ª",
        "00ä½ä¾[å¢å¹¢|å¢ç¢]": "100ä½ä¾[å¢å¹¢|å¢ç¢]",
        "[974å¹´ä»¥ååºççææä¸»å·|è¡¥ç¼]": "1974å¹´ä»¥ååºççææ[ä¸»å·|è¡¥ç¼]",
        "[]æ¥æ¬å­¦æ¯ä¼è®®|å æ¿å¤§çå®¶å­¦ä¼ç­": "[æ¥æ¬å­¦æ¯ä¼è®®|å æ¿å¤§çå®¶å­¦ä¼]ç­",
        "[æ±ªå½©éä¸å¯ä¸åå¯è²è³ä¸é¡¾æ¥]": "[æ±ªå½©é|å¯ä¸å|å¯è²è³|é¡¾æ¥]",
        'é£ç§å¥ç¹çåéº»çæè§|': 'é£ç§å¥ç¹çåéº»çæè§',
        '[è¯ç©åè³é£ç»å]': '[è¯ç©|è³é£]',
        'ç³å¤´|èæ æ¢': '[ç³å¤´|èæ æ¢]',
        'Xåæ°å¨é¨åºå': 'åæ°å¨é¨åºå',
        '[å¼ä¸ç³è¯·ä¹¦åæ³çéªèµè¯æ]': 'å¼ä¸ç³è¯·ä¹¦',
        'å½éåç¾åå¹´||å½éåç¾æ¥': '[å½éåç¾åå¹´|å½éåç¾æ¥]',
        "[å¯è±¡ç­åå­¦åç»è®¡åå­¦]ççè®º": "[å¯è±¡ç­åå­¦|ç»è®¡åå­¦]ççè®º",
        "[èçº¦èµæºåä¿æ¤ç¯å¢]çåå±æ¨¡å¼": "[èçº¦èµæº|ä¿æ¤ç¯å¢]çåå±æ¨¡å¼",
        "[åéååæ]": "[åé|åæ]",
        "æ²¡æ[äº²ç¼|è¡ç¼|æ²¡å°ç¼]": "æ²¡æ[äº²ç¼|è¡ç¼|å°ç¼]",
        '[é³å°|DVDæ­æ¾æº|å«æé¢é|å¹³é¢çµè§|ç©ºè°|ä¹¦æ¡|å®¢åè§|æ´è¡£æº|æ²å|è¡£æ|è¡£æ©±|æ·æµ´|æµ´ç¼¸|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|å°ç®±|å¾®æ³¢ç|å¨æ¿|ç¨é¤åº|çµç§æ°´å£¶|å¨æ¿ç¨å·|é¤æ¡]ç­è®¾æ½': '[é³å°|DVDæ­æ¾æº|å«æé¢é|å¹³é¢çµè§|ç©ºè°|ä¹¦æ¡|å®¢åè§|æ´è¡£æº|æ²å|è¡£æ|è¡£æ©±|æ·æµ´|æµ´ç¼¸|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|å°ç®±|å¾®æ³¢ç|ç¨é¤åº|å¨æ¿|çµç§æ°´å£¶|å¨æ¿ç¨å·|é¤æ¡]ç­è®¾æ½',
        '[å¯¹åº|åä¸ç§é¢è²|åä¸ç§å½¢ç¶]ç': '[å¯¹åº|åä¸ç§[é¢è²|å½¢ç¶]]ç',
        'å®æåå­¦ååºææ¶èçè¯åéè¯åé': 'å®æåå­¦ååºææ¶èçè¯åé',
        '[çº¢æ°´æ³¡|ç½æ°´æ³¡|èæ°´æ³¡|çº¢ç½æ°´æ³¡|é»ç½æ°´éåéæ°´æ³¡|çº¢é¡¶æ°´æ³¡|å¢¨æ°´æ³¡|äºè±æ°´æ³¡|ç´«èæ°´æ³¡]': '[çº¢æ°´æ³¡|ç½æ°´æ³¡|èæ°´æ³¡|çº¢ç½æ°´æ³¡|é»ç½æ°´æ³¡|éåéæ°´æ³¡|çº¢é¡¶æ°´æ³¡|å¢¨æ°´æ³¡|äºè±æ°´æ³¡|ç´«èæ°´æ³¡]',
        "å°æææ³¨è¯´æ": "å°ææ³¨è¯´æ",
        "åç¨çç[è®¡ååé¡¹ç®|ç»è®°é¡¹ç®]": "åç¨ç[è®¡ååé¡¹ç®|ç»è®°é¡¹ç®]",
        "AutoCAD2006ä¸­æçç[æä»¶æä½|ç»å¾è®¾ç½®|ç»å¶äºç»´å¾å½¢|ç¼è¾å¾å½¢å¯¹è±¡çåºæ¬å½ä»¤|å¾æ¡çå¡«åæ¹æ³åå¶è®¾ç½®|å¾å±ãåä»¥åå±æ§çå®ä¹|ä¸ç»´å¾å½¢åå»º|æ¸²æä¸çè²|å¾å½¢æå°]": "AutoCAD 2006ä¸­æçç[æä»¶æä½|ç»å¾è®¾ç½®|ç»å¶äºç»´å¾å½¢|ç¼è¾å¾å½¢å¯¹è±¡çåºæ¬å½ä»¤|å¾æ¡çå¡«åæ¹æ³åå¶è®¾ç½®|å¾å±ãåä»¥åå±æ§çå®ä¹|ä¸ç»´å¾å½¢åå»º|æ¸²æä¸çè²|å¾å½¢æå°]",
        "å®æ´å®¢æ·çå½å¨æçç[åç|åå±]": "å®æ´å®¢æ·çå½å¨æç[åç|åå±]",
        "æ»å»è°è|ä¸»åè°": "[æ»å»è°è|ä¸»åè°]",
        "[ç§å­¦å®¶åèåå½ç²®åç»ç»åä¸çå«çç»ç»]": "[ç§å­¦å®¶åèåå½ç²®åç»ç»|ä¸çå«çç»ç»]",
        "[ä¸æ¡§çè³äºæ¡§]çå¤§æµ·è¹": "[ä¸æ¡§|äºæ¡§]çå¤§æµ·è¹",
        "[æ°´çµåæçµæ°åå¿åå°æ°´çµä»£çæå·¥ç¨å»ºè®¾]": "[æ°´çµåæçµæ°åå¿|å°æ°´çµä»£çæå·¥ç¨å»ºè®¾]",
        "[âåè°ãä¸ªæ§ãæ±çãåæ°â]çåå­¦çå¿µ": "[âåè°|ä¸ªæ§|æ±ç|åæ°â]çåå­¦çå¿µ",
        "[å¿è¦æ§åç°å®å·®è·]": "[å¿è¦æ§|ç°å®å·®è·]",
        "[é¥®é£æåç¹è°æ¹æ³]": "[é¥®é£æå|ç¹è°æ¹æ³]",
        "[ææ­10å¬æ¤æ¡æ­4å¬æ¤]": "[ææ­10å¬æ¤|æ¡æ­4å¬æ¤]",
        "[æµ·åçä½è²æ»ä¼åæµ·åçæ°æ¿å]": "[æµ·åçä½è²æ»ä¼|æµ·åçæ°æ¿å]",
        "ææå¸¦å¤´ä½ç¨|å¢éåä½ç²¾ç¥[]": "[ææå¸¦å¤´ä½ç¨|å¢éåä½ç²¾ç¥]",
        "[å°å¯ç±åç¾è¤¶è£]": "[å°å¯ç±|ç¾è¤¶è£]",
        "çº¦å å¨_çé¢ç§¯ç1/2": "çº¦å å¨çé¢ç§¯ç1/2",
        "[åºæ¿æ§åèèæ§]": "[åºæ¿æ§|èèæ§]",
        "æé«ç[ç¥ååº¦åå½±åå]": "æé«ç[ç¥ååº¦|å½±åå]",
        "å½ä¼ç­|å¤®ä¼ç­|å½è¿æ°é": "[å½ä¼ç­|å¤®ä¼ç­|å½è¿æ°é]",
        "[è¿å¨ååå]ä¹ä¸­": "[è¿å¨|åå]ä¹ä¸­",
        "[å«ééå»ç]": "[å«é|éå»ç]",
        "å½å®¶[äº§ä¸æ¿ç­åæ¹é©æªæ½]": "å½å®¶[äº§ä¸æ¿ç­|æ¹é©æªæ½]",
        "[æ£è±ä¼ä¸åå±è§åååºåæ§æ£è±äº¤æå¸åºåå±è§å]": "[æ£è±ä¼ä¸åå±è§å|åºåæ§æ£è±äº¤æå¸åºåå±è§å]",
        "[è¿·æå­¤ç¬]": "[è¿·æ|å­¤ç¬]",
        "è±å½|ç¾å½": "[è±å½|ç¾å½]",
        "[ç¡«åç¿åæ°§åç¿]": "[ç¡«åç¿|æ°§åç¿]",
        "[çº¯ååç­æ´»]çç²èçæ¯åä¹èè¡¨é¢æåçæ··åæ¶²": "[çº¯å|ç­æ´»]çç²èçæ¯åä¹èè¡¨é¢æåçæ··åæ¶²",
        "[åºå±åç»ç»å»ºè®¾åååæè²ç®¡ç]å·¥ä½": "[åºå±åç»ç»å»ºè®¾|ååæè²ç®¡ç]å·¥ä½",
        "[åé¸æ¥çå­£è]": "åé¸æ¥çå­£è",
        "[é»é±¼|å¸¦é±¼|é²³é±¼|è¾ç±»|è¹ç±»åè´è»ç±»]": "[é»é±¼|å¸¦é±¼|é²³é±¼|è¾ç±»|è¹ç±»|è´è»ç±»]",
        "[]åæ¡¥å¤§å­¦ææ|è±åçå®¶å­¦ä¼ä¼å|è±é­èªç¶åå²åç©é¦èµæ·±å çç©å­¦ä¸å®¶": "[åæ¡¥å¤§å­¦ææ|è±åçå®¶å­¦ä¼ä¼å|è±é­èªç¶åå²åç©é¦èµæ·±å çç©å­¦ä¸å®¶]",
        "å­¦æ ¡æç|å­¦çå°±ä¸]ç­ç§ç§å°é¾": "[å­¦æ ¡æç|å­¦çå°±ä¸]ç­ç§ç§å°é¾",
        "æ´é¢å¥¶|ä¿æ¹¿æ°´|ç¼é¨ç²¾æ²¹|é¢é¨ç²¾æ²¹|ç¼è´´è|é¢è´´è]": "[æ´é¢å¥¶|ä¿æ¹¿æ°´|ç¼é¨ç²¾æ²¹|é¢é¨ç²¾æ²¹|ç¼è´´è|é¢è´´è]",
        "[åæ§|åé£|åçºª|ç": "[åæ§|åé£|åçºª]ç",
        "[åç±»åå·¥ç¨ååå·¥æ¹é¢çç»éª|": "[åç±»åå·¥ç¨ååå·¥æ¹é¢çç»éª|ç²¾æ¹çéå±æ·¬ç«ææ¯]",
        "å·«å¯æ²³æ¼æµ|åå¨è§é³é|åéæé¹|èå±±ç|å¤ç§æ|å«ä¸å±±|é¹ææ¥¸èªç¶ä¿æ¤åº|ä¾å®åç§ç¢|ç¦å»ºæçç©ç¾¤|åå¨æ°´ä¸åº¦ååº|ä¹ä»°|ä¹å|è¿ææ°ææ|ä¹è¸èæåè¿éå]": "[å·«å¯æ²³æ¼æµ|åå¨è§é³é|åéæé¹|èå±±ç|å¤ç§æ|å«ä¸å±±|é¹ææ¥¸èªç¶ä¿æ¤åº|ä¾å®åç§ç¢|ç¦å»ºæçç©ç¾¤|åå¨æ°´ä¸åº¦ååº|ä¹ä»°|ä¹å|è¿ææ°ææ|ä¹è¸èæåè¿éå]",
        "ãåä¼¯èç¹ç§é¦2ã|ãé¾å¤åºã]ç­": "[ãåä¼¯èç¹ç§é¦2ã|ãé¾å¤åºã]ç­",
        "[é»ç¼å|ç¼é¨ç±çº¹": "[é»ç¼å|ç¼é¨ç±çº¹]",
        "[æ´é¢å¥¶|ä¿æ¹¿æ°´|ç¼é¨ç²¾æ²¹|é¢é¨ç²¾æ²¹|ç¼è´´è|é¢è´´è]": "æ´é¢å¥¶|ä¿æ¹¿æ°´|ç¼é¨ç²¾æ²¹|é¢é¨ç²¾æ²¹|ç¼è´´è|é¢è´´è]",
        "[åºæ¬ç¹è´¨|ç»éªæè®­": "[åºæ¬ç¹è´¨|ç»éªæè®­]",
        "[ééå¼æ°´|å°è®¸ç½ç³": "[ééå¼æ°´|å°è®¸ç½ç³]",
        "èº«ä½å¥½|åæ°å¤§]ç": "[èº«ä½å¥½|åæ°å¤§]çéå¹´äºº",
        "[ç®¡çå­¦ç»æµå­¦|äººåèµæºç®¡çæ¹é¢": "[ç®¡çå­¦|ç»æµå­¦|äººåèµæºç®¡ç]æ¹é¢",
        "[èå°|æåº|å¬ç¤¾å¤§ä¼åºå°|": "[èå°|æåº|å¬ç¤¾å¤§ä¼åºå°]",
        "âå¿åä¸­åç±å½ç±å®¶ç¬çå¤©ä¸é£äºäºââæ°¸èåæä¸ºä¾ ä¸ºä¹ä¹¦åå¸¸ä¼´å½å£«åâ]": "[âå¿åä¸­åç±å½ç±å®¶ç¬çå¤©ä¸é£äºäºâ|âæ°¸èåæä¸ºä¾ ä¸ºä¹ä¹¦åå¸¸ä¼´å½å£«åâ]",
        "æ£èè«æç|æ£é¾è«ç]ç­": "[æ£èè«æç|æ£é¾è«ç]ç­",
        "[å¶å¯¹åºçææå|ä¸ä¸ä¸ªææåä¸­å¯¹åºä½": "[å¶å¯¹åºçææå|ä¸ä¸ä¸ªææåä¸­å¯¹åºä½]",
        "[ææçææ·±å|æ²éèä¸æ²å|ææ¬èä¸èæ³": "[ææçææ·±å|æ²éèä¸æ²å|ææ¬èä¸èæ³]",
        "[å¡è¿ªæåæ±½è½¦åå¬å¸|åè±æ¯åå¬å¸": "[å¡è¿ªæåæ±½è½¦åå¬å¸|åè±æ¯åå¬å¸]",
        "å¾æº|èå|æ¨¡å¼åæ¢]æé®": "[å¾æº|èå|æ¨¡å¼åæ¢]æé®",
        "|å¸å§|æ¿åºç´å±äºä¸åä½|åé¨é¨æå±äºä¸åä½]çæºææ¹é©æ¹æ¡": "[å¸å§|æ¿åºç´å±äºä¸åä½|åé¨é¨æå±äºä¸åä½]çæºææ¹é©æ¹æ¡",
        "âç©ºçâ|âå®éçâ]æè¯ç¶æ": "[âç©ºçâ|âå®éçâ]æè¯ç¶æ",
        "é¿ä¾æå°|è¾å±±]": "[é¿ä¾æå°|è¾å±±]",
        "è¿äºç­å¿åä¸æ¬é¡¹å·¥ä½çä¸­å¤äººå£«]": "è¿äºç­å¿åä¸æ¬é¡¹å·¥ä½çä¸­å¤äººå£«",
        "[åä¹¡æ°´å©è®¾æ½å»ºè®¾è½åæ°´èµæºå©ç¨æ°´å¹³ä¸ä¿éè½å|": "[åä¹¡æ°´å©è®¾æ½å»ºè®¾è½å|æ°´èµæºå©ç¨æ°´å¹³ä¸ä¿éè½å]",
        "Java|iOS]å¼åå¤å¹´": "[Java|iOS]å¼åå¤å¹´",
        "[ä¸å®çæ©æ¦å|ä¸éçè´¨æ": "[ä¸å®çæ©æ¦å|ä¸éçè´¨æ]",
        "[å¨æ°12è²é¢æå¢¨æ°´ç³»ç»|": "[å¨æ°12è²é¢æå¢¨æ°´ç³»ç»|è¶é«å¯åº¦æå°å¤´ææ¯âFINEâ]",
        "[éå¤§å³ç­|éè¦é¨ç½²|ãæ¿åºå·¥ä½æ¥åã|éç¹å·¥ä½": "[éå¤§å³ç­|éè¦é¨ç½²|ãæ¿åºå·¥ä½æ¥åã|éç¹å·¥ä½]",
        "è±è¯­ï¼Marc Edworthy]]]": "è±è¯­ï¼Marc Edworthy",
        "[å­©å­ä»¬èª": "å­©å­ä»¬èªå·±",
        "å®æ§èµæçå¤å«åæ|å®éèµæçå¤å«åæ[": "[å®æ§èµæçå¤å«åæ|å®éèµæçå¤å«åæ]",
        "é³åéè´­|å¿«ééè´­]åå": "[é³å|å¿«é]éè´­åå",
        "|åç±»æå­¦å®ä¹ |å®éªå®¤|å¤åè½æ¥åå|è®¡ç®æºç½ç»ä¸­å¿|æ ¡å­ç½]": "[åç±»æå­¦å®ä¹ |å®éªå®¤|å¤åè½æ¥åå|è®¡ç®æºç½ç»ä¸­å¿|æ ¡å­ç½]",
        "æ»è¾æ¶¦èº|è¡¥èæç®]": "[æ»è¾æ¶¦èº|è¡¥èæç®]",
        "å¬å®é¨|ä¸å½±è±ç]ç­åä½": "[å¬å®é¨|ä¸å½±è±ç]ç­åä½",
        "ç©å¿½èå®|è¿æ³ä¹±çºª]ç": "[ç©å¿½èå®|è¿æ³ä¹±çºª]ç",
        "Internetç½ç»|IVRè¯­é³ç³»ç»|WAPææºæºè½ä»¥|èªå¨å®ç¥¨æº]ç­å¤ç§æ¹å¼": "[Internetç½ç»|IVRè¯­é³ç³»ç»|WAPææºæºè½ä»¥|èªå¨å®ç¥¨æº]ç­å¤ç§æ¹å¼",
        'ãè¡æ¿ç®¡çä¸æ¿åºè¡ä¸ºç ç©¶ã|ãç¤¾ä¼ä¸»ä¹å¸åºç»æµè®ºã|ãå¯æç»­åå¸ååå±ç ç©¶--ä¸­å½åå·çå®è¯åæã]ç­': '[ãè¡æ¿ç®¡çä¸æ¿åºè¡ä¸ºç ç©¶ã|ãç¤¾ä¼ä¸»ä¹å¸åºç»æµè®ºã|ãå¯æç»­åå¸ååå±ç ç©¶--ä¸­å½åå·çå®è¯åæã]ç­',
        'è©èé¸ç|è°ç|é¢æ¤ç|å³èç|é£æ¹¿|å¤±ç |æ°å]': '[è©èé¸ç|è°ç|é¢æ¤ç|å³èç|é£æ¹¿|å¤±ç |æ°å]',
        'âä¸è½¬æ¬â|âä¸æ¥æ¬â]æ¹å¼': '[âä¸è½¬æ¬â|âä¸æ¥æ¬â]æ¹å¼',
        '[èå¤´è|æ°´éæµå¤´é»åçãéé¥­çãåçä¸ä¸ãéåçãééçå¤´': '[èå¤´è|æ°´éæµå¤´|é»åç|éé¥­ç|åçä¸ä¸|éåç|ééçå¤´]',
        'è¾åºç[åä¸ªéåä¸¤ä¸ªå¼ååº]': 'è¾åºç[åä¸ªé|ä¸¤ä¸ªå¼ååº]',
        '[é¨éåæ²|æ¥ç§åè¶³|æ°åæ¸©å': '[é¨éåæ²|æ¥ç§åè¶³|æ°åæ¸©å]',
        '[å¬å¸è£äº|å¯æ»è£|å¯¼èªäºä¸é¨æ»ç»ç': '[å¬å¸è£äº|å¯æ»è£|å¯¼èªäºä¸é¨æ»ç»ç]',
        'ä¼ æå¨ä¿¡å·è°ç|æ°æ®è½¬æ¢åå¤ç]è§£å³æ¹æ¡æ¹é¢': '[ä¼ æå¨ä¿¡å·è°ç|æ°æ®è½¬æ¢åå¤ç]è§£å³æ¹æ¡æ¹é¢',
        'ç«æ¡çç£|ä¾¦æ¥çç£|åç½æ§è¡çç£]äºé¡¹': '[ç«æ¡çç£|ä¾¦æ¥çç£|åç½æ§è¡çç£]äºé¡¹',
        'æ­¦å¨è£å¤ç§ç çäº§è®¸å¯è¯|ååç§ç çäº§æ¿æåä½ä¿å¯èµæ ¼]å®¡æ¥': '[æ­¦å¨è£å¤ç§ç çäº§è®¸å¯è¯|ååç§ç çäº§æ¿æåä½ä¿å¯èµæ ¼]å®¡æ¥',
        'å°¼æ³å°|é¡é|ä¸ä¸¹|å­å æ|å°åº¦|ä¸­å½å¤§éçè¥¿èèªæ²»åº]ç­å°': '[å°¼æ³å°|é¡é|ä¸ä¸¹|å­å æ|å°åº¦|ä¸­å½å¤§éçè¥¿èèªæ²»åº]ç­å°',
        "ç¾å½ç½å¾·å²å¤§å­¦ç": 'ç¾å½ç½å¾·å²å¤§å­¦',
        'è­æ°§å°æµå¼æ··åæèè£ç½®|ç¾å½é¶æ°å¬å¸åæ¸éè£ç½®|çº¤ç»´æ´»æ§ç­é«æå¸é]ç­': '[è­æ°§å°æµå¼æ··åæèè£ç½®|ç¾å½é¶æ°å¬å¸åæ¸éè£ç½®|çº¤ç»´æ´»æ§ç­é«æå¸é]ç­',
        '[å¯ç±èæ¶å°]ç': '[å¯ç±|æ¶å°]ç',
        "ä½åæä¸»æçâè·¨è¶å¼åå±çâå®éªæ ¡": 'ä½åæä¸»æçâè·¨è¶å¼åå±âçå®éªæ ¡',
        "ä¸­é«ç«¯è½¯ä½å®¶å·ä¸­é«ç«¯è½¯ä½å®¶å·": "ä¸­é«ç«¯è½¯ä½å®¶å·",
        "æ°æ°æä¸»ä¹": "æ°æä¸»ä¹",
        "ä¸æçä¸æç": "ä¸æç",
        "è®¸å¤[é®é¢|é®é¢è§£å³ä¹é]": "è®¸å¤é®é¢[|è§£å³ä¹é]",
        "[å°æ¹¾å¤§å­¦å²å­¦ç³»ä¸»ä»»|æ¯å©æ¶é²æ±¶å¤§å­¦å®¢åº§ææ|è·å°è±é¡¿å¤§å­¦å®¢åº§ææ]": "[å°æ¹¾å¤§å­¦[å²å­¦ç³»ä¸»ä»»|å²å­¦ç ç©¶ææé¿]|[æ¯å©æ¶é²æ±¶å¤§å­¦|è·å°è±é¡¿å¤§å­¦]å®¢åº§ææ]",
        "[å®£ä¼ å¹²äºå¯ç§é¿|å¿æèå¯ä¸»å¸­|å¿æèä¸»å¸­|é»åå¸ä½å®¶åä¼å¯ä¸»å¸­|ä¸­å½éä¿æèºç ç©¶ä¼çäº|æ¹åçæ°é´æèºå®¶åä¼å¯ä¸»å¸­]": "[å®£ä¼ å¹²äºå¯ç§é¿|å¿æè[å¯ä¸»å¸­|ä¸»å¸­]|é»åå¸ä½å®¶åä¼å¯ä¸»å¸­|ä¸­å½éä¿æèºç ç©¶ä¼çäº|æ¹åçæ°é´æèºå®¶åä¼å¯ä¸»å¸­]",
        "[ç½ç»åºç¨æ§è½å é|å®å¨åå®¹ç®¡ç|å®å¨äºä»¶ç®¡ç|ç¨æ·ç®¡ç|ç½ç»èµæºç®¡ç|ç½ç»èµæºä¼å|æ¡é¢ç³»ç»ç®¡ç]": "[ç½ç»åºç¨æ§è½å é|å®å¨åå®¹ç®¡ç|å®å¨äºä»¶ç®¡ç|ç¨æ·ç®¡ç|ç½ç»èµæº[ç®¡ç|ä¼å]|æ¡é¢ç³»ç»ç®¡ç]",
        "[ç è´¨å²©ç è´¨å²©|ç¢³é¸çå²©|çº¢å²©|ç¬¬åçºªæ¾æ£å ç§¯ç©]": "[ç è´¨å²©|ç¢³é¸çå²©|çº¢å²©|ç¬¬åçºªæ¾æ£å ç§¯ç©]",
        "[èå¡å|èå¡éè¿å°å¸¦]": "èå¡[å|éè¿å°å¸¦]",
        "[è±å½æµªæ¼«ä¸»ä¹é£æ¯ç»å®¶|èåçæ°´å½©ç»å®¶|èåçæ²¹ç»å®¶]": "[è±å½æµªæ¼«ä¸»ä¹é£æ¯ç»å®¶|èåç[æ°´å½©ç»å®¶|æ²¹ç»å®¶]]",
        "å¦ä½æè½ä½¿[ç»ç»çåééä¸­éä¸­å®ç°ä¸åçæ æææ|æææ§å¶åå°é£é©]": "å¦ä½æè½ä½¿[ç»ç»çåééä¸­å®ç°ä¸åçæ æææ|æææ§å¶åå°é£é©]",
        "[ææ°ç ç©¶|ææ°è®¾è®¡|ææ°å¶é ]ç": "ææ°[ç ç©¶|è®¾è®¡|å¶é ]ç",
        "[å¹³ç´ è°å»|äºå¥åº·ç¶æ|æ¢æ§ç¾ç|èå¼±æ§ç¾ç|çåæ¢å¤]ç­äººç¾¤": "[å¹³ç´ è°å»|äºå¥åº·ç¶æ|[æ¢æ§|èå¼±æ§]ç¾ç|çåæ¢å¤]ç­äººç¾¤",
        "[åå§å¸¸å§|æ¹åå¤§å­¦å¯æ ¡é¿|é©¬åæä¸»ä¹å­¦é¢åå£«çå¯¼å¸|é©¬åæä¸»ä¹å­¦é¢ææ|æ¹åçæçªåºè´¡ç®ä¸­éå¹´ä¸å®¶]": "[æ¹åå¤§å­¦[åå§å¸¸å§|å¯æ ¡é¿]|é©¬åæä¸»ä¹å­¦é¢[åå£«çå¯¼å¸|ææ]|æ¹åçæçªåºè´¡ç®ä¸­éå¹´ä¸å®¶]",
        "[å®éªå®¤|é³ä¹å®¤å®éªå®¤|é³ä¹å®¤|ç§ææ´»å¨å®¤|å­¦çæºæ¿|å¤åè½æå®¤|å­¦çå¬å¯|ç§ææ´»å¨å®¤|å­¦çæºæ¿|å¤åè½æå®¤|å­¦çå¬å¯]": "[å®éªå®¤|é³ä¹å®¤|ç§ææ´»å¨å®¤|å­¦çæºæ¿|å¤åè½æå®¤|å­¦çå¬å¯]",
        "æå½[éèé«å|éèé«åæ¯é»å°åº]": "æå½éèé«å[|æ¯é»å°åº]",
        "[é£æºç¨çæ|ç«ç®­ç¨æ¨è¿å|åç§æ¶¦æ»å|åç§æ¶²åæ²¹]": "[é£æºç¨çæ|ç«ç®­ç¨æ¨è¿å|åç§[æ¶¦æ»å|æ¶²åæ²¹]]",
        "[å±åç½ä¿¡æ¯ä¼ é|å¹¿åç½ä¿¡æ¯ä¼ é]": "[å±åç½|å¹¿åç½]ä¿¡æ¯ä¼ é",
        "[ãå¬å¸æ³ã|æå³æ³å¾|æå³æ³è§]çè§å®": "[ãå¬å¸æ³ã|æå³[æ³å¾|æ³è§]çè§å®]",
        "[[èªå·±|æ­£éä»¬]çåç§|èªå·±ççæ]": "[æ­£éä»¬çåç§|èªå·±ççæ]",
        "[å¹è®­å­¦åç®¡ç||å¹è®­å­¦åç®¡çåç¤¾åºåº·å¤åè°åæè¯ä¸å²]": "[å¹è®­å­¦åç®¡ç|ç¤¾åºåº·å¤åè°åæè¯ä¸å²]",
        "[ä¸æµ·å¸ä½è²æ»ä¼ç¬¬ä¸å±å§åä¼å¸¸å§|ä¸æµ·å¸ä½è²æ»ä¼ç¬¬ä¸å±å§åä¼å¯ä¸»å¸­]": "ä¸æµ·å¸ä½è²æ»ä¼ç¬¬ä¸å±å§åä¼[å¸¸å§|å¯ä¸»å¸­]",
        "[äººç±»åç³20ä½ä»¶|ç³å¶åä¸ä½ä»¶|éª¨è§å¨åç³|åºä¹³å¨ç©åç³]": "[äººç±»åç³20ä½ä»¶|ç³å¶åä¸ä½ä»¶|[éª¨è§å¨|åºä¹³å¨ç©]åç³]",
        "[ç±è®¾åºçå¸æ¿åºè¡ä½¿çç»æµç¤¾ä¼ç®¡çæé|[çæ¿åº|çæ¿åºé¨é¨]ä¸æ¾ç»[çè¾å¸æ¿åº|çè¾å¸æ¿åºé¨é¨]çç»æµç¤¾ä¼ç®¡çæé": "[ç±è®¾åºçå¸æ¿åºè¡ä½¿çç»æµç¤¾ä¼ç®¡çæé|çæ¿åº[|é¨é¨]ä¸æ¾ç»çè¾å¸æ¿åº[|é¨é¨]çç»æµç¤¾ä¼ç®¡çæé]",
        "[çè¾å¸æ¿åº|çè¾å¸æ¿åºé¨é¨]": "[çè¾å¸æ¿åº[|é¨é¨]]",
        "[ç°å­|åéäºåä¸å¹´ææ|åº·çåä¸å¹´ææ]": "[ç°å­[åéäºåä¸å¹´|åº·çåä¸å¹´]ææ]",
        '[åç±»æ¿å±å»ºç­|æ¿å±å»ºç­éå±è®¾æ½|å¸æ¿è®¾æ½æéè®¾é²]': '[åç±»æ¿å±å»ºç­[|éå±è®¾æ½]|å¸æ¿è®¾æ½æéè®¾é²]',
        '[å¤§åè¿éå¼å¯¼æå¡æ³|éè¦æå®¢ç»è®°æå¡æ³|ç¹æ®éè¦é¢çº¦æå¡æ³|åè¯­å¼æå¡æ³|ç½ç»å¼çéæå¡æ³]': '[å¤§åè¿éå¼å¯¼æå¡æ³|éç¹æå®¢ç»è®°æå¡æ³|ç¹æ®éè¦é¢çº¦æå¡æ³|åè¯­å¼æå¡æ³|ç½ç»å¼çéæå¡æ³]',
        "[èåæ°å­¦å®¶|æ²å°å¤«å¥å¾ä¸»|é¿è´å°å¥å¾ä¸»]": "[èåæ°å­¦å®¶|[æ²å°å¤«å¥|é¿è´å°å¥]å¾ä¸»]",
        "[åç­ç»è|èç­ç»è|çè|éµæ¯èèæ ª|ç¸å³åè§£é¶]": "[[åç­|èç­]ç»è|çè|éµæ¯èèæ ª|ç¸å³åè§£é¶]",
        "[å·äººæ°æ¿åº|å¸äººæ°æ¿åº|å°åºè¡æ¿å¬ç½²ç¿å±±ä¼ä¸ä¸»ç®¡é¨é¨]ä¼åæå³[é¨é¨|åä½]": "[[å·|å¸]äººæ°æ¿åº|å°åºè¡æ¿å¬ç½²ç¿å±±ä¼ä¸ä¸»ç®¡é¨é¨]ä¼åæå³[é¨é¨|åä½]",
        "ä¸éä¸éè·¯": "ä¸éè·¯",
        "æ¨ç®æåé¢å¯ä¸ä¸å¤çç¸éçéééé": "æ¨ç®æåé¢å¯ä¸ä¸å¤çç¸éçéé",
        "[éé¸¡|éé¸¡]": "[éé¸¡|éé¸­]",
        "ä¸æ¹[æ¹éª¨|è§å¨]": "ä¸æ¹[éª¨|è§å¨]",
        "ä»¤äººæ¦ç¶å¿å¨çè½½ä½è½½ä½": "ä»¤äººæ¦ç¶å¿å¨çè½½ä½",
        "[å¢ä½æ¯èµæ»åç¬¬ä¸|ä¸ªäººç¬¬äºå|ä¸ªäººç¬¬ä¸å]": "[å¢ä½æ¯èµæ»åç¬¬ä¸|ä¸ªäºº[ç¬¬äºå|ç¬¬ä¸å]]",
        '[ä»ä¹å¯ä»¥å|ä»ä¹ä¸å¯ä»¥å]': '[ä»ä¹å¯ä»¥å|ä¸å¯ä»¥å]',
        '[âç¬¬äºå±ä¸­å½è¯å§èºæ¯èâ|âç¬¬å«å±ä¸­å½æå§èâ|âå¨å½å°æ¹ææ²ä¼ç§å§ç®è¯æ¯å±æ¼â|çæèºç²¾åå·¥ç¨ä¸ç­å¥çæèºç²¾åå·¥ç¨ä¸ç­å¥]': '[âç¬¬äºå±ä¸­å½è¯å§èºæ¯èâ|âç¬¬å«å±ä¸­å½æå§èâ|âå¨å½å°æ¹ææ²ä¼ç§å§ç®è¯æ¯å±æ¼â|çæèºç²¾åå·¥ç¨ä¸ç­å¥]',
        'å¿çº§éç¹æç©ä¿æ¤åä½å¿çº§éç¹æç©ä¿æ¤åä½': 'å¿çº§éç¹æç©ä¿æ¤åä½',
        'ç¡é¸|åçç¡«é¸|æ°¯ç£ºé¸ãä¹ç¯äºèºãä¹ç¯äºèºãæ°¢æ°§åé ': '[ç¡é¸|åçç¡«é¸|æ°¯ç£ºé¸|ä¹ç¯äºèº|ä¹ç¯äºèº|æ°¢æ°§åé ]',
        'å¨å½å°çéå®å¬å¸|ç¹çº¦ç»éå¤æå¡äººå': 'å¨å½å°ç[éå®å¬å¸|ç¹çº¦ç»éå¤æå¡äººå]',
        'äºææ¥å¤|è´£ä»»è¿½ç©¶]è½å®æåµ': '[äºææ¥å¤|è´£ä»»è¿½ç©¶]è½å®æåµ',
        'ç°æç[é²ç«å¢|å®é²æ£æµ|å¥ä¾µæ£æµ|è´è½½åè¡¡|é¢å®½ç®¡ç|ç½ç»é²æ¯]ç­[è®¾å¤|ç½ç»]é®é¢': 'ç°æç[é²ç«å¢|[å®é²|å¥ä¾µ]æ£æµ|è´è½½åè¡¡|é¢å®½ç®¡ç|ç½ç»é²æ¯]ç­[è®¾å¤|ç½ç»]é®é¢',
        '[æ¥å¥ç¡¬ä»¶|è½¯ä»¶æä½çæ¹å¼]': '[æ¥å¥[ç¡¬ä»¶|è½¯ä»¶]æä½çæ¹å¼]',
        '[ä¾¯æ¹å]éè´æ§]å´åºç®]': '[ä¾¯æ¹å|éè´æ§|å´åºç®]',
        '[èåè¡åç­å¿é¡»è£å¤|éæ©åºåæ¥æ|å³å®æ¹å|è·¯çº¿å|åæ¬å¯è½çæ¿ä»£çº¿è·¯|åå¤å¥½å°å¾|æGPS|éæ©è¡¥ç»å°ç¹|äºåé®å¯å¾é¢çè¡¥åç»å»|é¢è®¢èµ·ç»ç¹çäº¤é|ä½å®¿]ç­': '[èåè¡åç­å¿é¡»è£å¤|éæ©åºåæ¥æ|å³å®[æ¹å|è·¯çº¿|å¯è½çæ¿ä»£çº¿è·¯]|åå¤å¥½[å°å¾|GPS]|éæ©è¡¥ç»å°ç¹|äºåé®å¯å¾é¢çè¡¥åç»å»|é¢è®¢èµ·ç»ç¹ç[äº¤é|ä½å®¿]]ç­',
        'IEC61032å¾7è¯å·11|GB/T16842è¯å·11|GB8898|IEC60065|IEC60598|GB7000|IEC60335|GB4706]ç­æ åè¦æ±': '[IEC61032å¾7è¯å·11|GB/T16842è¯å·11|GB8898|IEC60065|IEC60598|GB7000|IEC60335|GB4706]ç­æ åè¦æ±',
        'ä½ä¸ºå´å»ºæ¿å±ä¹ç¨ç': 'ä½ä¸ºå´å»ºæ¿å±ä¹ç¨',
        'é¡¹ç®ææ |å·¥ç¨è´¨éçç£ç§æä¿¡æ¯]å·¥ä½': '[é¡¹ç®ææ |å·¥ç¨è´¨éçç£|ç§æä¿¡æ¯]å·¥ä½',
        "[è±å­æ¯|çµè¯|å¹³é¢çµè§|ä¿é©ç®±|ä¹¦æ¡|ææ°|æ´è¡£å®¤|æ²å|æ¨è´¨/é¶æ¨å°æ¿|è¡£æ/è¡£æ©±|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|æµ´ç¼¸ORæ·æµ´|å¤éæå¡]": "[è±å­æ¯|çµè¯|å¹³é¢çµè§|ä¿é©ç®±|ä¹¦æ¡|ææ°|æ´è¡£å®¤|æ²å|æ¨è´¨/é¶æ¨å°æ¿|è¡£æ/è¡£æ©±|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|æµ´ç¼¸|æ·æµ´|å¤éæå¡]",
        "[è·å¾[çå­|åå±|å£®å¤§|ä¸ºå¨ç¤¾ä¼æå¡]": "[è·å¾[çå­|åå±|å£®å¤§]|ä¸ºå¨ç¤¾ä¼æå¡]",
        "[[[åºç¨çç©ææ¯|åå·¥|è¯ç©å¶å|]ç¸å³ä¸ä¸å¨æ¥å¶å­¦ç|å¹è®­æºæå­¦ç]": "[[åºç¨çç©ææ¯|åå·¥|è¯ç©å¶å|ç¸å³ä¸ä¸][å¨æ¥å¶å­¦ç|å¹è®­æºæå­¦ç]]",
        "[å¸å§åé¨é¨ä¹é´|æ¿åºåé¨é¨ä¹é´|å¸å§åé¨é¨ä¸æ¿åºåé¨é¨ä¹é´|å¸ç´é¨é¨ä¸ä¹¡ä¹é´|å¸ç´é¨é¨ä¸éä¹é´]çèè´£åå·¥]": "[å¸å§åé¨é¨ä¹é´|æ¿åºåé¨é¨ä¹é´|å¸å§åé¨é¨ä¸æ¿åºåé¨é¨ä¹é´|å¸ç´é¨é¨ä¸[ä¹¡|é]ä¹é´]çèè´£åå·¥",
        "[æ±æ²³|æºªæµ|æ¹æ³|æ°´å¡|æµ·å²¸]ç­æ°´åå²¸è¾¹|å¶æµæ°´å¤]": "[[æ±æ²³|æºªæµ|æ¹æ³|æ°´å¡|æµ·å²¸]ç­æ°´åå²¸è¾¹|å¶æµæ°´å¤]",
        "[æ·±åæ¹é©|æ©å¤§å¼æ¾|å¼è¿å½åå¤[èµé|ææ¯|äººæ]": "[æ·±åæ¹é©|æ©å¤§å¼æ¾|å¼è¿å½åå¤[èµé|ææ¯|äººæ]]",
        "[å¾·å½EVITAââ¡å¨èªå¨å¼å¸æº|CSIâ507SDå¤åæ°çæ¤ä»ª|æ¥æ¬ç¾è½èªå¨çµå¼åº|æ´åææ¯å®¤]|": "[å¾·å½EVITAââ¡å¨èªå¨å¼å¸æº|CSIâ507SDå¤åæ°çæ¤ä»ª|æ¥æ¬ç¾è½èªå¨çµå¼åº|æ´åææ¯å®¤]",
        "[åä¸­|å°å­¦]|æè²": "[åä¸­|å°å­¦]æè²",
        "[éå±ORç¡¬çº¸|å¡æ]": "[éå±|ç¡¬çº¸|å¡æ]",
        "ç¢±[ç¢±2|10|25]mg": "ç¢±[2|10|25]mg",
        "[å¸åº|ç¤¾ä¼çäº§]|": "[å¸åº|ç¤¾ä¼çäº§]",
        "é¿å·è©[ç®å°|åè°]|": "é¿å·è©[ç®å°|åè°]",
        "[æå­|éæ]|[å°è¾¹æ ä¸|[æ¿åå±å|åº­é¢ä¸­]çæ ä¸]": "[æå­|[éæ|å°è¾¹]æ ä¸|[æ¿åå±å|åº­é¢ä¸­]çæ ä¸]",
        "[é©¬åæä¸»ä¹[æ°æè§|å®æè§|åç[æ°æå®æçè®º|æ¿ç­|æ³å¾æ³è§]]": "[é©¬åæä¸»ä¹[æ°æè§|å®æè§]|åç[æ°æå®æçè®º|æ¿ç­|æ³å¾æ³è§]]",
        "åå·¥å§å³äºåç[ææ³|ç»ç»]ä½é£å»ºè®¾|ååæè²è®¡å|ä¸­å¿ç»å­¦ä¹ è®¡å]": "[åå·¥å§å³äºåç[ææ³|ç»ç»]ä½é£å»ºè®¾|ååæè²è®¡å|ä¸­å¿ç»å­¦ä¹ è®¡å]",
        "[[å½å|çå]åå¸[é³å|æèºç]äººå£«": "[å½å|çååå¸][é³å|æèºç]äººå£«",
        "[åè½¬è¤¶æ²|å¹³å§è¤¶æ²ORéæ©æ­å±æ¨è¦æé ä½]": "[åè½¬è¤¶æ²|å¹³å§è¤¶æ²|éæ©æ­å±æ¨è¦æé ä½]",
        "[[ææº|PDA|MP4|ç¬è®°æ¬çµè|å°åæ¥æ¶ç»ç«¯]": "[ææº|PDA|MP4|ç¬è®°æ¬çµè|å°åæ¥æ¶ç»ç«¯]",
        "[å¤§åå¸|ä¸­åå¸|å°åå¸|å°åé]": "[[å¤§|ä¸­|å°]åå¸|å°åé]",
        "[æ°¯æ°°èé¯ç­|ç¹è°±å]æ··åå·é¾]": "[æ°¯æ°°èé¯ç­æè«å|ç¹è°±åæ··åå·é¾]",
        "[æ±æå«çé¢éå»º|æ±æå«çé¢å åºé¡¹ç®|æ±æå°å­¦éå»ºé¡¹ç®|æ±æå¹¼å¿å­éå»ºé¡¹ç®]": "[æ±æå«çé¢[éå»º|å åº]é¡¹ç®|æ±æ[å°å­¦|å¹¼å¿å­]éå»ºé¡¹ç®]",
        "[è´¢æ¿ã[éè|å¶ä»ç»æµ|ç¤¾ä¼åå±]": "[è´¢æ¿|éè|å¶ä»ç»æµ|ç¤¾ä¼åå±]çæåµ",
        "[å¦é¨å¸è¹ä¸åä¼|æ°´è·¯è¿è¾è¡æ¿ä¸»ç®¡é¨é¨ORå¶ä»æå³é¨é¨]": "[å¦é¨å¸è¹ä¸åä¼|æ°´è·¯è¿è¾è¡æ¿ä¸»ç®¡é¨é¨|å¶ä»æå³é¨é¨]",
        "[æ¶è´¹èä¸ä¼ä¸æå³ç[ç»æµ|æ¿æ²»|ç¤¾ä¼|æ¥å¸¸æ´»å¨]èå´åç[è¡ä¸º|éè¦|æåº¦|å¨æº]ç­": "æ¶è´¹èä¸ä¼ä¸æå³ç[ç»æµ|æ¿æ²»|ç¤¾ä¼|æ¥å¸¸]æ´»å¨èå´åç[è¡ä¸º|éè¦|æåº¦|å¨æº]",
        "[[åäº¬|ä¸æµ·|å¤©æ´¥]ç­å°æ¹æ¿åº]|[ä¸­å¤®æ¿åº[æ°æ¿|ç¤¾ä¼ç¦å©|å¤èµç®¡ç]ç­é¨é¨çä¸é¡¹å¨è¯¢é¡¹ç®çé¡¾é®]]": "[[åäº¬|ä¸æµ·|å¤©æ´¥]ç­å°æ¹æ¿åº|ä¸­å¤®æ¿åº][æ°æ¿|ç¤¾ä¼ç¦å©|å¤èµç®¡ç]ç­é¨é¨çä¸é¡¹å¨è¯¢é¡¹ç®çé¡¾é®",
        "åæ¬[ä¸­å½[å¸é¿æµè¯|åå¸å¼ååºæèµç¯å¢è¯ä¼°]ç­å¨åçéè¦é¡¹ç®]]": "ä¸­å½[å¸é¿æµè¯|åå¸å¼ååºæèµç¯å¢è¯ä¼°]ç­éè¦é¡¹ç®",
        "[æºå³è¡æ¿äºå¡ç®¡ç|å¯¹å¤[èç»|åè°|æ¥å¾]|ä¼è®®çç»ç»å®æ|å¯¹å¤ä¿¡æ¯åå¸]]": "[æºå³è¡æ¿äºå¡ç®¡ç|å¯¹å¤[èç»åè°|æ¥å¾]|ä¼è®®çç»ç»å®æ|å¯¹å¤ä¿¡æ¯åå¸]",
        "[DESCåå®¹ä¸°å¯|æ´»å¨å¨é¢]": "[åå®¹ä¸°å¯|æ´»å¨å¨é¢]",
        "[å¦é¨ç¥éå¨æ­¦å¹³çæ´»ç»é¢æ°åå¹´æ¥æ­¦å¹³å¦é¨é´çäº¤æµæ´»å¨|æ°åå¹´æ¥æ­¦å¹³å¦é¨é´çäº¤æµæ´»å¨]": "[å¦é¨ç¥éå¨æ­¦å¹³çæ´»ç»é¢|æ°åå¹´æ¥æ­¦å¹³å¦é¨é´çäº¤æµæ´»å¨]",
        "[å½å[æ­¦æ¯ç|æè²ç]ç[ä¸å®¶|ææ|å­¦è]": "å½å[æ­¦æ¯ç|æè²ç]ç[ä¸å®¶|ææ|å­¦è]",
        "[è°èæ§|æ··åæ§]è¿è§]": "[è°èæ§|æ··åæ§]è¿è§",
        "[åä¸ªä¸åº|ä¸ä¸ç»]ç[éå¥|é¶å¥|éå¥|ä¼ç§èè¹æ°ä½åå¥]å±ä¸åäºä¸ª": "[[ä¸åº|ä¸ä¸ç»]ç[é|é¶|é]å¥|ä¼ç§èè¹æ°ä½åå¥]",
        "[æä½èççç»ç¨åº¦|è¿æä½èççç»ç¨åº¦|è¿éæ¬¡æ°|ç©¿åºéä¸ç©¿åºç¹è¸èåçº¿ä½çéè§åº¦|èºæ°è¿]ç­å ç´ éæ¬¡æ°ãç©¿åºéä¸ç©¿åºç¹è¸èåçº¿ä½çéè§åº¦åèºæ°è¿ç­å ç´ ": "[æä½èççç»ç¨åº¦|è¿éæ¬¡æ°|ç©¿åºéä¸ç©¿åºç¹è¸èåçº¿ä½çéè§åº¦|èºæ°è¿]ç­å ç´ ",
        "ç´æµ1mAçµå[ï¼U1mAï¼|0.75 U1mA|0.75 U1mA]": "[ç´æµ1mAçµåï¼U1mAï¼|0.75 U1mAä¸æ³æ¼çµæµ]",
        "æ®ç¾äººæ¥å[åº·å¤è®­ç»|æºææå»|èä¸å¹è®­|å°±ä¸æå¯¼]|[å¼å±æå|ä½è²æ´»å¨]]": "æ®ç¾äºº[æ¥å[åº·å¤è®­ç»|æºææå»|èä¸å¹è®­|å°±ä¸æå¯¼]|å¼å±[æå|ä½è²]æ´»å¨]",
        "[ç¾åº¦å¬å¸çæç¥è§å|ç¾åº¦å¬å¸çè¿è¥ç®¡ç]": "ç¾åº¦å¬å¸ç[æç¥è§å|è¿è¥ç®¡ç]",
        "[å·éå¬è·¯|æç»µé«éå¬è·¯|ç»µå¹¿é«éå¬è·¯]åæ¥ææéè·¯|ææ¸éè·¯|ææ¸é«éå¬è·¯]": "[å·éå¬è·¯|æç»µé«éå¬è·¯|ç»µå¹¿é«éå¬è·¯|ææéè·¯|ææ¸éè·¯|ææ¸é«éå¬è·¯]",
        "[[åå·¥|å¶é|é¢å|ç®¡é]ç­å¤å¿ä¸å¯å°çåä»¶": "[åå·¥|å¶é|é¢å|ç®¡é]ç­å¤å¿ä¸å¯å°çåä»¶",
        "[å·å¤[ç®¡ç|ç»æµ|æ³å¾|äººåèµæºç®¡ç]ç­æ¹é¢çç¥è¯åè½å]|[è½å¨[äºä¸åä½|æ¿åºé¨é¨]ä»äº[äººåèµæºç®¡ç]|[[æå­¦|ç§ç ]æ¹é¢å·¥ä½ç]å·¥åç®¡çå­¦ç§é«çº§ä¸é¨äººæ": "[å·å¤[ç®¡ç|ç»æµ|æ³å¾|äººåèµæºç®¡ç]ç­æ¹é¢çç¥è¯åè½å|è½å¨[äºä¸åä½|æ¿åºé¨é¨]ä»äº[äººåèµæºç®¡ç|æå­¦|ç§ç ]æ¹é¢å·¥ä½]çå·¥åç®¡çå­¦ç§é«çº§ä¸é¨äººæ",
        "[[éå±±|ä¸­å½é¶è|BBTVç¾è§é|ç§å¤§è®¯é£]ç­åä½ä¼ä¼´]|å¨çè½¯ä»¶å¼åå¤§èµç­è½¯ä»¶å¼åå¹³å°|éå¸å¨ççç åæºæ|æ¶²æ¶é¢æ¿ç­ä¸æ¸¸èµæºä¿é]": "[[éå±±|ä¸­å½é¶è|BBTVç¾è§é|ç§å¤§è®¯é£]ç­åä½ä¼ä¼´|å¨çè½¯ä»¶å¼åå¤§èµç­è½¯ä»¶å¼åå¹³å°|éå¸å¨ççç åæºæ|æ¶²æ¶é¢æ¿ç­ä¸æ¸¸èµæºä¿é]",
        "[[HTTP|TCP|UDPï¼SUDP|RUDPï¼|ç½å³ç©¿éæ¨¡ç»|å¨çIPè¡¨]": "[HTTP|TCP|UDP|SUDP|RUDP|ç½å³ç©¿éæ¨¡ç»|UDPç©¿é|RPNPç©¿é|å¨çIPè¡¨]",
        "å¨å¸[ç»æµ|[èµæº|ç¯å¢]åè°åå±": "å¨å¸[ç»æµ|èµæº|ç¯å¢]åè°åå±",
        "[æ±ç¯|æ±é¶]ç»åçä¹¦æ³ååè½¨è¿¹|[æ±ç¯|æ±é¶|å«åä½]çç¾çé­å]": "[[æ±ç¯|æ±é¶]ç»åçä¹¦æ³ååè½¨è¿¹|[æ±ç¯|æ±é¶|å«åä½]çç¾çé­å]",
        "[å¨å¸åæ¿ç¾¤æºå³ç´å±äºä¸åä½|åé¨é¨æå±äºä¸åä½]ãæºæç¼å¶ç®¡çè¯ã|æ°å¢äººåç[æ§å¶|åç¼]ç­æç»­": "[å¨å¸åæ¿ç¾¤æºå³[ç´å±äºä¸åä½|åé¨é¨æå±äºä¸åä½]ãæºæç¼å¶ç®¡çè¯ã|æ°å¢äººåç[æ§å¶|åç¼]æç»­]",
        "[[âç¨³ç²®|å¢çª|å´ç|æ©ç»|ä¿æ|å¼ºå·¥â]æ»ä½æè·¯": "[ç¨³ç²®|å¢çª|å´ç|æ©ç»|ä¿æ|å¼ºå·¥]æè·¯",
        "[[è½»æ¾|æäº®|åç¡®|åæ¶¦]": "[è½»æ¾|æäº®|åç¡®|åæ¶¦]",
        "[ä¿é©ç®±|ç¨æ|ä¹¦æ¡|ç¨è¡£è®¾å¤|ææ°|æ·æµ´|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´ç¼¸ORæ·æµ´|çµè§|çµè¯|æçº¿é¢é|è¿·ä½ å§|å°ç®±]": "[ä¿é©ç®±|ç¨æ|ä¹¦æ¡|ç¨è¡£è®¾å¤|ææ°|æ·æµ´|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´ç¼¸|æ·æµ´|çµè§|çµè¯|æçº¿é¢é|è¿·ä½ å§|å°ç®±]",
        "[åå°æµæ¼äºé¡¹|æ¿äº§æµæ¼äºé¡¹|è½¦è¾æµæ¼äºé¡¹|è®¾å¤æµæ¼äºé¡¹]": "[åå°|æ¿äº§|è½¦è¾|è®¾å¤]æµæ¼äºé¡¹",
        "[âä¸èµä¼ä¸â|ææ¸¸å¼åé¡¹ç®æä»¶çå®¡æ¥|æ¥æ¹|åè°æå¡]]": "[âä¸èµä¼ä¸â|ææ¸¸å¼åé¡¹ç®æä»¶ç[å®¡æ¥|æ¥æ¹|åè°æå¡]]",
        "ä¸é¨[åå§|ç±æçORå§æç]": "ä¸é¨[åå§|ç±æç|å§æç]",
        "[å£°é³|æ­å±çåå®¹|æ­å±èçé£åº¦ãä»ªè¡¨ãæ°è´¨æ­å±èçé£åº¦ãä»ªè¡¨ãæ°è´¨]": "[å£°é³|æ­å±çåå®¹|æ­å±èç[é£åº¦|ä»ªè¡¨|æ°è´¨]]",
        "[ä¸å¡å¹è®­||çè®ºè°ç |å®£ä¼ |ä¿¡æ¯|åºå±é¢å¤æ¡ææç[æ¶é|æ¥é|å¤åªä½ç¤ºè¯]å·¥ä½": "[ä¸å¡å¹è®­|çè®ºè°ç |å®£ä¼ |ä¿¡æ¯|åºå±é¢å¤æ¡ææç[æ¶é|æ¥é|å¤åªä½ç¤ºè¯]å·¥ä½]",
        "[éåºè½æºç»æä¸­[æ¸æ´è½æº|å¯åçè½æº|æ°è½æº]": "éåºè½æºç»æä¸­[æ¸æ´è½æº|å¯åçè½æº|æ°è½æº]",
        "[æ§è¡STS - 41Cæ¡|å¨å¤ªç©ºä¸­è®°å½äº168ä¸ªå°æ¶ç]ç": "[æ§è¡STS - 41Cæ¡|å¨å¤ªç©ºä¸­è®°å½äº168ä¸ªå°æ¶ç]",
        "[åºæ¬å·ç |ç¹å«å·ç ]|": "[åºæ¬å·ç |ç¹å«å·ç ]",
        "[[è¯å¥½|æ­£ç¡®]ç[åå£°æ¹æ³|åå£°æå·§]": "[[è¯å¥½|æ­£ç¡®]ç[åå£°æ¹æ³|åå£°æå·§]]",
        "ä¸»æºçææç¨æ·ç[æ³¨åå|çå|æåç»å½æ¶é´|ä½¿ç¨shellç±»å]ç­]": "ä¸»æºçææç¨æ·ç[æ³¨åå|çå|æåç»å½æ¶é´|ä½¿ç¨shellç±»å]ç­",
        "[å½å®¶å¤§æ¿æ¹é|[æ¿æ²»|ç»æµ|ç¤¾ä¼çæ´»]]ä¸­çéè¦é®é¢]": "[å½å®¶å¤§æ¿æ¹é|[æ¿æ²»|ç»æµ|ç¤¾ä¼çæ´»]ä¸­çéè¦é®é¢]",
        "[è®¡ç®æºç§å­¦ä¸ææ¯|è½¯ä»¶å·¥ç¨è®¡ç®æºç§å­¦ä¸ææ¯ãè½¯ä»¶å·¥ç¨ãç½ç»å·¥ç¨ãçµå­ä¿¡æ¯å·¥ç¨ãéä¿¡å·¥ç¨ãèªå¨åãä¿¡æ¯ç®¡çä¸ä¿¡æ¯ç³»ç»ç­7ä¸ªæ¬ç§ä¸ä¸|ç½ç»å·¥ç¨|çµå­ä¿¡æ¯å·¥ç¨|éä¿¡å·¥ç¨|èªå¨å|ä¿¡æ¯ç®¡çä¸ä¿¡æ¯ç³»ç»]ç­7ä¸ªæ¬ç§ä¸ä¸": "[è®¡ç®æºç§å­¦ä¸ææ¯|è½¯ä»¶å·¥ç¨|ç½ç»å·¥ç¨|çµå­ä¿¡æ¯å·¥ç¨|éä¿¡å·¥ç¨|èªå¨å|ä¿¡æ¯ç®¡çä¸ä¿¡æ¯ç³»ç»]",
        "[æå±å°æ°æ°æ|åéå°æ°æ°æ]|å°æ°æ°æ[å¦å¥³|å¿ç«¥]ä¿æ¤ç­æå³äºå®": "[æ£æå±å°æ°æ°æ|åéå°æ°æ°æ|å°æ°æ°æ[å¦å¥³|å¿ç«¥]ä¿æ¤]",
        "[ä¸­åæå|æ±æ·®æå|ééµæå|å´æå]": "[ä¸­å|æ±æ·®|ééµ|å´]æå",
        "å¤§é[åè´¹|æ­¦å¨å¼¹è¯]|": "å¤§é[åè´¹|æ­¦å¨å¼¹è¯]",
        "[è°æ´|ç¼å²ORçº¿æ§åå¤ç]": "[è°æ´|ç¼å²|çº¿æ§åå¤ç]",
        "[é¡¹ç®å»ºè®¾ç[ç¨åº|è´¨é|å®å¨|è¿åº¦|èµéä½¿ç¨ï¼ç»ç®ï¼|å³ç®|ç«£å·¥]ç­å¨è¿ç¨": "é¡¹ç®å»ºè®¾ç[ç¨åº|è´¨é|å®å¨|è¿åº¦|èµéä½¿ç¨ï¼ç»ç®ï¼|å³ç®|ç«£å·¥]ç­å¨è¿ç¨",
        "[æ±æ|éå¿äºº|ä¸­å±åå|[ä¸­å½æå§å®¶åä¼ä¼å|æ²³åçæå§å®¶åä¼çäº|æ²³åçèºæ¯åä½ä¸­å¿ç¹çº¦å¯¼æ¼|ä¸é¨å³¡å¸æå§å®¶åä¼ä¸»å¸­|ä¸é¨å³¡å¸æåå±èºæ¯ç§ç§é¿]": "[æ±æ|éå¿äºº|ä¸­å±åå|ä¸­å½æå§å®¶åä¼ä¼å|æ²³åçæå§å®¶åä¼çäº|æ²³åçèºæ¯åä½ä¸­å¿ç¹çº¦å¯¼æ¼|ä¸é¨å³¡å¸æå§å®¶åä¼ä¸»å¸­|ä¸é¨å³¡å¸æåå±èºæ¯ç§ç§é¿]",
        "å¨çæå¤§è§æ¨¡ç[æç´¢å¼æ[è¥é|ä¼å]ä¸ä¸ä¼è®®æç´¢å¼ææç¥å¤§ä¼": "å¨çæå¤§è§æ¨¡çæç´¢å¼æ[è¥é|ä¼å]ä¸ä¸ä¼è®®æç´¢å¼ææç¥å¤§ä¼",
        "[ãé­æ³å°å¥³å¥å¶StrikerS THE COMICSãç¬¬ä¸å·åè¡æ¬æ¼«ç»ï¼æ¥æï¼|ãé­æ³å°å¥³å¥å¶StrikerS THE COMICSãçç¹ä½ä¸­æç]": "ãé­æ³å°å¥³å¥å¶StrikerS THE COMICSã[ç¬¬ä¸å·åè¡æ¬æ¼«ç»ï¼æ¥æï¼|ç¹ä½ä¸­æç]",
        "[æ°æ[å¤ç±|æç©]ç[æ¢æ|æ¶é|æ´ç|åºçè§å]ç­å·¥ä½": "[æ°æ[å¤ç±|æç©]ç[æ¢æ|æ¶é|æ´ç|åºçè§å]ç­å·¥ä½]",
        "[å¤«å¦»ä¹é´ç¸æ¿¡ä»¥æ²«|[ç¶|æ¯]å¥³ä¹é´è¡èç¸è¢­ç[ç¹ç¹æ»´æ»´|ç»å¸ç»é¢]": "[å¤«å¦»ä¹é´ç¸æ¿¡ä»¥æ²«|[ç¶|æ¯]å¥³ä¹é´è¡èç¸è¢­ç[ç¹ç¹æ»´æ»´|ç»å¸ç»é¢]]",
        "[é´å¹³ã[é´ä¸|é´å»|é´å¥|é³å¹³|é³ä¸|é³å»|é³å¥]": "[é´å¹³|é´ä¸|é´å»|é´å¥|é³å¹³|é³ä¸|é³å»|é³å¥]",
        "[The Patriotic Front | PF]": "[The Patriotic Front|PF]",
        "[åå¬åºæ|ç»è¥åºå°]ç[äº§æè¯æ|ç§æ1å¹´ä»¥ä¸çç§èµåå|": "[åå¬åºæ|ç»è¥åºå°]ç[äº§æè¯æ|ç§æ1å¹´ä»¥ä¸çç§èµåå|åæ³çéªèµè¯æ]",
        "å¨è¾¹[6ä¸ªå¿ï¼å¸ï¼|55ä¸ªä¹¡é|600å¤ä¸ªå¤©ç¶æ]": "å¨è¾¹[6ä¸ªå¿ï¼å¸ï¼|55ä¸ªä¹¡é|500å¤ä¸ªå¤©ç¶æ]",
        "é£ç©ç[åæ°|äºå³|å½ç»|é´é³]å±æ§ç­|äººä½çççå¯åç¸å³ç[çè®º|ç»éª]]": "[é£ç©ç[åæ°|äºå³|å½ç»|é´é³]å±æ§|äººä½çççå¯åç¸å³ç[çè®º|ç»éª]]",
        "å[å·¥å§|ç®¡å§ä¼]ç[å°ç« ç®¡ç|æºè¦ä¿å¯]å·¥ä½]": "å[å·¥å§|ç®¡å§ä¼]ç[å°ç« ç®¡ç|æºè¦ä¿å¯]å·¥ä½",
        "[è¿åæ¼èºç|æåå¤«ç|è·å¾æ´å¤§çä¸ªäººåå±ï¼æä¸ºææçè³åå¤«å·¨æ|è·å¾æ´å¤§çä¸ªäººåå±|æä¸ºææçè³åå¤«å·¨æ]": "[è¿åæ¼èºç|æåå¤«ç|è·å¾æ´å¤§çä¸ªäººåå±|æä¸ºææçè³åå¤«å·¨æ]",
        "[ãç¬¬äºé¡¹ä¿®ç¼ãççè®º|ãç¬¬äºé¡¹ä¿®ç¼ãçå¯æä½æ§]": "ãç¬¬äºé¡¹ä¿®ç¼ãç[çè®º|å¯æä½æ§]",
        "[æºå³|äºä¸åä½äººå|ä¼ä¸ç®¡çäººå|ä¸ä¸ææ¯äººå]ç»è®¡|[æºå³|äºä¸]åä½å·¥èµç»è®¡|äººäºä¿¡æ¯ç®¡ç]å·¥ä½": "[[æºå³|äºä¸åä½äººå|ä¼ä¸ç®¡çäººå|ä¸ä¸ææ¯äººå]ç»è®¡|[æºå³|äºä¸]åä½å·¥èµç»è®¡|äººäºä¿¡æ¯ç®¡çå·¥ä½]",
    }
    place_fix_map = {
        "å¨èå·|[å·¥ä¸å­åº|é«æ°åº]": "å¨èå·[å·¥ä¸å­åº|é«æ°åº]",
    }
    qua_fix_map = {
        "å¨ä¸ä¸(ä¼è´¨è¯¾)æ¯èµä¸­": "å¨ä¸ä¸ï¼ä¼è´¨è¯¾ï¼æ¯èµä¸­",
        "å°±éåç¸å³èä¸(å·¥ç§)": "å°±éåç¸å³èä¸ï¼å·¥ç§ï¼",
        "å¨[å­¦ä¸ä½å­¦ææ³]ç­æ¹é¢": "å¨å­¦ä¸[|ä½å­¦ææ³]ç­æ¹é¢",
        "æ ¹æ®ãä¸­å±æ é³åºå§ãæ é³åºäººæ°æ¿åºå³äºå°åãæ é³åºäººæ°æ¿åºæºææ¹é©æ¹æ¡ãçéç¥ã(æ é³å§å[2010]14å·)ç²¾ç¥": "æ ¹æ®ãä¸­å±æ é³åºå§ãæ é³åºäººæ°æ¿åºå³äºå°åãæ é³åºäººæ°æ¿åºæºææ¹é©æ¹æ¡ãçéç¥ãï¼æ é³å§å[2010]14å·ï¼ç²¾ç¥",
        "æ[é¡¹ç®ææ¯æ°´å¹³çé«ä½|ç»æµæççå¤§å°|ç¤¾ä¼æççå¤§å°]": "æ[é¡¹ç®ææ¯æ°´å¹³çé«ä½|[ç»æµæç|ç¤¾ä¼æç]çå¤§å°]",
        "å¨[å®¶å­æ¸¸æ|å®¶å­æ¸¸æç»­ä½]ä¸­": "å¨å®¶å­æ¸¸æ[|ç»­ä½]ä¸­",
        "[å¨æ ¡é¢å¯¼çé«åº¦éè§|å¨ä½å¸ççå±ååªå]]": "å¨[æ ¡é¢å¯¼çé«åº¦éè§|å¨ä½å¸ççå±ååªå]ä¸",
        "å¨[é¶ä¸40åº¦é«æ¸©|è¶ä½æ¸©ç¯å¢]ä¸­|å¨[å¹²ç¥|æ½®æ¹¿|é£å°]ç­åä¸ªç¯å¢ä¸­": "[å¨[é¶ä¸40åº¦é«æ¸©|è¶ä½æ¸©ç¯å¢]ä¸­|å¨[å¹²ç¥|æ½®æ¹¿|é£å°]ç­åä¸ªç¯å¢ä¸­]",
        "ä»ç¾å½ç[æ°ä¸»|å®ªæ³]|ç¾å½ç¤¾ä¼çé®é¢|ç¾å½çç§»æ°åå²|ç¾å½äººççæ´»ä¹ æ¯]": "ä»[ç¾å½ç[æ°ä¸»|å®ªæ³]|ç¾å½ç¤¾ä¼çé®é¢|ç¾å½çç§»æ°åå²|ç¾å½äººççæ´»ä¹ æ¯]",
        "ä»¥[æå­|å½©å¾]]": "ä»¥[æå­|å½©å¾]",
        'ä¸è´¯ä¸è´¯': 'ä¸è´¯',
        "[": "_",
        "ä»¥æç§å­(2006)122å·æ": "ä»¥æç§å­ï¼2006ï¼122å·æ",
        "é¤äºç¼(æ§)éçä»¥åæäºç¹æ®ç¨éççåä»¥å¤": "é¤äºç¼ï¼æ§ï¼éçä»¥åæäºç¹æ®ç¨éççåä»¥å¤",
        "æºä¼ ç»(åä»£)ç»ç»ç ç©¶é¡¹ç®": "æºä¼ ç»ï¼åä»£ï¼ç»ç»ç ç©¶é¡¹ç®",
        "ä»¥()çåä¹": "ä»¥ï¼ï¼çåä¹",
        "å¨å¨å½äº¿åä¹¡(é)ç¤¾ä¼ç»æµåå±ç»éªäº¤æµä¼ä¸": "å¨å¨å½äº¿åä¹¡ï¼éï¼ç¤¾ä¼ç»æµåå±ç»éªäº¤æµä¼ä¸",
    }
    time_fix_map = {
        "å¨()å": "å¨ï¼ï¼å",
        "(12æ11æ¥)": "ï¼12æ11æ¥ï¼",
        "(12æ8æ¥)": "ï¼12æ8æ¥ï¼",
    }
    text_fix_map = {
        "AGM-114åå¦åå¯¼å¼¹å±åå±åºä¸¤ä»£ï¼ä¸ä»£äº982å¹´æäº§ã": "AGM-114åå¦åå¯¼å¼¹å±åå±åºä¸¤ä»£ï¼ä¸ä»£äº1982å¹´æäº§ã",
        "5æ21æ¥ï¼å§æçå¦»å­å¶èäºå¨ä¼æ¯æ¦å½å°å»é¢é¡ºå©äº§ä¸ä¸å¥³ã": "5æ21æ¥ï¼å§æçå¦»å­å¶èäºå¨ä¼æ¯é¡¿å½å°å»é¢é¡ºå©äº§ä¸ä¸å¥³ã",
        "ç§æè¾å¯¼åæ´»å¨è¾å¯¼ç»ç»å¹è®­ï¼ä½¿åæ ¡æå¸åçéæµã": "ç§æè¾å¯¼ååæ´»å¨è¾å¯¼ç»ç»å¹è®­ï¼ä½¿åæ ¡æå¸åçéæµã",
    }

    def clean_arg(txt):

        # txt = re.sub("\s*([\|\[\]])\s*", r"\1", txt)

        # rm blanks
        txt = txt.strip()
        txt = re.sub("([\u4e00-\u9fa5\|\[\]])\s+", r"\1", txt)
        txt = re.sub("\s+([\u4e00-\u9fa5\|\[\]])", r"\1", txt)

        # OR -> |
        txt = re.sub("([^a-zA-Z])OR([^a-zA-Z])", r"\1|\2", txt)

        # redundant characters
        txt = re.sub("\|+", "|", txt)
        txt = txt.strip("_")

        # rm ï¿½
        txt = re.sub("ï¿½", "", txt)
        return txt

    for sample in data:
        text = sample["natural"]

        # if "å½éåç¾åå¹´" in sample["natural"] and "å½éåç¾æ¥" in sample["natural"]:
        #     print("!23")

        for spo in sample["logic"]:

            # fix subject
            if spo["subject"] in subj_fix_map:
                spo["subject"] = subj_fix_map[spo["subject"]]

            # fix predicate
            spo["predicate"] = spo["predicate"].strip()
            if spo["predicate"] in pred_fix_map:
                spo["predicate"] = pred_fix_map[spo["predicate"]]

            # fix object
            new_objs = []
            for obj in spo["object"]:
                if obj in obj_fix_map:
                    new_objs.append(obj_fix_map[obj])
                else:
                    new_objs.append(obj)
            spo["object"] = new_objs

            # fix place
            if spo["place"] in place_fix_map:
                spo["place"] = place_fix_map[spo["place"]]

            # fix time
            if spo["time"] in time_fix_map:
                spo["time"] = time_fix_map[spo["time"]]

            # fix qualifier
            if spo["qualifier"] in qua_fix_map:
                spo["qualifier"] = qua_fix_map[spo["qualifier"]]

            # fix text
            if text in text_fix_map:
                text = text_fix_map[text]
                sample["natural"] = text

            # # special case
            # spe_txts = {
            #     "ãçèè£èãæ¬è´¨ä¸æ¯å¨çæåæ¬¢è¿çãè±éèçã",
            #     "æç«å·¥ä½é¢å¯¼å°ç»ï¼å®è¡é¢å¯¼æé©è´£ä»»å¶ï¼",
            #     "è³ä»å¨èèé´ä»æâä»å°æ¨ªè¡",
            #     "å§æçæ´ä¸ªNBAçæ¶¯é½æ¯å¨",
            #     "å®è£åºå®çµè¯ææ¥æç§»å¨çµè¯çåæ·æ°350æ·ï¼",
            #     "å·²æ¢æå¨éç80å¤ç§ç¿èä¸­ï¼é",
            # }
            # for txt in spe_txts:
            #     if txt in text:
            #         print(">>>")

            if text == 'è³ä»å¨èèé´ä»æâä»å°æ¨ªè¡ï¼ç°å®æºªéæ¥¼è·¯ï¼å·è¡æ¿âç«å¸çæäºï¼å¹¿ä¸ºç§°éã' and spo["object"][0] == '=ç°å®æºªéæ¥¼è·¯':
                spo["object"][0] = 'ç°å®æºªéæ¥¼è·¯'
                spo["predicate"] = "="
            if text == 'ãçèè£èãæ¬è´¨ä¸æ¯å¨çæåæ¬¢è¿çãè±éèçãæ¸¸æçææºçï¼åèç±ç¾å½æ¸¸æå¼ååRiot.Gamesæ¨åºã' \
                    and spo["object"][0] == 'ç±Xæ¨åº':
                spo["predicate"] = 'ç±Xæ¨åº'
                spo["object"][0] = 'Riot.Games'
                spo["place"] = "_"

            if text == 'æç«å·¥ä½é¢å¯¼å°ç»ï¼å®è¡é¢å¯¼æé©è´£ä»»å¶ï¼æ¯ä¸ªæç±ä¸¤åå¯èé¢å¯¼è´è´£ï¼éä¸»è¦é¢å¯¼å®ææä¸å®ææ·±å¥åæè¿è¡æ£æ¥ã' \
                    and spo["object"][0] == 'ç±Xè´è´£':
                spo["predicate"] = 'ç±Xè´è´£'
                spo["object"][0] = 'ä¸¤åå¯èé¢å¯¼'
                spo["time"] = "_"

            if text == 'å§æçæ´ä¸ªNBAçæ¶¯é½æ¯å¨ç«ç®­éåº¦è¿çï¼èº«ç©¿11å·çè¡£çä»åºåä¸åº32.5åéï¼å¯ä»¥å¾å°19.0å9.2ç¯®æ¿1.9çå¸½ã' \
                    and spo["object"][0] == 'å¨Xåº¦è¿':
                spo["predicate"] = 'å¨Xåº¦è¿'
                spo["object"][0] = 'ç«ç®­é'
                spo["time"] = "_"

            if 'å®è£åºå®çµè¯ææ¥æç§»å¨çµè¯çåæ·æ°350æ·ï¼å¶ä¸­æ¥æç§»å¨çµè¯åæ·æ°290æ·' in text \
                    and spo["predicate"] == "å " and spo["object"][0] == 'æ»æ°ç44.14%':
                spo["subject"] = "å®è£åºå®çµè¯ææ¥æç§»å¨çµè¯çåæ·æ°"
            if 'å®è£åºå®çµè¯ææ¥æç§»å¨çµè¯çåæ·æ°350æ·ï¼å¶ä¸­æ¥æç§»å¨çµè¯åæ·æ°290æ·' in text \
                    and spo["predicate"] == "å " and spo["object"][0] == 'æ»æ°ç36.57%':
                spo["subject"] = "æ¥æç§»å¨çµè¯åæ·æ°"
                text = 'å®è£åºå®çµè¯ææ¥æç§»å¨çµè¯çåæ·æ°350æ·ï¼å¶ä¸­æ¥æç§»å¨çµè¯åæ·æ°290æ·ï¼åå«å æ»æ°ç44.14%å36.57%ï¼ã'
                sample["natrual"] = text

            if text == 'å·²æ¢æå¨éç80å¤ç§ç¿èä¸­ï¼éçå¨éå±ä¸çé¦ä½ï¼é¨ãéãé·ãé°ãéãéãéä»¥åééå±éé»ãè¤ç³ãæµ·æ³¡ç³ãç¬å±ç³ãéåç³ç­å±ä¸­å½ååã' and \
                    spo["time"] == "80å¤ç§ç¿è":
                spo["time"] = "_"
                spo["predicate"] = 'æ¢æXçå¨é'
                spo["object"][0] = '80å¤ç§ç¿è'
            if text == 'å·²æ¢æå¨éç80å¤ç§ç¿èä¸­ï¼éçå¨éå±ä¸çé¦ä½ï¼é¨ãéãé·ãé°ãéãéãéä»¥åééå±éé»ãè¤ç³ãæµ·æ³¡ç³ãç¬å±ç³ãéåç³ç­å±ä¸­å½ååã' and \
                    spo["predicate"] == 'éçå¨é':
                spo["subject"] = 'éçå¨é'
                spo["predicate"] = 'å±ä¸çé¦ä½'
                spo["object"][0] = "_"
            if text == 'å·²æ¢æå¨éç80å¤ç§ç¿èä¸­ï¼éçå¨éå±ä¸çé¦ä½ï¼é¨ãéãé·ãé°ãéãéãéä»¥åééå±éé»ãè¤ç³ãæµ·æ³¡ç³ãç¬å±ç³ãéåç³ç­å±ä¸­å½ååã' and \
                    spo["predicate"] == 'éåç³çå¨é':
                spo["subject"] = 'éåç³çå¨é'
                spo["predicate"] = 'å±ä¸­å½åå'
                spo["object"][0] = "_"

            if text == 'é´é¨å¼å£äºèç¶ç£èä¸ï¼è·å°¾ç«¯2ï¼700â3ï¼262 mmã' and spo["predicate"] == 'è·X2ï¼700Y mm':
                spo["predicate"] = 'è·XY'
                spo["object"][1] = "2ï¼700â3ï¼262 mm"

            if text == 'æ¸¡èè°·SEOå¤§èµæä¸äºç¹ç¹ï¼ï¿½èµ·æ¹æ¯å¨çæå¤§è§æ¨¡çæç´¢å¼æè¥éåä¼åä¸ä¸ä¼è®®æç´¢å¼ææç¥å¤§ä¼ï¼Search Engine Strategiesï¼çä¸­å½ä¸»åæ¹æµ©ç»´ä¼ åªï¼ä»¥å¼å¯¼SEOæå¡è¯æ§åå±ä¸ºç®çï¼å¬åæ§åä¸ä¸æ§é½æ¯è¾é«ï¼' \
                    and spo["subject"] == 'ï¿½èµ·æ¹':
                text = re.sub("ï¿½èµ·æ¹", "åèµ·æ¹", text)
                sample["natural"] = text
                spo["subject"] = 'åèµ·æ¹'

            if text == 'åå¦ç»£ç³å¤´ãèæ æ¢ç­ï¼çº¿ç²ï¼æéä¸å¿è¿äºååã' and spo["predicate"] == "æé" and spo["object"][0] == 'ä¸å¿è¿äºåå':
                spo["subject"] = "æé"
                spo["predicate"] = 'ä¸å¿è¿äºåå'

            if text == 'ä¿¡æ¯ç®¡çâæ½å·¥é¡¹ç®ç®¡çæ¯ä¸é¡¹å¤æçç°ä»£åçç®¡çæ´»å¨ï¼æ´è¦ä¾é å¤§éçä¿¡æ¯ä»¥åå¯¹å¤§éä¿¡æ¯çç®¡çï¼å¹¶åºç¨çµå­è®¡ç®æºè¿è¡è¾å©ã' and \
                    spo["object"][0] == 'å¤§éçä¿¡æ¯|å¯¹å¤§éä¿¡æ¯çç®¡ç':
                spo["subject"] = "æ½å·¥é¡¹ç®ç®¡ç"
                spo["object"][0] = '[å¤§éçä¿¡æ¯|å¯¹å¤§éä¿¡æ¯çç®¡ç]'

            if text == 'è¿ç§è§ç¹çå¤§å¤æ°æ¯å·æä½¿å½æçèµæ¬å®¶ï¼è¿æä¸»å¼ è¿è¡èªç±å¼æ°ä¸»æ¹é©çäººã' and spo["object"][0] == 'èµæ¬å®¶|':
                spo["object"][0] = '[èµæ¬å®¶|ä¸»å¼ è¿è¡èªç±å¼æ°ä¸»æ¹é©çäºº]'
            if text == 'è¿ç§è§ç¹çå¤§å¤æ°æ¯å·æä½¿å½æçèµæ¬å®¶ï¼è¿æä¸»å¼ è¿è¡èªç±å¼æ°ä¸»æ¹é©çäººã' and spo["predicate"] == 'ä¸»å¼ è¿è¡':
                spo["subject"] = 'äºº'

            if text == "åææèµåªéè¦ä¼ ç»åç¼©æºç©ºè°çä¸åï¼ä¸­æè¿è¡èçµéåªéè¦ä¼ ç»ç©ºè°ç[/b]1/8[/b]ââ[/b]1/10[/b]ï¼åæç»´æ¤è´¹ç¨ä½ã" and spo[
                "predicate"] == "åªéè¦Xç[/b]1/8[/b]ââ[/b]1/10[/b]":
                text = "åææèµåªéè¦ä¼ ç»åç¼©æºç©ºè°çä¸åï¼ä¸­æè¿è¡èçµéåªéè¦ä¼ ç»ç©ºè°ç1/8ââ1/10ï¼åæç»´æ¤è´¹ç¨ä½ã"
                sample["natural"] = text
                spo["predicate"] = "åªéè¦Xç1/8ââ1/10"

            if text == 'å¨çæ·ç±äººå£ä¸º70780918äººã' and spo["object"][0] == '79780918äºº':
                spo["object"][0] = '70780918äºº'

            if text == "å¶ä¸­ï¼60ã¡åä¸­å°æ·åæ´»è·ï¼å æ¯è¶è¿40%ï¼å¢å çº¦6%ï¼ç½æ¹ãçç°å æ¯è¶è¿5æï¼ç¦ç°å¨5æå·¦å³ã" and spo["time"] == "4æ":
                spo["time"] = "_"

            if text == "è¿æ­è½½äºMOTOBLURæå¡ï¼è¿æ¯ä¸ä¸ªèªæçäº¤äºèååºç¨ã" and spo["predicate"] == "MOTOBLURæå¡":
                spo["predicate"] = "æ­è½½äº"
                spo["object"] = ["MOTOBLURæå¡", ]
                spo["subject"] = "_"

            if text == "2017å¹´1æ3æ¥æé´ï¼ç±ä¸­å¤®çºªå§å®£ä¼ é¨ãä¸­å¤®çµè§å°èåå¶ä½ççµè§ä¸é¢çãæéè¿éèªèº«ç¡¬ãå°å¨ä¸­å¤®çµè§å°ç»¼åé¢éé¦æ­ã" and \
                    spo["subject"] == "ãæéè¿éèªèº«ç¡¬ãå°å¨Xé¦æ­" and spo["predicate"] == "ä¸­å¤®çµè§å°ç»¼åé¢é":
                spo["subject"] = "ãæéè¿éèªèº«ç¡¬ã"
                spo["predicate"] = "å°å¨Xé¦æ­"
                spo["object"][0] = "ä¸­å¤®çµè§å°ç»¼åé¢é"
                spo["time"] = "2017å¹´1æ3æ¥æé´"

            if text == "2ãæ¢äºæ´ï¼è¯äººæ¢åºä¹å­ï¼æ¢ç«æ¬çç¶äº²ï¼ä¹¾éå­å¹´ï¼1741ï¼ä¸¾äººï¼æ¬¡å¹´ä¸­è¿å£«ï¼æ­¥å¥ä»éï¼å®å¾å·ææã" \
                    and spo["time"] == "1742":
                spo["time"] = "1741"
            if text == "ä½æ³ï¼éç¨è±çä»ãç³ç²ãç½ç ç³åç²¾ç½é¢ç²æ··åçå¶èæã" and spo["subject"] == "éç¨Xæ··åçå¶èæ" \
                    and spo["predicate"] == "ç²¾ç½é¢ç²":
                spo["predicate"] = "éç¨Xæ··åçå¶èæ"
                spo["object"][0] = "[è±çä»|ç³ç²|ç½ç ç³|ç²¾ç½é¢ç²]"
                spo["subject"] = "_"

            if text == "1998å¹´å¸å§å®£ä¼ é¨ãå¸æèæäºâä¸é¨å³¡å¸åä½³æèºå·¥ä½èâç§°å·ï¼1999å¹´å¸æ¿åºæäºâä¸é¨å³¡å¸å³å¨æ¨¡èâç§°å·ã" and \
                    spo["subject"] == "[]" and spo["predicate"] == "_" and spo["qualifier"] == "_" and spo[
                "place"] == "_" \
                    and spo["time"] == "_" and spo["object"][0] == "_":
                spo["subject"] = "[å¸å§å®£ä¼ é¨|å¸æè]"
                spo["predicate"] = "æäºXç§°å·"
                spo["object"] = ["âä¸é¨å³¡å¸åä½³æèºå·¥ä½èâ", ]
                spo["time"] = "1998å¹´"
                sample["logic"].append({
                    "subject": "å¸æ¿åº",
                    "predicate": "æäºXç§°å·",
                    "object": ["âä¸é¨å³¡å¸å³å¨æ¨¡èâ", ],
                    "time": "1999å¹´",
                    "place": "_",
                    "qualifier": "_",
                })

            if spo["predicate"] == 'åºäºXå¯¹çå¯¹è±¡' and spo["object"][0] == "è¦ä¿æ¤ç":
                spo["predicate"] = 'åºäºXçå¯¹è±¡'
                spo["object"] = "è¦ä¿æ¤"

            if text == "è¿ç§æ¹æ³è½ç¶æ¯ä¼ä¸å¨è¥éç¯å¢ä¸­è¿è¡çï¼ä½åä¸æ¯çº¯èªç¶çï¼æ¯äººä»¬æ ¹æ®è°æ¥ç®çä¸»å¨å°ãæç®çå°æ½å ä¸äºå½±åï¼æä»¥ï¼è¿ç§æ¹æ³å¾å¾è½å¤æç§ç ç©¶ç®çåå¾æ¯è¾åç¡®ãææçèµæï¼æ¯åºç¨èå´æ¯è¾å¹¿æ³çæ¹æ³ã" and \
                    spo["subject"] == "å¨Xè¿è¡":
                spo["subject"] = "ä¼ä¸"

            if text == "è¥¿çº¢æ¿å¤æ±ï¼å¯ä»¥å©å°¿ï¼è¾ççäººä¹å®é£ç¨ã" and \
                    spo["predicate"] == "è¥¿çº¢æ¿" == spo["subject"]:
                spo["predicate"] = "å¤"
            if text == "å å¼ºåçº§æ®ç¾äººè¾å©å¨å·æå¡ä¸­å¿ï¼ç«ï¼å»ºè®¾ï¼æ¨è¿è¾å©å¨å·æå¡è¿ç¤¾åºãå°å®¶åº­ï¼ä¾åºéåè¾å©å¨å·9.2ä¸ä»¶ã" and \
                    spo["predicate"] == "è¾å©å¨å·æå¡" == spo["subject"]:
                spo["predicate"] = "å°"
            if text == "å¹¶ä¸ç±äºå¯¹å¼éå¾·å°çæ·±å¥äºè§£ï¼ä»æ¯è°é½æ´çå¾éã" and \
                    spo["predicate"] == "ä»" == spo["subject"]:
                spo["predicate"] = "æ¯è°é½æ´çå¾é"
            if text == "è¯¥æå°ä¹¡éè·¯ä¸ºåè·¯ï¼äº¤éä¸æ¹ä¾¿ï¼" and \
                    spo["predicate"] == "è¯¥æå°ä¹¡éè·¯" == spo["subject"]:
                spo["predicate"] = "ä¸º"
            if text == "èµµå·è¯´â¶ãå ä¸ºå®æä¸è¯å¨ã" and \
                    spo["predicate"] == "å®" == spo["subject"]:
                spo["predicate"] = "æXå¨"

            if text == "æ¬åï¼æªå¬å¼ï¼çåäºå¡æçæ¯ä¾ï¼æä¸èºäººåºæ¬é½ç¨èºåï¼" and \
                    spo["predicate"] == "é½ç¨" == spo["object"][0]:
                spo["object"][0] = "èºå"
            if text == "âè¯ä¿¡ä¸ºæ¬ï¼è´¨éä¸ºæ ¹âï¼åæ¬æä¸­ååå¤ä¸åçä¼ ç»ç¾å¾·ï¼" and \
                    spo["predicate"] == "ä¸º" == spo["object"][0]:
                spo["object"][0] = "æ ¹"
            if text == "å¨æ¥æ¬å´æ£æå±é¾çæ¶åï¼æ¯ä¾ç°å±¡æ¬¡æå«æ¥æ¬å´æ£çå°ä¸¥ï¼åææè¡¡å¤ç±æ£æï¼å¤æåæä¸­é©åå¢ã" and \
                    spo["predicate"] == "å±¡æ¬¡æå«" == spo["object"][0]:
                spo["object"][0] = "æ¥æ¬å´æ£çå°ä¸¥"
            if text == "å¨æ¥æ¬å´æ£æå±é¾çæ¶åï¼æ¯ä¾ç°å±¡æ¬¡æå«æ¥æ¬å´æ£çå°ä¸¥ï¼åææè¡¡å¤ç±æ£æï¼å¤æåæä¸­é©åå¢ã" and \
                    spo["predicate"] == "åå¸" == spo["object"][0]:
                spo["object"][0] = "_"
                spo["qualifier"] = "å¯é"
            if text == "å¨æ¥æ¬å´æ£æå±é¾çæ¶åï¼æ¯ä¾ç°å±¡æ¬¡æå«æ¥æ¬å´æ£çå°ä¸¥ï¼åææè¡¡å¤ç±æ£æï¼å¤æåæä¸­é©åå¢ã" and \
                    spo["predicate"] == "äººå£" == spo["object"][0]:
                spo["object"][0] = "_"
                spo["qualifier"] = "é¼ç"

            if text == "3ï¼æ­å±ä¸­ç¹å«å¼ºè°æ°æ¯çæ§å¶ï¼å¼ºè°ï¼è¿è´¯æ§ï¼åé³è²çä¼ç¾ï¼è¦æ±æ­å±ä¸­è¯­æ°å¯äºååï¼ææè¡¨è¾¾çæã" and \
                    spo["predicate"] == "è¦æ±X":
                spo["object"] = ["[è¯­æ°å¯äºåå|ææè¡¨è¾¾çæ]"]
            if text == "åç¦»åºç¨å±åé¢åå±æå©äºå¯¹é¢åæ¨¡åçæ½è±¡åä¸æ­ç²¾åï¼ä¹æå©å®æ½äººåå¿«éæå»ºæè°æ´äº§åï¼ä»¥æ»¡è¶³ä¼ä¸åå±ååçç®¡çéè¦ã" and \
                    spo["predicate"] == "æå©Xå¿«é[æå»º|è°æ´]Y":
                spo["object"] = ["å®æ½äººå", "äº§å"]
            if text == "æ±æ­¦å¸ç»æä¹ä¸å¹´ï¼å³å¬åå138å¹´ï¼âæ²³æ°´æº¢äºå¹³åï¼å¤§é¥¥ï¼äººç¸é£âçäºå®ï¼å·²åºç°äºå®æ¹çºªå½ã" and \
                    spo["predicate"] == "=å¬åå138å¹´":
                spo["predicate"] = "="
                spo["object"] = ["å¬åå138å¹´"]
            if text == "åå§å§åãå¯éé¿è°­æå°ï¼" and spo["subject"] == "è°­æå°" and spo["object"][0] == "[åå§å§å|å¯éé¿]":
                spo["predicate"] = "ISA"
            if text == "'èå¯¹äºåç¦æå¹³çäººæ¥è¯´ï¼æ´éè¦åçèèªæ¸¸ç§»çåçï¼å°èä¸ãåèãå°è¹çèèªéä¸­åè£¹å¨è¸é¨ï¼å°å¤§è¿æ ¹é¨ãå¤ä¾§çèèªä¸æåºå®å¨èé¨ï¼åé åºç²çæè´çèº«æã" and \
                    spo["predicate"] == "å°Xéä¸­Y":
                spo["predicate"] = "å°Xéä¸­åè£¹Y"
                spo["object"] = ['[èä¸|åè|å°è¹]çèèª', 'å¨è¸é¨']
            if text == 'å¿«æ¿èµ·æä¸­çæ­¦å¨åä½ çå°ä¼ä¼´åå æ å¤ºèµæºçææä¸­å§ï¼' and spo["predicate"] == "å¿«æ¿èµ·" == spo["object"][0]:
                spo["object"][0] = "æä¸­çæ­¦å¨"
            if text == "ä»¥æ±ææ¬èé¢åééä¸»å¹²éæ³°å·è·¯ä»¥åæ¯çº¿è¡éå»ºè®¾ä¸ºä¸»è¦åå®¹çç¬¬äºææ´å»ºé¡¹ç®ç¡®å®ï¼è®¾è®¡æ¹æ¡å·²ç»éè¿ä¸å®¶å®¡æ¥ï¼å·¥ç¨å¯æ11æä¸­æ¬å¼å·¥ã" \
                    and spo["subject"] == "ç¬¬äºææ´å»ºé¡¹ç®" == spo["object"][0]:
                spo["object"][0] = "_"
            if text == "ç±äºGPSææ¯æå·æçå¨å¤©åãé«ç²¾åº¦åèªå¨æµéçç¹ç¹ï¼ä½ä¸ºåè¿çæµéææ®µåæ°ççäº§åï¼å·²ç»èå¥äºå½æ°ç»æµå»ºè®¾ãå½é²å»ºè®¾åç¤¾ä¼åå±çåä¸ªåºç¨é¢åã" \
                    and spo["subject"] == "çäº§å" == spo["object"][0]:
                spo["object"][0] = "æ°ç"
            if text == "è¿ç§ææ³å½æ¶å¨ç©ççä¸ä½æ®éå­å¨ï¼èä¸ç±æ¥å·²ä¹ã" \
                    and spo["subject"] == "è¿ç§ææ³" == spo["object"][0]:
                spo["object"][0] = "[æ®éå­å¨|ç±æ¥å·²ä¹]"
            if text == "å ä¸ºç¼ºä¹å¹¼åï¼æä»¥å¨ä»é«åºå¥åçä¹å­é«éµç¬åçæå¯¼ä¸æ¯å¤©ç»åã" \
                    and spo["subject"] == "é«éµç¬" == spo["object"][0]:
                spo["object"][0] = "é«åºå¥åçä¹å­"
            if text == "å»ºç­å¤æ´éå¥ï¼æ¯æµæ±çå¾·æ¸å¿åä¿å­æå®æ´çå¤ä»£æ¡¥æ¢ï¼ç°ä¸ºççº§æç©ä¿æ¤åä½ã" \
                    and spo["subject"] == "å¾·æ¸å¿" == spo["object"][0]:
                spo["object"][0] = "æµæ±ç"
            if text == "ç°å¨ç¦»æå®¶å·²ç»ä¸è¿äºï¼ä½æ¯è¦å¨äºåéä¹åå°è¾¾ï¼å°±è¦æè¿è·¯ç©¿è¿ä¸ä¸ªå¤§åçåè½¦åºã" \
                    and spo["subject"] == "æå®¶" == spo["object"][0]:
                spo["subject"] = "_"
            if text == "è¥ç±äºå¡çæ¬èº«è´¨éé®é¢é æçæåï¼å·¥ä½äººåå¨ç¥¨é¢ä¸è¿è¡æ æ³¨ï¼ä¹å®¢å¨æææåç±è½¦ç«å·¥ä½äººåå¼å¯¼ä»ä¸ç¨ééè¿åºç«ã" \
                    and spo["subject"] == "è´¨éé®é¢" == spo["object"][0]:
                spo["object"][0] = "å¡çæ¬èº«"
            if text == "çæ°´å¹³ï¼ææçè¿äºçæå©äºèªç¶åå¨©çæ¯äº²åçäº§çå©´å¿ï¼å©´å¿çå¿è·³çå°ä¿æå¨ä¸ä¸ªæ­£å¸¸çèå´åï¼ç±äºå­å®«çéå½æ¶ç¼©åï¼åæ¶å ä¸è¯å¥½çæ°§æ°åè¡æ¶²ä¾åºç»åçæ¯äº²çè¯å¥½æè§ï¼å©´å¿ä¼æ¯è¾èéï¼åå¨©çæ¯ä¸é¶æ®µäº§éçæ©å¼ åæ¨è¿é½å¾é¡ºçï¼å©´å¿è½ä»¥è¯å¥½çä½ç½®æè½¬å¾ä¸è¿å¥äº§éèåºçï¼çäº§æ´å çèªå¨ï¼éè¿æ¾å¼çä¼é´ï¼èä½¿å¾æ¯äº²çèº«ä½ä¸ä¼äº§çç»ç»ãå¨å®ãèèä¸å¿è¦çæä¼¤ï¼ç±äºå¯ä»¥æè½¬ï¼ç¹å«æ¯ä¸ä¼ä¼¤åå©´å¿çå¤´é¨åèº«ä½ã" \
                    and spo["subject"] == "ä¼é´" == spo["object"][0]:
                spo["object"][0] = "æ¾å¼ç"
            if text == "proav éå¯¹ä¸ä¸å½±é³åºç¨ï¼åæ¬æ¼æ­å®¤è®¾å¤ãè§é¢å¼å³ãå½é³æ£ãå¤§åæå½±ä»ªãæ°å­ä¿¡æ¯åå¸ç³»ç»ãéæå¼å®¶åº­å½±é¢ãæè²åºæåç¤¼æå ç­ï¼adiå¬å¸æ¥æä¸çæé½å¨çicè§£å³æ¹æ¡ã" \
                    and spo["subject"] == "ä¸çæé½å¨ç" == spo["object"][0]:
                spo["subject"] = "icè§£å³æ¹æ¡"
            if text == "ï¼3ï¼ä¼ ç»èæ¥ï¼å¦è±æèï¼æ¸æèï¼ç«¯åèï¼ä¸­ç§èç­ï¼ååæç©¿æ±æåºæ¸¸ï¼æè£ç­ç±å¤èé¨è´è´£èç³»èµå©ï¼ï¼ä¸¾è¡ç¸åºçä¼ ç»ä»ªå¼ï¼å¦ç«¯åç¥­å±åï¼ä¸­ç§ææç­ï¼" \
                    and spo["subject"] == "å¤èé¨" == spo["object"][0]:
                spo["subject"] = "èç³»èµå©"
            if text == "2011å¹´ï¼è£è·âç¬¬äºå±ä¸­å½æºæ¢°å·¥ä¸ä¼ç§ä¼ä¸å®¶âè£èªç§°å·ï¼" \
                    and spo["subject"] == "âç¬¬äºå±ä¸­å½æºæ¢°å·¥ä¸ä¼ç§ä¼ä¸å®¶â" == spo["object"][0]:
                spo["object"][0] = "ç¬¬äºå±"
            if text == "æ è®ºæ¯ççº¯ç´ èãè¤èæè¤ç´ æ­éçèï¼å¯å°åå¤å¥½çä¸»æãéæåä½æå¨é¨ä¸æ¬¡æå¥ï¼å çå¹¶è®¾å®ç¨åºåï¼çèè¿ç¨èªå¨è¿è¡ï¼æ é¡»ä¸äººç¿»çæç§çï¼æ éç»éªï¼ç°å­¦ç°ä¼ã" \
                    and spo["subject"] == "çèè¿ç¨" == spo["object"][0]:
                spo["object"][0] = "_"
            if text == "å¶é ä¸å·¥äººå·¥èµçå¿«éä¸æ¶¨åææå³å¨åçéå¹´åå°ï¼æ­£éæ¸æ¹å30å¹´æ¥ä¸­å½ç»æµèµä»¥é«éå¢é¿çåºç¡ââå¤§éå»ä»·å³å¨åï¼äººå£çº¢å©éæ¸æ¶å¤±çé®é¢æ¯ä¸­å½ä¼ä¸ä¸å¾ä¸ç´è§çä¸ä¸ªä¸¥å³»é®é¢ã" \
                    and spo["subject"] == "äººå£çº¢å©éæ¸æ¶å¤±" == spo["object"][0]:
                spo["object"][0] = "ä¸¥å³»é®é¢"
            if text == "æ»èçå¤å±ï¼æ«éå½¢ï¼è¾¹ç¼æåºç¶ç¼æ¯ï¼å¤å±ç»¿è²ï¼è´¨ç¡¬èå¤å¼¯ï¼åå±ç´«çº¢è²ï¼å¼å±æç´ç«ï¼åç«¯å·å¾®æ¯ï¼" \
                    and spo["subject"] == "å¤å±" == spo["object"][0]:
                spo["object"][0] = "ç»¿è²"
            if text == "å¨2009å¹´1æ18æ¥çé¢é²å®«é¢çç¾çè¾¹åä¸äººæ®æ¥å¤§åå¬çæ´»å¨æ»ç»å¤§ä¼ä¸ï¼èªæ²»åºäººå¤§å¸¸å§ä¼å¯ä¸»ä»»æç§¦çæåºï¼âè¿æ¬¡æ®æ¥æ´»å¨å¨æ°çå»çå«çäºä¸ä¸æ¯å²æ åä¾çï¼å¨å¨å½ä¹èµ°å¨äºåé¢ã" \
                    and spo["subject"] == "æç§¦ç" == spo["object"][0]:
                spo["object"][0] = "èªæ²»åºäººå¤§å¸¸å§ä¼å¯ä¸»ä»»"
                spo["predicate"] = "ISA"
            if text == "æ ¹æ®çäººç¤¾åãè½¬åäººåèµæºç¤¾ä¼ä¿éé¨åå¬åå³äºå½å®¶åºæ¬å»çä¿é©ãå·¥ä¼¤ä¿é©åçè²ä¿é©è¯åç®å½ä¸­é¨åè¯åè¿è¡è°æ´è§èçéç¥ãï¼ç²¤äººç¤¾å½ã2013ã1252å·ï¼è¦æ±ï¼ç»åæå¸å®éå¯¹é¨åè¯åè¿è¡è°æ´è§èï¼æ¯å¦è¥¿è¯é¨åç¬¬1022å·âéç»äººçº¢ç»èçæç´ ï¼éç»äººä¿çº¢ç´ ï¼âä¿®æ¹ä¸ºâéç»äººçº¢ç»èçæç´ ï¼CHOç»èï¼âï¼è±æåç§°ä¿®æ¹ä¸ºâRecombinant Human Erythropoietinï¼CHO cellï¼âã" \
                    and spo["subject"] == "éç»äººçº¢ç»èçæç´ " == spo["object"][0]:
                spo["object"][0] = "éç»äººçº¢ç»èçæç´ ï¼CHOç»èï¼"
                spo["subject"] = "éç»äººçº¢ç»èçæç´ ï¼éç»äººä¿çº¢ç´ ï¼"
            if text == "ç¬¬ååæ¡è¥ä¸æ§è¿è¾è¹è¶åéè¥ä¸æ§è¿è¾è¹è¶ä¸´æ¶ä»äºè¥ä¸æ§è¿è¾ï¼æªæè§å®åçãè¥è¿è¯ãçï¼åãè¥è¿è¯ãè¢«åæ£åä»ç»§ç»­è¥è¿çï¼ç±å¸æ°´è·¯è¿è¾è¡æ¿ä¸»ç®¡é¨é¨æå¸æ°´è·¯è¿è¾è¡æ¿ä¸»ç®¡é¨é¨å§æçå¸æ°´è·¯è¿è¾ç®¡çå¤è´£ä»¤å¶åæ­¢è¥è¿ï¼å¹¶æè¿æ³æå¾å¤ä»¥2åç½æ¬¾ï¼ç½æ¬¾æé«éé¢ä¸å¾è¶è¿3ä¸åã" \
                    and spo["subject"] == "å¸æ°´è·¯è¿è¾è¡æ¿ä¸»ç®¡é¨é¨å§æç" == spo["object"][0]:
                spo["subject"] = "å¸æ°´è·¯è¿è¾ç®¡çå¤"
            if text == "å¼ æ©ç§åæéåè¿ä¸¤ä½ååå»èçä¼ä¸é«å±äººç©é½åFCPAæå³ï¼åºå«ä¹å¤ä»å¨äºï¼å¼ æ©ç§æ¯ä½ä¸ºå¯è½çåè´¿æ¹åå°å¤ç½ï¼èæéååæ¯ä½ä¸ºå¯è½çè¡è´¿æ¹åå°æ©ç½ã" \
                    and spo["subject"] == "ä¸¤ä½ååå»èçä¼ä¸é«å±äººç©" == spo["object"][0]:
                spo["subject"] = "[å¼ æ©ç§|æéå]"
                spo["object"][0] = "å»èçä¼ä¸é«å±äººç©"
            if text == "æäººè¯´ï¼âä¸­å½æ²¡æåå¤ï¼åå¤ä¸å¨ä¸­å½âï¼è½æ¯æ¤æ¨ä¹è¯­ï¼èå¯¹äºåå¤ææåçå°çä½æ°ä»¬ï¼åæä½è¨è¯­è¾©é©³å¢ï¼" \
                    and spo["subject"] == "ä½æ°ä»¬" == spo["object"][0]:
                spo["object"][0] = "åå¤ææåçå°ç"
            if text == "æ²³åæèºåºçç¤¾è¿æ¬¡è¿è¡æ°çå¼æï¼å¿å°ç»æ²³åçåºçäºä¸å¸¦æ¥æ°æï¼å¦æè¿ä½å¾å¥½ï¼ä¹ä¼å¸¦æ¥æåä¸ç»æµçåæçã" \
                    and spo["subject"] == "å¼æ" == spo["object"][0]:
                spo["object"][0] = "æ°ç"
            if text == "èªä»å»å¹´åå¸ä»¥æ¥ï¼è¾è®¯åºåçææ¸¸ãçèè£èãæ¯æå¹³åæ°å¢500ä¸æ¥æ´»è·ç¨æ·ï¼ç®åå·²ç»è¾¾å°5000ä¸ï¼ç­åº¦è¶è¿ä»»å¤©å çãå£è¢å¦æªGoãã" \
                    and spo["subject"] == "ãå£è¢å¦æªGoã" == spo["object"][0]:
                spo["subject"] = "ãçèè£èã"
            if text == "æ³¨ï¼å¶ä¸­ å±¿å¤´ââé»å²© 8å ï¼è½¦ç¨1å°æ¶å·¦å³ï¼ï¼å¯ä½¿ç¨å°å·å¬äº¤ICå¡ã" \
                    and spo["subject"] == "å±¿å¤´ââé»å²©" == spo["object"][0]:
                spo["subject"] = "å±¿å¤´"
                spo["object"][0] = "é»å²©"
                spo["predicate"] = "ââ"
            if text == "ä»¥èº«ç¯é©ç»ä¸´ç»é¡¶ï¼çºæè½è½èå±±ï¼å¹³ç´è¡åéï¼ç¹å¦ä¸å¹æ ç©·å°½çå¤©ç¶ç»å·ï¼å±ç¤ºäºè«è«å¤§å°ã" \
                    and spo["subject"] == "èå±±" == spo["object"][0]:
                spo["object"][0] = "è½è½"
            if text == "ä»¥èº«ç¯é©ç»ä¸´ç»é¡¶ï¼çºæè½è½èå±±ï¼å¹³ç´è¡åéï¼ç¹å¦ä¸å¹æ ç©·å°½çå¤©ç¶ç»å·ï¼å±ç¤ºäºè«è«å¤§å°ã" \
                    and spo["subject"] == "å°åº¦åºå­éåº" == spo["object"][0]:
                spo["object"][0] = "åä½©å°"
            if text == "ä½ä¸ºæ°åºçè¯äººï¼èè½¼å¼è¾äºè±ªæ¾è¯é£ï¼åæ°åºè¯äººè¾å¼ç¾å¹¶ç§°ä¸ºâèè¾âã" \
                    and spo["subject"] == "è¾å¼ç¾" == spo["object"][0]:
                spo["object"][0] = "è¯äºº"
            if text == "åå¼ æ©ç§ç¸åï¼å¨è¢«è¿«å»èä¹åï¼å°æ¹¾äººæéåä¹å¨èªå·±çèä¸çæ¶¯ä¸­è·å¾äºä¸ä¸ªä»¤äººè³ç¾¡çå°ä½ââæè®¯ä¸­å½åºæ»è£ã" \
                    and spo["subject"] == "æéå" == spo["object"][0]:
                spo["object"][0] = "[å°æ¹¾äºº|æè®¯ä¸­å½åºæ»è£]"
            if spo["subject"] == "æ¨èå±æ§æ¯ä¾" and spo["predicate"] == "ææ§=1ï¼4ï¼1ï¼7ï¼2":
                spo["predicate"] = "ï¼"
                spo["object"][0] = "èåãæ ¹éª¨ãèº«æ³ãçµæ§ãææ§=1ï¼4ï¼1ï¼7ï¼2"

        # rm blanks
        sample["natural"] = clean_arg(sample["natural"])

        for spo in sample["logic"]:
            # rm redundant blanks
            for k, v in spo.items():
                if k not in {"object", "objects"}:
                    spo[k] = clean_arg(spo[k])
                    # if len(re.findall("\[", spo[k])) != len(re.findall("\]", spo[k])):
                    #     print(spo)
                    #     print(text)
                    #     print(">>>>>>bad {}>>>>>>>".format(k))

                elif k == "object":
                    new_objs = []
                    for obj in spo[k]:
                        obj = clean_arg(obj)
                        # if len(re.findall("\[", obj)) != len(re.findall("\]", obj)):
                        #     print(spo)
                        #     print(text)
                        #     print(">>>>>>bad object>>>>>>>")

                        new_objs.append(obj)
                    spo[k] = new_objs

    def parse_spe_txt2list(spe_txt, jt=""):
        sep = "\u2E82"
        star = spe_txt.find("[")
        end = -1
        if star != -1:
            stack = []
            for idx in range(star, len(spe_txt)):
                c = spe_txt[idx]
                if c == "[":
                    stack.append(c)
                elif c == "]":
                    stack.pop()
                    if len(stack) == 0:
                        end = idx
                        break

        res = []
        if star != -1 and end != -1:
            pre = spe_txt[:star]
            mid = spe_txt[star + 1:end]
            post = spe_txt[end + 1:]

            mid_sub = mid[:]
            stack = []
            for idx, c in enumerate(mid):
                if c == "[":
                    stack.append(c)
                elif c == "]":
                    stack.pop()
                elif c == "|" and len(stack) == 0:
                    mid_sub = mid_sub[:idx] + sep + mid_sub[idx + 1:]

            mid_segs = mid_sub.split(sep)
            tmp = [jt.join([pre, seg, post]) for seg in mid_segs]
            for txt in tmp:
                res.extend(parse_spe_txt2list(txt))
        else:
            res.append(spe_txt)
        return res

    def get_spe_txt_spans(spe_txt, text, is_pred=False):
        # target_str = re.sub("[\]\[\|]", "", spe_txt)
        # if is_pred:
        #     target_str = re.sub("([^a-zA-Z]|^)[XYZU]([^a-zA-Z]|$)", r"\1\2", target_str)

        # if is_pred:
        #     segs = re.split("[XYZU]", spe_txt)
        # else:
        #     segs = re.split("[\]\[\|]", spe_txt)
        #
        # segs = [s.strip() for s in segs if s.strip() != ""]
        search_str = spe_txt  # "".join(segs)

        candidate_spans, _ = Preprocessor.search_char_spans_fr_txt(search_str, text, "ch")
        spans = candidate_spans[0]
        spans = [(spans[i], spans[i + 1]) for i in range(0, len(spans), 2)]

        preid2c = {}
        pat = "[\]\[\|XYZU]+" if is_pred else "[\]\[\|]+"
        for m in re.finditer(pat, spe_txt):
            if is_pred:
                if spe_txt[m.span()[0]] in set("XYZU") and m.span()[0] - 1 >= 0 and (
                        0 <= ord(spe_txt[m.span()[0] - 1]) - ord("A") <= 25 or 0 <= ord(spe_txt[m.span()[0] - 1]) - ord(
                    "a") <= 25) or \
                        spe_txt[m.span()[1] - 1] in set("XYZU") and m.span()[1] < len(spe_txt) and (
                        0 <= ord(spe_txt[m.span()[1]]) - ord("A") <= 25 or 0 <= ord(spe_txt[m.span()[1]]) - ord(
                    "a") <= 25):
                    continue
            preid2c[m.span()[0] - 1] = m.group()

        start = re.match("[\]\[\|XYZU]+", spe_txt) if is_pred else re.match("[\]\[\|]+", spe_txt)
        spans_str = start.group() if start is not None else ""
        offset = len(spans_str)

        for sp in spans:
            for sp_idx in range(*sp):
                spans_str += "({}, {})".format(sp_idx, sp_idx + 1)
                offset += 1
                if offset - 1 in preid2c:
                    spans_str += preid2c[offset - 1]
                    offset += len(preid2c[offset - 1])

        spans_str_list = []
        for sps_str in parse_spe_txt2list(spans_str):
            sps = [int(s) for s in re.findall("\d+", sps_str)]
            sps = merge_spans(sps)
            spans_str_list.append(sps)
        return spans_str_list

    predefined_p = dict()

    # trans predicate
    for sample in data:
        text = sample["natural"]
        for spo in sample["logic"]:
            predicate = spo["predicate"]
            # re.match("[A-Z=]+$", predicate) and predicate not in text
            if predicate != "_" and re.match("[A-Z=]+$", predicate) and predicate not in text:
                predefined_p[predicate] = predefined_p.get(predicate, 0) + 1

    # predefined_p_map = {"DESC": "æè¿°",
    #                     "ISA": "æ¯ä¸ç§",
    #                     "IN": "ä½äº",
    #                     "BIRTH": "çäº",
    #                     "DEATH": "æ­»äº",
    #                     "=": "ç­äº",
    #                     "NOT": "ä¸",
    #                     }

    predefined_p_set = {"DESC", "ISA", "IN", "BIRTH", "DEATH", "=", "NOT"}
    new_data = []
    bad_spo_list = []
    for sample_id, sample in tqdm(enumerate(data), desc="transform"):
        ori_sample = copy.deepcopy(sample)
        # sample["natural"] = sample["natural"] + "[SEP]" + "ï¼".join(predefined_p_map.values())
        text = sample["natural"]

        sample["logic"] = Preprocessor.unique_list(sample["logic"])
        new_spo_list = []
        for spo in sample["logic"]:
            # fix some wrong samples
            # if spo["object"][0] == "":
            #     print("!!!")

            if spo["predicate"] in predefined_p_set:
                if spo["object"][0] == "" and spo["predicate"] in {"DEATH", "BIRTH"}:
                    if spo["place"] != "":
                        spo["object"][0] = spo["place"]
                        spo["place"] = ""
                    elif spo["time"] != "":
                        spo["object"][0] = spo["time"]
                        spo["time"] = ""

                # # trans predicate
                # new_predicate = re.sub(k, predefined_p_map[k], spo["predicate"])
                # spo["predicate"] = new_predicate

            ori_spo = copy.deepcopy(spo)
            for key in spo:
                if spo[key] == "":
                    spo[key] = []
                elif key != "object" and key != "objects":
                    if re.search(".*\[.*\|.*\].*", spo[key]):  # need to split
                        ori_str = spo[key]
                        try:
                            split_list = parse_spe_txt2list(ori_str)
                            span_list = get_spe_txt_spans(ori_str, text, is_pred=True if key == "predicate" else False)
                        except Exception:
                            print(ori_str)
                            print(span_list)
                            print(key)
                            print(text)
                            print(ori_spo)
                            print("==================split error anns=======================")

                        # check spans
                        for idx, sp in enumerate(span_list):
                            # try:
                            extr_txt = Preprocessor.extract_ent_fr_txt_by_char_sp(sp, text, "ch")
                            # except Exception:
                            #     print("!!!")
                            try:
                                cor_str = re.sub("([^a-zA-Z]|^)[XYZU]([^a-zA-Z]|$)", r"\1\2", split_list[idx]) \
                                    if key == "predicate" else split_list[idx]
                                assert extr_txt == cor_str
                            except Exception:
                                print(text)
                                print(key)
                                print(ori_str)
                                print(extr_txt)
                                print(cor_str)
                                print("==================span search error==================")
                        comb_list = [{"char_span": [sp, ], "text": split_list[idx]} for idx, sp in enumerate(span_list)
                                     if len(sp) > 0]
                        spo[key] = comb_list
                    else:
                        if key == "predicate" and spo[key] in predefined_p_set:
                            spo[key] = [
                                {"text": spo[key],
                                 "char_span": [[]],
                                 }, ]
                        else:
                            target_str = spo[key]
                            spe_p_map0 = {
                                'å¯ä»¥æ¾Xè®¡ç®AMP': 'å¯ä»¥æ¾è®¡ç®AMP',
                                "å¯¹Xè¿è¡ç®¡ç«¯çç¼æç": "å¯¹è¿è¡ç®¡ç«¯çç¼æç",
                                "å¯¹Xè¿è¡Y": "å¯¹|è¿è¡",
                                "å¨Xçç»è¥çå¿µæå¯¼ä¸": "å¨çç»è¥çå¿µæå¯¼ä¸",
                                "ç¹Xå®æ": "ç¹å®æ",
                                "å·æXçç¹ç¹": "å·æçç¹ç¹",
                                "ä¸ºXæéç": "ä¸ºæéç",
                                "ä¸ç´åå°Xçéç": "ä¸ç´åå°çéç",
                                "ä»¥Xä¸ºY": "ä»¥|ä¸º|",
                                "åXsockrrlates": "å|sockrrlates",
                                "ç¨Xè¿è¡ä¸­å¿å®ä½": "ç¨|è¿è¡ä¸­å¿å®ä½",
                                "æXçå±å¹": "æ|çå±å¹",
                                "å¸¦ç»Xæ´å¤çéæ©": "å¸¦ç»|æ´å¤çéæ©",
                                "æ´æ¯å°Xé¦æ¬¡ä¸æå°Y": "æ´æ¯å°|é¦æ¬¡ä¸æå°|",
                                "ç¸æ¯XæY": "ç¸æ¯|æ|",
                                "æ¯å¨Xçåºç¡ä¸ä¼åè®¾è®¡": "æ¯å¨|çåºç¡ä¸ä¼åè®¾è®¡",
                                "å¨Xä¸ä¼åè®¾è®¡": "å¨|ä¸ä¼åè®¾è®¡",
                                "æ¯Xä¸è´¯çå®æ¨": "æ¯|ä¸è´¯çå®æ¨",
                                "å¯¹Xçåå±äº§çäºå½±å": "å¯¹|çåå±äº§çäºå½±å",
                            }
                            if target_str in spe_p_map0:
                                target_str = spe_p_map0[target_str]

                            char_spans, _ = Preprocessor.search_char_spans_fr_txt(target_str, text, "ch")
                            spo[key] = [
                                {"text": spo[key],
                                 "char_span": char_spans,
                                 }, ]

                            for ch_sp in char_spans:
                                extr_txt = Preprocessor.extract_ent_fr_txt_by_char_sp(ch_sp, text, "ch")
                                spe_p_map = {
                                    "ä¸XçStarch RX1500ç¸å½": "ä¸çStarch RX1500ç¸å½",
                                    'å°Xè£å¥UNIXæå¡å¨': 'å°è£å¥UNIXæå¡å¨',
                                    'å°Xèå¥å°DKNYçè®¾è®¡å½ä¸­': 'å°èå¥å°DKNYçè®¾è®¡å½ä¸­',
                                    'ä¸Xåä½çäº§SK-1Z02Dæ­£åé²çç»¼åå½äºä»ª': 'ä¸åä½çäº§SK-1Z02Dæ­£åé²çç»¼åå½äºä»ª',
                                    'ç»å¸¸ä»¥Xæ¥ç§°å¼éå£å°çLYF': 'ç»å¸¸ä»¥æ¥ç§°å¼éå£å°çLYF',
                                    "å¯¹XåXä¸ªç®æ ": "å¯¹åXä¸ªç®æ ",
                                }
                                if key == "predicate":
                                    if target_str in spe_p_map:
                                        target_str = spe_p_map[target_str]
                                    else:
                                        target_str = re.sub("[XYZU\|]", "", target_str)
                                try:
                                    assert extr_txt == target_str
                                except Exception:
                                    print(target_str)
                                    print(extr_txt)
                                    print(key)
                                    print(text)
                                    print(ori_spo)
                                    print("==================error anns=======================")

                elif key == "object":
                    new_objs = []
                    for obj in spo[key]:
                        if re.search(".*\[.*\|.*\].*", obj):  # need to split
                            ori_str = obj

                            try:
                                split_list = parse_spe_txt2list(obj)
                                span_list = get_spe_txt_spans(ori_str, text, is_pred=False)
                            except Exception:
                                print(ori_str)
                                print(key)
                                print(text)
                                print(ori_spo)
                                print("==================error anns=======================")

                            # check spans
                            for idx, sp in enumerate(span_list):
                                extr_txt = Preprocessor.extract_ent_fr_txt_by_char_sp(sp, text, "ch")
                                try:
                                    cor_str = split_list[idx]
                                    assert extr_txt == cor_str
                                except Exception:
                                    print(text)
                                    print(key)
                                    print(ori_str)
                                    print(extr_txt)
                                    print(cor_str)
                                    print("==================span search error==================")

                            comb_list = [{"char_span": [sp, ], "text": split_list[idx]} for idx, sp in
                                         enumerate(span_list) if len(sp) > 0]
                            new_objs.append(comb_list)
                        else:
                            if obj == "":
                                pass
                            else:
                                try:
                                    char_spans, _ = Preprocessor.search_char_spans_fr_txt(obj, text, "ch")
                                except Exception:
                                    print(obj)
                                    print(key)
                                    print(text)
                                    print(ori_spo)
                                    print("==================obj error anns=======================")

                                new_objs.append([
                                    {"text": obj,
                                     "char_span": char_spans,
                                     }, ])
                    spo[key] = new_objs

            for p in spo["predicate"]:
                if re.search("[XYZU]", p["text"]) is None and len(spo["object"]) > 0 and p[
                    "text"] not in predefined_p_set:
                    p["text"] += "X"

            # align predicate and the corresponding subset of objects (by XYZU)
            ext_spo_list = []
            id_map = {"X": 0, "Y": 1, "Z": 2, "U": 3}
            spe_p_map = {
                "ä¸XçStarch RX1500ç¸å½": "ä¸[OBJ]çStarch RX1500ç¸å½",
                'å°Xè£å¥UNIXæå¡å¨': 'å°[OBJ]è£å¥UNIXæå¡å¨',
                'å°Xèå¥å°DKNYçè®¾è®¡å½ä¸­': 'å°[OBJ]èå¥å°DKNYçè®¾è®¡å½ä¸­',
                'ä¸Xåä½çäº§SK-1Z02Dæ­£åé²çç»¼åå½äºä»ª': 'ä¸[OBJ]åä½çäº§SK-1Z02Dæ­£åé²çç»¼åå½äºä»ª',
                'ç»å¸¸ä»¥Xæ¥ç§°å¼éå£å°çLYF': 'ç»å¸¸ä»¥[OBJ]æ¥ç§°å¼éå£å°çLYF',
                "å¯¹XåXä¸ªç®æ ": "å¯¹[OBJ]åXä¸ªç®æ ",
            }

            bad_spo = False
            for p in spo["predicate"]:
                sub_objs = []
                if p["text"] in spe_p_map or p["text"] in predefined_p_set:
                    sub_objs.append(spo["object"][0])
                else:
                    for ph in re.findall("[XYZU]", p["text"]):
                        idx = id_map[ph]
                        try:
                            sub_objs.append(spo["object"][idx])
                        except Exception:
                            # print(spo)
                            # print(text)
                            # print(">>>>>>>>>>>>>>>>object list: out of index>>>>>>>>>>>>>>>>>>>>>>>>>")
                            bad_spo = True
                            bad_spo_list.append({
                                "text": text,
                                "bad_spo": ori_spo,
                                "ori_sample": ori_sample,
                            })
                            break

                if bad_spo:
                    break

                # XYZU -> [OBJ]
                if p["text"] in spe_p_map:
                    p["text"] = spe_p_map[p["text"]]
                else:
                    assert len(spo["object"]) <= 4

                    if len(spo["object"]) >= 1:
                        p["text"] = re.sub("X", "[OBJ]", p["text"])
                    if len(spo["object"]) >= 2:
                        p["text"] = re.sub("Y", "[OBJ]", p["text"])
                    if len(spo["object"]) >= 3:
                        p["text"] = re.sub("Z", "[OBJ]", p["text"])
                    if len(spo["object"]) == 4:
                        p["text"] = re.sub("U", "[OBJ]", p["text"])

                new_spo = copy.deepcopy(spo)
                new_spo["predicate"] = [p, ]
                new_spo["object"] = sub_objs
                ext_spo_list.append(new_spo)
            if len(spo["predicate"]) == 0:
                ext_spo_list.append(spo)

            # product
            open_spo_list = []
            for new_spo in ext_spo_list:
                lists4prod = []
                for k, l in new_spo.items():
                    if k in {"object", "objects"} or len(l) == 0:
                        continue
                    lists4prod.append([{"type": k, **i} for i in l])

                for objs in new_spo["object"]:
                    new_objs = []
                    for i in objs:
                        new_objs.append({"type": "object", **i})
                    lists4prod.append(new_objs)

                open_spo_list.extend([list(item) for item in itertools.product(*lists4prod)])

            # choose the best span from candidates
            filtered_open_spo_list = []
            for spo in open_spo_list:
                if any(len(arg["char_span"]) > 1 for arg in spo):
                    new_spo = []
                    for arg_idx_i, arg_i in enumerate(spo):
                        if len(arg_i["char_span"]) > 1:
                            fin_ch_sp = None
                            fin_dis_score = 9999
                            for ch_sp_i in arg_i["char_span"]:
                                dis_score = 0
                                for arg_idx_j, arg_j in enumerate(spo):
                                    if arg_idx_i == arg_idx_j:
                                        continue
                                    for ch_sp_j in arg_j["char_span"]:
                                        if len(ch_sp_j) == 0:
                                            continue
                                        dis_score += min(abs(ch_sp_i[0] - ch_sp_j[1]), abs(ch_sp_j[0] - ch_sp_i[1]))
                                if dis_score < fin_dis_score:
                                    fin_dis_score = dis_score
                                    fin_ch_sp = ch_sp_i

                            arg_cp = copy.deepcopy(arg_i)
                            arg_cp["char_span"] = fin_ch_sp
                            new_spo.append(arg_cp)
                        else:
                            arg_cp = copy.deepcopy(arg_i)
                            arg_cp["char_span"] = arg_cp["char_span"][0]
                            new_spo.append(arg_cp)
                else:
                    new_spo = [{**arg, "char_span": arg["char_span"][0]} for arg in spo]
                filtered_open_spo_list.append(new_spo)

            # clean
            # filter arg with blank span; strip blanks around entity
            fin_open_spo_list = []
            for spo in filtered_open_spo_list:
                if any((len(arg["char_span"]) == 0 and arg["text"] not in predefined_p_set)
                       or arg["text"].strip() == ""
                       for arg in spo):
                    # print(spo)
                    # print(text)
                    # print(">>>>>>>>>>>>>>>>>>>>invalid arg>>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
                    bad_spo_list.append({
                        "text": text,
                        "bad_spo": ori_spo,
                        "ori_sample": ori_sample,
                    })
                    continue

                # strip blanks
                for arg in spo:
                    if arg["text"] in predefined_p_set:
                        # print("predefined predicate")
                        continue
                    utils.strip_entity(arg)

                fin_open_spo_list.append(spo)

            new_spo_list.extend(fin_open_spo_list)
        new_sample = {
            "id": sample_id,
            "text": text,
            "open_spo_list": new_spo_list,
        }
        new_data.append(new_sample)
    return new_data, predefined_p, bad_spo_list


def convert_saoke():
    new_data, predefined_pred_set, bad_spo_list = preprocess_saoke()

    for sample in new_data:
        span_list = []
        for spo in sample["open_spo_list"]:
            for arg in spo:
                span_list.append(arg["char_span"])
        tok_res = ChineseWordTokenizer.tokenize_plus(sample["text"], span_list=span_list)
        sample["word_list"] = tok_res["word_list"]
        sample["word2char_span"] = tok_res["word2char_span"]

    train_data_rate = 0.8
    val_data_rate = 0.1
    train_num = int(len(new_data) * train_data_rate)
    valid_num = int(len(new_data) * val_data_rate)
    test_num = len(new_data) - train_num - valid_num
    random.shuffle(new_data)
    train_data = new_data[:train_num]
    valid_data = new_data[train_num:train_num + valid_num]
    test_data = new_data[-test_num:]

    return train_data, valid_data, test_data


def convert_casie():
    from glob import glob
    data = []
    for file_path in glob("../../data/ori_data/casie_bk/*.json"):
        sample = load_data(file_path)
        file_name = file_path.split("/")[-1]
        idx = re.search("\d+", file_name).group()
        sample["id"] = idx
        data.append(sample)

    new_data = []
    for sample in data:
        if "cyberevent" not in sample:
            continue
        text = sample["content"]
        new_sample = {
            "id": sample["id"],
            "text": text,
            "event_list": []
        }

        span2text = {}
        char_span_list = []
        for hp in sample["cyberevent"]["hopper"]:
            for event in hp["events"]:
                trigger = event["nugget"]["text"]
                trigger_char_span = [event["nugget"]["startOffset"], event["nugget"]["endOffset"]]
                span2text[str(trigger_char_span)] = trigger

                new_event = {
                    "trigger": trigger,
                    "trigger_char_span": trigger_char_span,
                    "realis": event["realis"],
                    "event_type": "{}.{}".format(event["type"], event["subtype"]),
                    "argument_list": [],
                }
                if "argument" in event:
                    for arg in event["argument"]:
                        arg_char_span = [arg["startOffset"], arg["endOffset"]]
                        arg_txt = arg["text"]
                        span2text[str(arg_char_span)] = arg_txt

                        new_event["argument_list"].append({
                            "text": arg_txt,
                            "char_span": arg_char_span,
                            "role": arg["role"]["type"],
                            "type": arg["type"],
                        })
                new_sample["event_list"].append(new_event)
        # check spans
        span2text_ = dict(sorted(span2text.items(), key=lambda x: int(re.search("\d+", x[0]).group())))
        for sp_str, txt in span2text_.items():
            sp_se = re.search("\[(\d+), (\d+)\]", sp_str)
            sp = [int(sp_se.group(1)), int(sp_se.group(2))]
            char_span_list.append(sp)

            extr_txt = Preprocessor.extract_ent_fr_txt_by_char_sp(sp, text)
            try:
                assert extr_txt == txt
            except Exception:
                print(sample["sourcefile"])
                print(extr_txt)
                print(txt)
                print("===========================")

        # word2char_span and word list
        tok_res = WhiteWordTokenizer.tokenize_plus(text, span_list=char_span_list)
        new_sample["word_list"] = tok_res["word_list"]
        new_sample["word2char_span"] = tok_res["word2char_span"]
        new_data.append(new_sample)

    random.shuffle(new_data)
    test_data = new_data[-100:]
    save_as_json_lines(test_data, "../../data/ori_data/casie/test_data.json")

    valid_num = int(900 / 8) - 1
    for start_idx in range(0, 900, valid_num):
        end_idx = start_idx + valid_num
        if end_idx > 900:
            break
        valid_data = new_data[start_idx:end_idx]
        train_data = new_data[:start_idx] + new_data[end_idx:-100]
        save_as_json_lines(train_data, "../../data/ori_data/casie/train_data_{}.json".format(start_idx // valid_num))
        save_as_json_lines(valid_data, "../../data/ori_data/casie/valid_data_{}.json".format(start_idx // valid_num))
        print("{}, {}".format(len(train_data), len(valid_data)))


def convert_chfin():
    data_dir = "../../data/ori_data/chfinann_bk"
    save_dir = "../../data/ori_data/chfinann"
    train_data = load_data(os.path.join(data_dir, "train.json"))
    val_data = load_data(os.path.join(data_dir, "dev.json"))
    test_data = load_data(os.path.join(data_dir, "test.json"))

    def convert_chfinann(data):
        new_data = []
        for sample in data:
            text = " ".join(sample[1]["sentences"])

            # mention 2 spans
            all_char_span_list = []
            offset = [0, ]
            for sent in sample[1]["sentences"]:
                offset.append(offset[-1] + len(sent) + 1)
            mention2spans = {m: [[offset[sp[0]] + sp[1], offset[sp[0]] + sp[2]] for sp in sent_spans] for m, sent_spans
                             in sample[1]["ann_mspan2dranges"].items()}
            for m, char_spans in mention2spans.items():
                all_char_span_list.extend(char_spans)
                for sp in char_spans:
                    assert m == text[sp[0]:sp[1]]

            event_list = []
            for event in sample[1]["recguid_eventname_eventdict_list"]:
                event_type = event[1]
                arg_list = []
                for arg_type, mention in event[2].items():
                    if mention is None:
                        continue
                    for m_span in mention2spans[mention]:
                        arg_list.append({
                            "text": mention,
                            "char_span": m_span,
                            "type": arg_type,
                        })
                event_list.append({
                    "event_type": event_type,
                    "argument_list": arg_list,
                })

            tok_res = ChineseWordTokenizer.tokenize_plus(text, span_list=all_char_span_list)
            new_sample = {
                "id": sample[0],
                "text": text,
                **tok_res,
                "event_list": event_list,
            }
            new_data.append(new_sample)
        return new_data

    new_train_data = convert_chfinann(train_data)
    new_valid_data = convert_chfinann(val_data)
    new_test_data = convert_chfinann(test_data)

    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    save_as_json_lines(new_train_data, os.path.join(save_dir, "train_data.json"))
    save_as_json_lines(new_valid_data, os.path.join(save_dir, "valid_data.json"))
    save_as_json_lines(new_test_data, os.path.join(save_dir, "test_data.json"))


# def add_postag_n_deprel(data):
#     ddp = DDParser(use_pos=True, buckets=True)
#     texts = [sample["text"] for sample in data]
#     m = hashlib.md5()
#     m.update("\n".join(texts).encode('utf-8'))
#     cache_path = "./parse_cache_{}.jsonlines".format(m.hexdigest())
#
#     all_data_len = len(texts)
#     if os.path.exists(cache_path):
#         parse_results = load_data(cache_path)
#         assert len(parse_results) == all_data_len
#     else:
#         parse_results = []
#         for idx in tqdm(range(0, all_data_len, 100), desc="ddp parse"):
#             parse_results.extend(ddp.parse(texts[idx:idx + 100]))
#         save_as_json_lines(parse_results, cache_path)
#
#     for sample_idx, sample in tqdm(enumerate(data), desc="add features"):
#         text = sample["text"]
#         all_ch_sp_list = Preprocessor.get_all_possible_char_spans(sample)
#         tok_res = ChineseWordTokenizer.tokenize_plus(text, span_list=all_ch_sp_list)
#         ctok_word2char_span = tok_res["word2char_span"]
#         cchar2tok_span = utils.get_char2tok_span(ctok_word2char_span)
#         sample["word2char_span"] = ctok_word2char_span
#         sample["word_list"] = tok_res["word_list"]
#
#         ddp_res = parse_results[sample_idx]
#         ddp_tok2char_span = utils.get_tok2char_span_map4ch(ddp_res["word"])
#         ddp_char2tok_span = utils.get_char2tok_span(ddp_tok2char_span)
#
#         pos_tag_list, deprel_list = [], []
#         for wid, word in enumerate(tok_res["word_list"]):
#             ch_sp = ctok_word2char_span[wid]  # word to char span
#             tok_sps = ddp_char2tok_span[ch_sp[0]:ch_sp[1]]  # char span to ddp tok span
#             # try:
#             #     assert tok_sps[0][0] == tok_sps[-1][1] - 1
#             # except:
#             #     print("pre duie")
#             if tok_sps[0][0] == tok_sps[-1][1] - 1:  # if this word corresponds to a single token in ddp result
#                 ddp_tok_id = tok_sps[0][0]
#             else:
#                 # if this word corresponds to multiple tokens in ddp result,
#                 # find the most similar token by edit distance
#                 # ignore "[" and "]"
#                 ddp_tok_id = None
#                 min_dis = 9999
#                 for tk_idx in range(tok_sps[0][0], tok_sps[-1][1]):
#                     ddp_wd = ddp_res["word"][tk_idx]
#                     l_dis = Levenshtein.distance(re.sub("[\[\]]", "", ddp_wd), re.sub("[\[\]]", "", word))
#                     if l_dis < min_dis:
#                         min_dis = l_dis
#                         ddp_tok_id = tk_idx
#             pos_tag = ddp_res["postag"][ddp_tok_id]
#             pos_tag_list.append(pos_tag)
#
#             ddp_head_tok_id = ddp_res["head"][ddp_tok_id] - 1
#             deprel = ddp_res["deprel"][ddp_tok_id]
#
#             ddp_head_ch_sp = ddp_tok2char_span[ddp_head_tok_id]
#             ddp_head_ctok_spans = cchar2tok_span[ddp_head_ch_sp[0]:ddp_head_ch_sp[1]]
#             for head_ctok_idx in range(ddp_head_ctok_spans[0][0], ddp_head_ctok_spans[-1][1]):
#                 deprel_list.append([wid, head_ctok_idx, deprel])
#         sample["pos_tag_list"] = pos_tag_list
#         sample["dependency_list"] = deprel_list
#     return data


def preprocess_duie2():
    data_dir = "../../data/ori_data/duie_comp2021_bk"
    save_dir = "../../data/ori_data/duie_comp2021"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    train_data = load_data(os.path.join(data_dir, "train_data.json"))
    valid_data = load_data(os.path.join(data_dir, "valid_data.json"))
    test_data_1 = load_data(os.path.join(data_dir, "test_data_1.json"))
    test_data_2 = load_data(os.path.join(data_dir, "test_data_2.json"))

    def clean_txt(txt):
        txt = re.sub("âç¾å½ææä¸è¯éªåä¼âASTMF2085-2012", "âç¾å½ææä¸è¯éªåä¼â ASTM F2085-2012", txt)
        txt = re.sub("è±ç£è¯åï¼9.7BBCèæ¶5å¹´", "è±ç£è¯åï¼9.7 BBCèæ¶5å¹´", txt)
        txt = re.sub("éè½½æ²ãB1A4ãå´å°é", "éè½½æ²ãB1Aãå´å°é", txt)
        txt = re.sub("oppoR7", "oppo R7", txt)
        txt = re.sub("æ¥¼ä¸»æ­¥æ­¥é«/vivoE1T", "æ¥¼ä¸»æ­¥æ­¥é«/vivo E1T", txt)
        txt = re.sub("Eyes on me 8My heart will go on", "Eyes on me 8 My heart will go on", txt)
        txt = re.sub("NetflixNetflix", "Netflix", txt)
        txt = re.sub("åçå®CCTV5NBAæåçº¿", "åçå®CCTV5 NBAæåçº¿", txt)
        txt = re.sub("ç¼©åä¸ºPCTCGP", "ç¼©åä¸ºPCTCG", txt)
        txt = re.sub("1CN5Aç§æå¹¿åº", "1 CN5Aç§æå¹¿åº", txt)
        txt = re.sub("1ã3M3Må¬å¸", "1ã3Må¬å¸", txt)
        txt = re.sub("QUTBBS", "QUT BBS", txt)
        txt = re.sub("SHES\.H\.Eç»å", "S.H.Eç»å", txt)
        txt = re.sub("4 NBCNBC\(National Broadcasting", "4 NBC(National Broadcasting", txt)

        txt = re.sub("([a-zA-Z0-9]+)((1[0-9]|20)\d{2}å¹´)", r"\1 \2", txt)
        txt = re.sub("([a-zA-Z0-9]+)((1[0-9]|20)\d{2}-\d{2}-\d{2})", r"\1 \2", txt)
        txt = re.sub("([a-zA-Z0-9]+)(1[0-2]æ)", r"\1 \2", txt)
        txt = re.sub("(1)([3-9]æ)", r"\1 \2", txt)
        txt = re.sub("([a-zA-Z2-9])(0?[1-9]æ)", r"\1 \2", txt)

        txt = re.sub("([a-zA-Z]{3,})(\d+)", r"\1 \2", txt)
        txt = re.sub("(\d+)([a-zA-Z]{3,})", r"\1 \2", txt)
        txt = re.sub("(\d{3,})([a-zA-Z]+)", r"\1 \2", txt)
        txt = re.sub("([a-zA-Z]+)(\d{3,})", r"\1 \2", txt)

        txt = re.sub("([a-zA-Z]+)(Inc\.)", r"\1 \2", txt)
        return txt

    # fix data
    for sample in tqdm(train_data + valid_data + test_data_1 + test_data_2, desc="clean"):
        # fix text
        text = sample["text"]
        if text == "2  æ±ç¾é³21967åºçå¨æ±è¥¿åæï¼1989å¹´æ¯ä¸äºæ±è¥¿ç§æå¸èå¤§å­¦å¤è¯­ç³»è±è¯­ä¸ä¸ï¼åæ¥åéè³é¹°æ½­éè·¯ä¸ä¸­ä»»è±è¯­æå¸":
            text = "2  æ±ç¾é³ 1967åºçå¨æ±è¥¿åæï¼1989å¹´æ¯ä¸äºæ±è¥¿ç§æå¸èå¤§å­¦å¤è¯­ç³»è±è¯­ä¸ä¸ï¼åæ¥åéè³é¹°æ½­éè·¯ä¸ä¸­ä»»è±è¯­æå¸"
            sample["text"] = text
        if text == "äººç©ç®ä»çç2ï¼å¥³ï¼19441ï¼1968å¹´æ¯ä¸äºåäº¬å¤§å­¦ç©çç³»":
            text = "äººç©ç®ä»çç2ï¼å¥³ï¼1944ï¼1968å¹´æ¯ä¸äºåäº¬å¤§å­¦ç©çç³»"
            sample["text"] = text
        if text == "å½±çä¿¡æ¯çµè§å§å½±çåç§°ï¼èå¨èå å¥ç¬¬äºå­£  å½±çç±»åï¼æ¬§ç¾å§  å½±çè¯­è¨ï¼è±è¯­  ä¸æ å¹´ä»½ï¼20121æ¼åè¡¨å§æä»ç»ç¾å½èå å¥ï¼åäº²å¥³å­©CeCeï¼Bella Thorneé¥°ï¼åéºèRockyï¼Zendaya Colemané¥°ï¼åæ¬åªæ¯ä¸¤ä¸ªç±è·³èçæ®éåä¸­ç":
            text = "å½±çä¿¡æ¯çµè§å§å½±çåç§°ï¼èå¨èå å¥ç¬¬äºå­£  å½±çç±»åï¼æ¬§ç¾å§  å½±çè¯­è¨ï¼è±è¯­  ä¸æ å¹´ä»½ï¼2012 1æ¼åè¡¨å§æä»ç»ç¾å½èå å¥ï¼åäº²å¥³å­©CeCeï¼Bella Thorneé¥°ï¼åéºèRockyï¼Zendaya Colemané¥°ï¼åæ¬åªæ¯ä¸¤ä¸ªç±è·³èçæ®éåä¸­ç"
            sample["text"] = text

        if "spo_list" in sample:
            # fix cases
            if text in {"äºå«æ¶ï¼1976å¹´åºçäºæ²³åçéè®¸å¿ï¼2013å¹´æºæåæ´ååæ²³åæ¬£èµç½ç»ç§æéå¢ï¼ä»»è¯¥éå¢è£äºé¿ï¼ä¸æ³¨äºæå½å¤§ä¸­å°åä¼ä¸æä¾ä¸ä¸çç½ç»æå¡ï¼å¸¦å¨å¾å¤ä¼ä¸ç½ç»æ¹åçè½¬å",
                        "1962å¹´å¨æçå¨ä¸­å½æ¤ç©ä¿æ¤å­¦ä¼æç«å¤§ä¼ä¸ï¼ä½äºãæå½å®³è«åä¸é²æ²»ç ç©¶ç°ç¶åå±æãçå­¦æ¯æ¥åï¼1963å¹´å¨ãäººæ°æ¥æ¥ãä¸åè¡¨ãç»åèä½é²æ²»å®³è«ãä¸æ"}:
                new_spo_list = [spo for spo in sample["spo_list"] if spo["predicate"] != "æå±ä¸è¾"]
                sample["spo_list"] = new_spo_list
            if "http://news.sohu.com/20081221/n261333381.shtml 12æ20æ¥" in text:
                text = "2008å¹´12æ20æ¥ï¼è¥¿åæ¿æ³å¤§å­¦å¨ç©ä¿æ¤æ³ç ç©¶ä¸­å¿æçæç« åæ¶ï¼ç±è¥¿åæ¿æ³å¤§å­¦åä¸­å½ç¤¾ä¼ç§å­¦é¢æ³å­¦ç ç©¶æå±åä¸»åçâä¸­å½ãå¨ç©ä¿æ¤æ³ãç ç©¶é¡¹ç®âæ­£å¼å¯å¨ ããå¾ä¸ºè¥¿åæ¿æ³å¤§å­¦å¨ç©ä¿æ¤æ³ç ç©¶ä¸­å¿ä¸»ä»»å­æ±(å·¦ä¸)ä»è¥¿åæ¿æ³å¤§å­¦æ ¡é¿è´¾å®ææ(å·¦äº)æä¸­æ¥è¿âè¥¿åæ¿æ³å¤§å­¦å¨ç©ä¿æ¤ç ç©¶ä¸­å¿âççå¾"
                sample["text"] = text

                for spo in sample["spo_list"]:
                    if spo["object"]["@value"] == "2008":
                        spo["object"]["@value"] = "2008å¹´12æ20æ¥"

            if text == "2012å¹´ äººåå§éä¹æ³°å§ ç¥¨æ¿12.67äº¿":
                new_spo_list = [spo for spo in sample["spo_list"] if spo["subject"] != "12å¹´"]
                sample["spo_list"] = new_spo_list

            for spo in sample.get("spo_list", []):
                if spo["object"]["@value"] == "162015å¹´12æ25æ¥":
                    spo["object"]["@value"] = "2015å¹´12æ25æ¥"
                if "ä½ä¸ºæ°ä¸è¾ã00:00ãä¸­ç¬¬ä¸é¦ç¡®å®æ¶å¥çæ­æ²" in text \
                        and spo["object"]["@value"] == "0:00" and spo["predicate"] == "æå±ä¸è¾":
                    spo["object"]["@value"] = "00:00"
                if "B1A4" in text and spo["object"]["@value"] == "B1A":
                    spo["object"]["@value"] = "B1A4"
                if "MojangAB" in text and spo["subject"] == "Mojang":
                    spo["subject"] = "MojangAB"
                if "ãNã" in text and spo["object"]["@value"] == "n":
                    spo["object"]["@value"] = "N"
                if spo["subject"] == "å°å¾å¸å¤§ä½ä¸ºç¬¬ä¸å­£" and spo["object"]["@value"] == "r":
                    spo["object"]["@value"] = "Mark-Paul Gosselaar"
                if "Lesley Chiang" in text and spo["object"]["@value"] == "esley Chiang":
                    spo["object"]["@value"] = "Lesley Chiang"
                if "ç¥å¥æµ·çå¢" in text and "inArea" in spo["object"] and spo["object"]["inArea"] == "is":
                    spo["object"]["inArea"] = "ä¸­å½å½å"
                if "è±æ´å¨" in text and spo["object"]["@value"] == "IGA":
                    spo["object"]["@value"] = "IIGA"
                if text == "ä»åå¸åä¹ç»é²éåç¾ç§æ®é¦æç«äº2010æ27æ¥,å¨å¹¿å·ä»åå¸åä¹ç»éå°å¹´ç´ è´¨æå±è®­ç»ä¸­å¿ä¸¾è¡äºåä¹ç»é²éåç¾ç§æ®é¦å¼é¦åå¹¿ä¸çé²éåç¾ç§æ®åºå°æçä»ªå¼" \
                        and spo["object"]["@value"] == "2010æ27æ¥":
                    text = "ä»åå¸åä¹ç»é²éåç¾ç§æ®é¦æç«äº2010å¹´10æ27æ¥,å¨å¹¿å·ä»åå¸åä¹ç»éå°å¹´ç´ è´¨æå±è®­ç»ä¸­å¿ä¸¾è¡äºåä¹ç»é²éåç¾ç§æ®é¦å¼é¦åå¹¿ä¸çé²éåç¾ç§æ®åºå°æçä»ªå¼"
                    spo["object"]["@value"] = "2010å¹´10æ27æ¥"

                if text == 'æ¯å¦å¼ èºè°çä¸¤å±å¨å°¼æ¯å½éçµå½±èéç®å¥ ï¼ç¬¬38å±ææå½éçµå½±èéçå¥ ï¼ä¸¤å±è±å½çµå½±å­¦é¢å¥æä½³å¤è¯­ç ï¼ç¬¬55å±å°æ¹¾çµå½±éé©¬å¥æä½³å¯¼æ¼å¥ ï¼ç¬¬05å±ä¸­å½çµå½±åè¡¨å¥æä½³å¯¼æ¼å¥ ï¼2008å½±åä¸çåäººå¤§å¥' and \
                        spo["object"]["@value"] == 'ä¸­å½çµå½±åè¡¨å¥æä½³å¯¼æ¼å¥' and spo["object"]["period"] == "5":
                    spo["object"]["period"] = "05"
                if text == 'éæè¯ï¼æ¾è·å¾ç¬¬ä¸å±è±å½ä¸åå½éåè¯­çµå½±èä¼ç§ç·éè§å¥ï¼ç¬¬21å±åäº¬å¤§å­¦ççµå½±èæä½³å¯¼æ¼å¤å¥³ä½å¥ï¼ç¬¬23å±åäº¬å¤§å­¦ççµå½±èæä½³ç¼å§å¥ç­å¥é¡¹ï¼åä¸¤å¹´èåä½ä¸½å¨ä¸äºä¹æ¯é¹å¾äººå°½çç¥' \
                        and spo["object"]["@value"] == 'è±å½ä¸åå½éåè¯­çµå½±èä¼ç§ç·éè§å¥' and spo["object"]["period"] == "3":
                    spo["object"]["period"] = "ä¸"
                if text == '1982å¹´ï¼è®¸å æå­ãæ©ç»ä¿éãï¼åå¤ºç¬¬ä¸å±é¦æ¸¯çµå½±éåå¥æä½³ç·ä¸»è§å¥' \
                        and spo["object"]["@value"] == 'é¦æ¸¯çµå½±éåå¥æä½³ç·ä¸»è§å¥' and spo["object"]["period"] == "1":
                    spo["object"]["period"] = "ä¸"
                if text == 'äºæ´²çµå½±å¤§å¥ç»èº«æå°±å¥:ç¬¬02å±ï¼2008å¹´:å±±ç°æ´æ¬¡ç¬¬04å±ï¼2010å¹´:é¿ç±³è¾¾å·´å½»ç¬¬05å±ï¼2011å¹´:é¹ææç¬¬06å±ï¼2012å¹´:è®¸éåç¬¬08å±ï¼2014å¹´:ä¾¯å­è´¤ç¬¬09å±ï¼2015å¹´:æææ³½ç¬¬10å±ï¼2016å¹´:æ æ¨å¸æ&è¢åå¹³ç¬¬11å±ï¼2017å¹´:å¾åç¬¬12å±ï¼2018å¹´:å¼ è¾åç¬¬13å±ï¼2019å¹´:ææ²§ä¸' \
                        and spo["object"]["@value"] == 'äºæ´²çµå½±å¤§å¥ç»èº«æå°±å¥' and "period" in spo["object"] and spo["object"][
                    "period"] == "9":
                    spo["object"]["period"] = "09"
                if "ç¬¬ä¹å±å½éåè£å°å§è·å¥åå" in text and spo["object"]["period"] == "9":
                    spo["object"]["period"] = "ä¹"
                if "åå¤ºç¬¬ä¸å±é¦æ¸¯çµå½±éåå¥æä½³ç·ä¸»è§å¥" in text and spo["object"]["period"] == "1":
                    spo["object"]["period"] = "ä¸"
                if "è·ç¬¬å«å±ä¸­å½çµå½±éé¸¡å¥æä½³å¥³ä¸»è§å¥" in text and spo["object"]["period"] == "8":
                    spo["object"]["period"] = "å«"
                if "ç¬¬ä¹å±é¦æ¸¯çµå½±éåå¥æä½³å¥³éè§" in text and spo["object"]["period"] == "9":
                    spo["object"]["period"] = "ä¹"
                if "ãåå¸å¬æ¿ãè·ç¬¬ä¸å±é¦æ¸¯çµå½±éåå¥" in text and spo["object"]["period"] == "3":
                    spo["object"]["period"] = "ä¸"
                if text == 'äºæ´²çµå½±å¤§å¥ç»èº«æå°±å¥:ç¬¬02å±ï¼2008å¹´:å±±ç°æ´æ¬¡ç¬¬04å±ï¼2010å¹´:é¿ç±³è¾¾å·´å½»ç¬¬05å±ï¼2011å¹´:é¹ææç¬¬06å±ï¼2012å¹´:è®¸éåç¬¬08å±ï¼2014å¹´:ä¾¯å­è´¤ç¬¬09å±ï¼2015å¹´:æææ³½ç¬¬10å±ï¼2016å¹´:æ æ¨å¸æ&è¢åå¹³ç¬¬11å±ï¼2017å¹´:å¾åç¬¬12å±ï¼2018å¹´:å¼ è¾åç¬¬13å±ï¼2019å¹´:ææ²§ä¸' \
                        and spo["object"]["@value"] == 'äºæ´²çµå½±å¤§å¥ç»èº«æå°±å¥' and "period" in spo["object"] and spo["object"][
                    "period"] == "6":
                    spo["object"]["period"] = "06"
                if text == 'ã007ãããè°å½±ééãä½ä¸ºæ¾ç»çç¹å·¥çï¼ççäºæ´ä¸ªå¥½è±åï¼åèè³ä»æ¶è·çº¦70äº¿ç¾åï¼åèè³ä»æ¶è·çº¦11äº¿ç¾åï¼å¨æ¶è·ä¸è²ç¥¨æ¿çåæ¶ï¼ä¹èµ¢å°½äºå£ç¢' \
                        and spo["object"]["@value"] == '70äº¿ç¾å' and spo["subject"] == "7":
                    spo["subject"] = "007"
                if text == '1987å¹´ï¼ç±ä½å®¶å¼ å¼¦ç¼å§ãæäºæå¯¼æ¼ççµå½±ãäºãå®æï¼æ½è¹å å¨è¯¥çä¸­æ®æ¼å¾ä¸½èèè·å¾ç¬¬å«å±ä¸­å½çµå½±éé¸¡å¥æä½³å¥³ä¸»è§å¥ãç¬¬18å±æå¤§å©é¶å°ç±³çº³å½éçµå½±èæä½³å¥³ä¸»è§å¥' \
                        and spo["object"]["@value"] == 'ä¸­å½çµå½±éé¸¡å¥æä½³å¥³ä¸»è§' and spo["object"]["period"] == "8":
                    spo["object"]["period"] = "å«"
                if text == 'è¯¥çè·1988å¹´ç¬¬å«å±ä¸­å½çµå½±éé¸¡å¥æä½³å¥³ä¸»è§å¥ï¼æ½è¹ï¼' \
                        and spo["object"]["@value"] == 'ä¸­å½çµå½±éé¸¡å¥æä½³å¥³ä¸»è§' and spo["object"]["period"] == "8":
                    spo["object"]["period"] = "å«"
                if text == "åºçäº1984å¹´9æ26æ¥ï¼æ²³åéå·ï¼å­¦åï¼æ¬ç§ï¼ç¹é¿ï¼ä¸»æãæè¯µãè¡¨æ¼ãå½ç»ãå¹³é¢æ¨¡ç¹ãç«¥å£°æ¨¡ä»¿ æ¯ä¸é¢æ ¡/ä¸ä¸ï¼ä¸­å½ä¼ åªå¤§å­¦/æ­é³ä¸ä¸»æèºæ¯ä¸ä¸ï¼ç®¡æåï¼ä¸­å¤®çµè§å°ä½è²é¢éï¼CCTV5ï¼ä½è²æ¨æ¥ãå¤©æ°ä½è²ãä¸»æäººï¼åæ¶ï¼è¿ä¸»æä¸­å¤®çµè§å°ç»æµé¢éï¼CCTV2ï¼ãç¬¬ä¸å°è±¡ããä¸­å¤®çµè§å°åä¸é¢éãåä¸æ°è±¡ãåä¸­å½æ°è±¡é¢éãå¤©æ°ç´æ­é´ãç­æ ç®" \
                        and spo["object"]["@value"] == "" and spo["predicate"] == "æ¯ä¸é¢æ ¡":
                    spo["object"]["@value"] = "ä¸­å½ä¼ åªå¤§å­¦"
                if text == "ãç½è²æ¢¦å¹»ãæ¯ä¸é¨äº1998å¹´1æ1æ¥åºåççµè§å§ï¼ç±å¤ªçº²å¯¼æ¼ï¼ç±ç°å²·ãè®¸äºåãçä¸½ä¸½ åä½æ´ä¸»æ¼ï¼ä¸å±æ20éï¼æ¯é48åé" \
                        and spo["object"]["@value"] == "" and spo["predicate"] == "ä¸æ æ¶é´":
                    spo["object"]["@value"] = "1998å¹´1æ1æ¥"
                    spo["subject"] = "ç½è²æ¢¦å¹»"
                if text == "ãéªç¹è¡å¨ç¬¬äºå­£ãæ¯ä¸é¨ç±ç¼å§Stephanie Morgensternç¼åçä¸é¨å¨ä½å§æçµè§å§ï¼åºåæ¶é´ä¸º2012å¹´09æ20æ¥" \
                        and spo["object"]["@value"] == "" and spo["predicate"] == "ä¸æ æ¶é´":
                    spo["object"]["@value"] = "2012å¹´09æ20æ¥"
                    spo["subject"] = "éªç¹è¡å¨ç¬¬äºå­£"

                if text == "ç¬¬å­å åæ¦ 1982.1.25 æ±èæ·®å® ââ2001å¹´10æ é¦å±ç¾äºå¨å½æ°æå¤§èµæ±èå°åºéæèµä¸ç­å¥ æä½³æ¿æå¥ ææ¥ä¹æç§°å·ï¼2002å¹´6æ ä¸­é©æ°æéç§å¤§èµç¬å±ç»ç¹ç­å¥ï¼2004å¹´6æ ä¸­å¤®çµè§å°ãéå¸¸6+1ãï¼ æ¹åå«è§è¶çº§å¥³çæé½èµåºå10 ï¼ 2011å¹´ ç¬¬ä¸å±åäººæåå¤§éç¬¬ä¸å ç¨çµé­å±æ­çæ­æï¼å±çæ æ°å¬ä¼æµæ³ªï¼ä¸æ§âå°åæ¬¢âï¼å æ­¤æåæå¸å¤§åæ¬¢ 9 S" \
                        and spo["predicate"] == "è·å¥" and "period" in spo["object"] and spo["object"]["period"] == "":
                    spo["object"]["period"] = "é¦"
                if text == "2010å¹´è·Music Radioä¸­å½Topæè¡æ¦åå°æä½³åä½æ­æå¥ï¼ç¬¬å«å±ä¸åå²çé³ä¹æ¦é¢å¥å¸ç¤¼å²çåå°æä½³å±ä½æ­æå¥ãå²çæä½³ä½æ²äººå¥æå¥ä»¥éåºä¸ºèµ·ç¹ï¼å¼å¯2018-2020âä¸æ­¢ æ¯æå¥âä¸çå·¡åæ¼å±ä¼" \
                        and spo["predicate"] == "è·å¥" and "period" in spo["object"] and spo["object"]["period"] == "":
                    spo["object"]["period"] = "å«"

                if spo["predicate"] in {"ä¸ä¸ä»£ç ", "é®æ¿ç¼ç "} and text[
                    re.search(spo["object"]["@value"], text).span()[0] - 1] == "0":
                    spo["object"]["@value"] = "0" + spo["object"]["@value"]

                # strip redundant whitespaces and unknown characters
                spo["subject"] = clean_text(spo["subject"])
                for k, item in spo["object"].items():
                    spo["object"][k] = clean_text(item)

            # fix text by patterns
            text = clean_txt(text)
            sample["text"] = text

            # fix spo
            new_spo_list = []
            for spo in sample["spo_list"]:
                spo["subject"] = clean_txt(spo["subject"])
                for k, v in spo["object"].items():
                    spo["object"][k] = clean_txt(v)

                if spo["subject"].lower() not in text.lower() or spo["object"]["@value"].lower() not in text.lower():
                    # drop wrong spo
                    continue

                if spo["subject"] not in text:  # if not in, try recover upper case
                    m = re.search(re.escape(spo["subject"].lower()), text.lower())
                    # print("{}----{}".format(spo["subject"], text[m.span()[0]:m.span()[1]]))
                    spo["subject"] = text[m.span()[0]:m.span()[1]]

                if spo["object"]["@value"] not in text:
                    m = re.search(re.escape(spo["object"]["@value"].lower()), text.lower())
                    # print("{}----{}".format(spo["object"], text[m.span()[0]:m.span()[1]]))
                    spo["object"]["@value"] = text[m.span()[0]:m.span()[1]]
                new_spo_list.append(spo)

            filtered_spo_list = []
            for spo in new_spo_list:
                assert spo["subject"] in text

                if spo["subject"].strip() == "":
                    continue

                bad_spo = False
                for item in spo["object"].values():

                    assert item in text
                    if item.strip() == "":
                        bad_spo = True
                        break

                if not bad_spo:
                    filtered_spo_list.append(spo)

            sample["spo_list"] = filtered_spo_list

    train_data_path = os.path.join(save_dir, "train_data.json")
    valid_data_path = os.path.join(save_dir, "valid_data.json")
    test_data_1_path = os.path.join(save_dir, "test_data_1.json")
    test_data_2_path = os.path.join(save_dir, "test_data_2.json")
    save_as_json_lines(train_data, train_data_path)
    save_as_json_lines(valid_data, valid_data_path)
    save_as_json_lines(test_data_1, test_data_1_path)
    save_as_json_lines(test_data_2, test_data_2_path)


def trans2duee_format(pred_data):
    new_pred_data = []
    for sample in pred_data:
        new_event_list = []
        for event in sample["event_list"]:
            new_arg_list = []
            for arg in event["argument_list"]:
                new_arg_list.append({
                    "role": arg["type"],
                    "argument": arg["text"],
                })
            if len(new_arg_list) > 0:
                new_event_list.append({
                    "event_type": event["event_type"],
                    "arguments": new_arg_list
                })
        new_sample = {
            "id": sample["id"],
            "event_list": new_event_list,
        }
        new_pred_data.append(new_sample)
    return new_pred_data


def rule_fix_one2many(data, threshold=0.9):
    '''
    å¤çé¨åpredicateçä¸å¯¹å¤å³ç³»
    å¤ä¸ªå³ç³»ä¸­è³å°ä¿ç confidence æå¤§çä¸ä¸ªï¼ä¹ä¿çè¶è¿è®¾å®éå¼ï¼0.5ï¼çå³ç³»
    '''
    one2many_predicate = ['ç¶äº²', 'æ¯äº²', 'ä¸å¤«', 'å¦»å­', 'æä»£', 'æ»é¨å°ç¹', 'äººå£æ°é', 'æå¨åå¸',
                          'æ¹ç¼èª', 'æç«æ¥æ', 'ç®ç§°', 'å½ç±', 'é¥°æ¼', 'è·å¥', 'ä¸æ æ¶é´', 'ç¥¨æ¿', 'éé³']

    one2many_text = []

    for predicate in one2many_predicate:
        one2many_data = []
        for d_json in data:
            sp = {}
            for spo in d_json['spo_list']:
                if spo['predicate'] == predicate:
                    if (spo['subject'], spo['predicate']) not in sp:
                        sp[(spo['subject'], spo['predicate'])] = []
                    sp[(spo['subject'], spo['predicate'])].append(spo['object'])
                    # ä¸å¯¹å¤
                    if len(sp[(spo['subject'], spo['predicate'])]) > 1:
                        one2many_data.append(d_json)
                        break
        for d in data:
            for o_d in one2many_data:
                if d['text'] == o_d['text']:
                    other_spo_list = []
                    sp_prob = {}
                    for spo in o_d['spo_list']:
                        # ä¸ä¸ªæ ·æ¬æå¤ä¸ªä¸å¯¹å¤å³ç³»ï¼å·²è¢«å¤ç
                        if 'conf' not in spo.keys():
                            continue
                        prob = float(spo.pop('conf'))
                        if spo['predicate'] in one2many_predicate:
                            if (spo['subject'], spo['predicate']) not in sp_prob:
                                sp_prob[(spo['subject'], spo['predicate'])] = []
                                sp_prob[(spo['subject'], spo['predicate'])].append((prob, spo))

                            else:
                                if prob > sp_prob[(spo['subject'], spo['predicate'])][0][0] or prob > threshold:
                                    sp_prob[(spo['subject'], spo['predicate'])].append((prob, spo))
                                    sp_prob[(spo['subject'], spo['predicate'])] = sorted(
                                        sp_prob[(spo['subject'], spo['predicate'])], key=lambda x: x[0], reverse=True)
                                    tmp = [p for i, p in enumerate(sp_prob[(spo['subject'], spo['predicate'])]) if
                                           i == 0 or p[0] > threshold]
                                    sp_prob[(spo['subject'], spo['predicate'])] = tmp
                        else:
                            other_spo_list.append(spo)
                    p_spo_list = list(sp_prob.values())
                    tmp_spo_list = []
                    for p_spo in p_spo_list:
                        for p_spo_item in p_spo:
                            tmp_spo_list.append(p_spo_item[1])
                    d['spo_list'] = tmp_spo_list
                    d['spo_list'] += other_spo_list
        one2many_text += [o_d['text'] for o_d in one2many_data]
    # å»é¤éä¸å¯¹å¤ææ¬çæ¦çæ ç­¾
    for d in data:
        if d['text'] not in one2many_text:
            for spo in d['spo_list']:
                spo.pop('conf')
            # print()
    # å»é
    for d in data:
        unique = set()
        unique_spo = []
        for spo in d['spo_list']:
            spo_str = spo['subject'] + spo['predicate']
            for object in list(spo['object'].values()):
                spo_str += object
            if spo_str in unique:
                continue
            else:
                unique.add(spo_str)
                unique_spo.append(spo)
        d['spo_list'] = unique_spo
    return data


def trans2duie2_format(pred_data):
    scheme = {
        "æ¯ä¸é¢æ ¡": {"object_type": {"@value": "å­¦æ ¡"}, "predicate": "æ¯ä¸é¢æ ¡", "subject_type": "äººç©"},
        "åå®¾": {"object_type": {"@value": "äººç©"}, "predicate": "åå®¾", "subject_type": "çµè§ç»¼èº"},
        "éé³": {"object_type": {"inWork": "å½±è§ä½å", "@value": "äººç©"}, "predicate": "éé³", "subject_type": "å¨±ä¹äººç©"},
        "ä¸»é¢æ²": {"object_type": {"@value": "æ­æ²"}, "predicate": "ä¸»é¢æ²", "subject_type": "å½±è§ä½å"},
        "ä»£è¨äºº": {"object_type": {"@value": "äººç©"}, "predicate": "ä»£è¨äºº", "subject_type": "ä¼ä¸/åç"},
        "æå±ä¸è¾": {"object_type": {"@value": "é³ä¹ä¸è¾"}, "predicate": "æå±ä¸è¾", "subject_type": "æ­æ²"},
        "ç¶äº²": {"object_type": {"@value": "äººç©"}, "predicate": "ç¶äº²", "subject_type": "äººç©"},
        "ä½è": {"object_type": {"@value": "äººç©"}, "predicate": "ä½è", "subject_type": "å¾ä¹¦ä½å"},
        "ä¸æ æ¶é´": {"object_type": {"inArea": "å°ç¹", "@value": "Date"}, "predicate": "ä¸æ æ¶é´", "subject_type": "å½±è§ä½å"},
        "æ¯äº²": {"object_type": {"@value": "äººç©"}, "predicate": "æ¯äº²", "subject_type": "äººç©"},
        "ä¸ä¸ä»£ç ": {"object_type": {"@value": "Text"}, "predicate": "ä¸ä¸ä»£ç ", "subject_type": "å­¦ç§ä¸ä¸"},
        "å å°é¢ç§¯": {"object_type": {"@value": "Number"}, "predicate": "å å°é¢ç§¯", "subject_type": "æºæ"},
        "é®æ¿ç¼ç ": {"object_type": {"@value": "Text"}, "predicate": "é®æ¿ç¼ç ", "subject_type": "è¡æ¿åº"},
        "ç¥¨æ¿": {"object_type": {"inArea": "å°ç¹", "@value": "Number"}, "predicate": "ç¥¨æ¿", "subject_type": "å½±è§ä½å"},
        "æ³¨åèµæ¬": {"object_type": {"@value": "Number"}, "predicate": "æ³¨åèµæ¬", "subject_type": "ä¼ä¸"},
        "ä¸»è§": {"object_type": {"@value": "äººç©"}, "predicate": "ä¸»è§", "subject_type": "æå­¦ä½å"},
        "å¦»å­": {"object_type": {"@value": "äººç©"}, "predicate": "å¦»å­", "subject_type": "äººç©"},
        "ç¼å§": {"object_type": {"@value": "äººç©"}, "predicate": "ç¼å§", "subject_type": "å½±è§ä½å"},
        "æ°å": {"object_type": {"@value": "æ°å"}, "predicate": "æ°å", "subject_type": "è¡æ¿åº"},
        "æ­æ": {"object_type": {"@value": "äººç©"}, "predicate": "æ­æ", "subject_type": "æ­æ²"},
        "è·å¥": {"object_type": {"inWork": "ä½å", "onDate": "Date", "@value": "å¥é¡¹", "period": "Number"}, "predicate": "è·å¥",
               "subject_type": "å¨±ä¹äººç©"},
        "æ ¡é¿": {"object_type": {"@value": "äººç©"}, "predicate": "æ ¡é¿", "subject_type": "å­¦æ ¡"},
        "åå§äºº": {"object_type": {"@value": "äººç©"}, "predicate": "åå§äºº", "subject_type": "ä¼ä¸"},
        "é¦é½": {"object_type": {"@value": "åå¸"}, "predicate": "é¦é½", "subject_type": "å½å®¶"},
        "ä¸å¤«": {"object_type": {"@value": "äººç©"}, "predicate": "ä¸å¤«", "subject_type": "äººç©"},
        "æä»£": {"object_type": {"@value": "Text"}, "predicate": "æä»£", "subject_type": "åå²äººç©"},
        "é¥°æ¼": {"object_type": {"inWork": "å½±è§ä½å", "@value": "äººç©"}, "predicate": "é¥°æ¼", "subject_type": "å¨±ä¹äººç©"},
        "é¢ç§¯": {"object_type": {"@value": "Number"}, "predicate": "é¢ç§¯", "subject_type": "è¡æ¿åº"},
        "æ»é¨å°ç¹": {"object_type": {"@value": "å°ç¹"}, "predicate": "æ»é¨å°ç¹", "subject_type": "ä¼ä¸"},
        "ç¥ç±": {"object_type": {"@value": "å°ç¹"}, "predicate": "ç¥ç±", "subject_type": "äººç©"},
        "äººå£æ°é": {"object_type": {"@value": "Number"}, "predicate": "äººå£æ°é", "subject_type": "è¡æ¿åº"},
        "å¶çäºº": {"object_type": {"@value": "äººç©"}, "predicate": "å¶çäºº", "subject_type": "å½±è§ä½å"},
        "ä¿®ä¸å¹´é": {"object_type": {"@value": "Number"}, "predicate": "ä¿®ä¸å¹´é", "subject_type": "å­¦ç§ä¸ä¸"},
        "æå¨åå¸": {"object_type": {"@value": "åå¸"}, "predicate": "æå¨åå¸", "subject_type": "æ¯ç¹"},
        "è£äºé¿": {"object_type": {"@value": "äººç©"}, "predicate": "è£äºé¿", "subject_type": "ä¼ä¸"},
        "ä½è¯": {"object_type": {"@value": "äººç©"}, "predicate": "ä½è¯", "subject_type": "æ­æ²"},
        "æ¹ç¼èª": {"object_type": {"@value": "ä½å"}, "predicate": "æ¹ç¼èª", "subject_type": "å½±è§ä½å"},
        "åºåå¬å¸": {"object_type": {"@value": "ä¼ä¸"}, "predicate": "åºåå¬å¸", "subject_type": "å½±è§ä½å"},
        "å¯¼æ¼": {"object_type": {"@value": "äººç©"}, "predicate": "å¯¼æ¼", "subject_type": "å½±è§ä½å"},
        "ä½æ²": {"object_type": {"@value": "äººç©"}, "predicate": "ä½æ²", "subject_type": "æ­æ²"},
        "ä¸»æ¼": {"object_type": {"@value": "äººç©"}, "predicate": "ä¸»æ¼", "subject_type": "å½±è§ä½å"},
        "ä¸»æäºº": {"object_type": {"@value": "äººç©"}, "predicate": "ä¸»æäºº", "subject_type": "çµè§ç»¼èº"},
        "æç«æ¥æ": {"object_type": {"@value": "Date"}, "predicate": "æç«æ¥æ", "subject_type": "æºæ"},
        "ç®ç§°": {"object_type": {"@value": "Text"}, "predicate": "ç®ç§°", "subject_type": "æºæ"},
        "æµ·æ": {"object_type": {"@value": "Number"}, "predicate": "æµ·æ", "subject_type": "å°ç¹"},
        "å·": {"object_type": {"@value": "Text"}, "predicate": "å·", "subject_type": "åå²äººç©"},
        "å½ç±": {"object_type": {"@value": "å½å®¶"}, "predicate": "å½ç±", "subject_type": "äººç©"},
        "å®æ¹è¯­è¨": {"object_type": {"@value": "è¯­è¨"}, "predicate": "å®æ¹è¯­è¨", "subject_type": "å½å®¶"},
    }
    new_pred_data = []
    ori_test_data = load_data("../../data/ori_data/duie_comp2021_bk/test_data_2.json")

    for sample_idx, sample in tqdm(enumerate(pred_data), "trans 2 duie2 format"):
        new_spo_list = []
        spe_rel_map = {}
        spe_key2predicate = {
            "inWork": {"éé³", "è·å¥", "é¥°æ¼"},
            "inArea": {"ç¥¨æ¿", "ä¸æ æ¶é´"},
            "onDate": {"è·å¥"},
            "period": {"è·å¥"}
        }
        for spo in sample["relation_list"]:
            if spo["predicate"] in spe_key2predicate:
                spe_rel_map.setdefault(spo["predicate"], {})
                spe_rel_map[spo["predicate"]].setdefault(spo["subject"], set()).add(spo["object"])

        for spo in sample["relation_list"]:
            if spo["predicate"] not in spe_key2predicate:
                new_spo = {
                    "predicate": spo["predicate"],
                    "subject": spo["subject"],
                    "object": {"@value": spo["object"], },
                    "conf": spo["conf"],
                }
                for spe_k, pred_set in spe_key2predicate.items():
                    if spo["predicate"] in pred_set and spe_k in spe_rel_map and \
                            spo["subject"] in spe_rel_map[spe_k] and spo["object"] in spe_rel_map[spe_k]:
                        inter_set = spe_rel_map[spe_k][spo["subject"]].intersection(
                            spe_rel_map[spe_k][spo["object"]])
                        if len(inter_set) > 0:
                            new_spo["object"][spe_k] = inter_set.pop()
                sch = scheme[spo["predicate"]]
                new_spo["subject_type"] = sch["subject_type"]
                new_spo["object_type"] = {k: v for k, v in sch["object_type"].items()
                                          if k in new_spo["object"]}
                new_spo_list.append(new_spo)

        # save max conf
        spo_mark2spo = {}
        spo_mark2max_conf = {}
        for spo in new_spo_list:
            conf = spo["conf"]
            del spo["conf"]
            spo_mark = str(spo)
            if spo_mark not in spo_mark2spo or conf > spo_mark2max_conf[spo_mark]:
                spo["conf"] = conf
                spo_mark2spo[spo_mark] = spo
                spo_mark2max_conf[spo_mark] = conf
        new_spo_list = spo_mark2spo.values()

        one_subj_rel = {"é¥°æ¼", "ç¥¨æ¿", "éé³", 'è·å¥'}
        one_subj_rel2obj2cand = {}
        one_obj_rel = {'ç¶äº²', 'æ¯äº²', 'ä¸å¤«', 'å¦»å­', 'æä»£', 'æ»é¨å°ç¹', 'äººå£æ°é', 'æå¨åå¸',
                       'æ¹ç¼èª', 'æç«æ¥æ', 'ç®ç§°', 'å½ç±', 'ä¸æ æ¶é´'}
        one_obj_rel2subj2cand = {}
        filtered_spo_list = []
        for spo in new_spo_list:
            if spo["predicate"] in one_subj_rel:
                one_subj_rel2obj2cand.setdefault(spo["predicate"], {})
                obj = spo["object"]["@value"] + spo["object"].get("inWork", "") \
                    if spo["predicate"] == "é¥°æ¼" else spo["object"]["@value"]
                if obj not in one_subj_rel2obj2cand[spo["predicate"]] or \
                        spo["conf"] > one_subj_rel2obj2cand[spo["predicate"]][obj]["conf"]:
                    one_subj_rel2obj2cand[spo["predicate"]][obj] = spo
            elif spo["predicate"] in one_obj_rel:
                one_obj_rel2subj2cand.setdefault(spo["predicate"], {})
                subj = spo["subject"]
                if subj not in one_obj_rel2subj2cand[spo["predicate"]] or \
                        spo["conf"] > one_obj_rel2subj2cand[spo["predicate"]][subj]["conf"]:
                    one_obj_rel2subj2cand[spo["predicate"]][subj] = spo
            else:
                filtered_spo_list.append(spo)
        for obj2cand in one_subj_rel2obj2cand.values():
            filtered_spo_list.extend(obj2cand.values())
        for subj2cand in one_obj_rel2subj2cand.values():
            filtered_spo_list.extend(subj2cand.values())

        # rm the key conf
        filtered_spo_list = [{k: v for k, v in spo.items() if k != "conf"} for spo in filtered_spo_list]

        # text and spo might have been split with blanks by clean_txt()
        # recover text and spo by original text
        ori_text = ori_test_data[sample_idx]["text"]
        # try:
        assert re.sub("\s", "", sample["text"]) == re.sub("\s", "", ori_text)
        # except Exception:
        #     print("debug")
        for spo in filtered_spo_list:
            if spo["subject"] not in ori_text:
                segs = [s for s in utils.search_segs(spo["subject"], ori_text) if s.strip() != ""]
                spo["subject"] = "".join(segs)
                assert spo["subject"] in ori_text

            for k, v in spo["object"].items():
                if v not in ori_text:
                    segs = [s for s in utils.search_segs(v, ori_text) if s.strip() != ""]
                    spo["object"][k] = "".join(segs)
                    assert spo["object"][k] in ori_text

        new_sample = {
            "text": ori_text,
            "spo_list": Preprocessor.unique_list(filtered_spo_list),
        }
        new_pred_data.append(new_sample)

    # new_pred_data = rule_fix_one2many(new_pred_data)
    return new_pred_data


def merge_events4doc_ee(event_list, sim_entity=0.6):
    new_event_list = []
    event_type_dict = {}
    for event_dict in event_list:
        new_argument_dict = {}
        for argument_dict in event_dict["argument_list"]:
            new_argument_dict.setdefault(argument_dict["type"], []).append(argument_dict["text"])

        event_type_dict.setdefault(event_dict["event_type"], []).append(new_argument_dict)

    for event_type, event_so_list in event_type_dict.items():
        # event_so_list = sorted(event_so_list, key=lambda x: len(x), reverse=False)
        drop_duplicate_event(event_so_list, sim_entity)
        for event_so_dict in event_so_list:
            event_dict = {}
            event_dict["event_type"] = event_type
            event_dict["argument_list"] = []
            for role, arg_list in event_so_dict.items():
                new_arg_dict = {}
                arg_list_sort = sorted(arg_list, key=lambda x: len(x), reverse=True)
                for index, argument in enumerate(arg_list_sort):
                    add_new_arg = True
                    new_arg_dict_copy = copy.deepcopy(new_arg_dict)
                    for new_arg, new_arg_value in new_arg_dict_copy.items():
                        new_arg_value_copy = copy.deepcopy(new_arg_value)
                        new_arg_value_copy.append(new_arg)
                        break_circle = False
                        for temp_arg in new_arg_value_copy:
                            condition = set(argument).issubset(set(temp_arg)) or set(temp_arg).issubset(
                                set(argument))
                            if condition:
                                break_circle = True
                                if len(argument) > len(new_arg):
                                    new_arg_dict[argument] = new_arg_value_copy
                                    new_arg_dict.pop(new_arg)
                                    add_new_arg = False
                                    break
                                else:
                                    new_arg_dict.setdefault(new_arg, []).append(argument)
                                    add_new_arg = False
                                    break

                        if break_circle: break
                    if add_new_arg:
                        new_arg_dict.setdefault(argument, [])
                for new_arg, new_arg_value in new_arg_dict.items():
                    # if new_arg_value:
                    #     print("ä¿çï¼", new_arg)
                    #     print("å é¤ï¼", new_arg_value)
                    #     print("--")
                    argument_dict = {}
                    argument_dict["type"] = role
                    argument_dict["text"] = new_arg
                    if argument_dict not in event_dict["argument_list"]:
                        event_dict["argument_list"].append(argument_dict)
            new_event_list.append(event_dict)
    return new_event_list


def drop_duplicate_event(event_so_list, sim_entity):
    event_so_list_copy = copy.deepcopy(event_so_list)
    for i in range(len(event_so_list_copy)):

        for j in range(i + 1, len(event_so_list_copy)):
            is_duplicate = True

            s_i_role_set = set()
            for event_s_i, event_o_i in event_so_list_copy[i].items():
                s_i_role_set.add(event_s_i)
            s_j_role_set = set()
            # å¦æä¸¤ä¸ªäºä»¶æ²¡æç¸åroleï¼å°±åå¹¶ï¼å¦ææç¸åroleï¼å¤æ­roleå¯¹åºçvalueæ¯å¦æéå¤çï¼æéå¤çå°±åå¹¶ã
            # TODO å®ä½éä¹æäºï¼å¯ä»¥å å¥
            for event_s_j, event_o_j in event_so_list_copy[j].items():
                s_j_role_set.add(event_s_j)
                event_so_i_value = event_so_list_copy[i].get(event_s_j, [])
                if event_so_i_value:  # å¤æ­æä¸ªç¸åroleçvalueæ¯å¦æéå¤ç
                    has_duplicate_value = False
                    for entity_i in event_so_i_value:
                        for entity_j in event_o_j:
                            if (entity_i in entity_j) or \
                                    (entity_j in entity_i) or \
                                    jaccard_similarity(entity_i, entity_j) > sim_entity or \
                                    set(entity_i).issubset(entity_j) or \
                                    set(entity_j).issubset(entity_i):
                                has_duplicate_value = True
                                break
                        if has_duplicate_value:
                            break
                    if not has_duplicate_value:
                        is_duplicate = False
                        break
            if is_duplicate:  # ä¸¤ä¸ªç»æè¿è¡åå¹¶
                for event_s_i, event_o_i in event_so_list_copy[i].items():
                    if event_s_i not in event_so_list[j]:
                        event_so_list[j].update({event_s_i: event_o_i})
                    else:
                        for event_o in event_o_i:
                            if event_o not in event_so_list[j][event_s_i]:
                                event_so_list[j][event_s_i].append(event_o)

                event_so_list.remove(event_so_list_copy[i])
                drop_duplicate_event(event_so_list, sim_entity)
                return
            else:
                continue
    return


def jaccard_similarity(s1, s2):
    def add_space(s):
        return ' '.join(list(s))

    # å°å­ä¸­é´å å¥ç©ºæ ¼
    s1, s2 = add_space(s1), add_space(s2)
    # è½¬åä¸ºTFç©éµ
    cv = CountVectorizer(tokenizer=lambda s: s.split())
    corpus = [s1, s2]
    vectors = cv.fit_transform(corpus).toarray()
    # è·åè¯è¡¨åå®¹
    ret = cv.get_feature_names()
    # print(ret)
    # æ±äº¤é
    numerator = np.sum(np.min(vectors, axis=0))
    # æ±å¹¶é
    denominator = np.sum(np.max(vectors, axis=0))
    # è®¡ç®æ°å¡å¾·ç³»æ°
    return 1.0 * numerator / denominator


def trans2duee_fin_format(data):
    new_data = []
    for sample in data:
        new_sample = {
            "id": sample["id"],
            "event_list": [],
        }
        for event in sample["event_list"]:
            if len(event["argument_list"]) == 0:
                continue
            arg_list = utils.unique_list([{"type": arg["type"], "text": arg["text"]} for arg in event["argument_list"]])
            event_type = event["event_type"]
            if event_type in {"æ­£å¼ä¸å¸", "ç­¹å¤ä¸å¸", "ç»æ­¢ä¸å¸", "æåä¸å¸"}:
                arg_list.append({"type": "ç¯è", "text": event_type})
                event_type = "å¬å¸ä¸å¸"

            new_sample["event_list"].append({
                "event_type": event_type,
                "argument_list": sorted(arg_list, key=lambda x: str(x)),
            })
        event_list = merge_events4doc_ee(utils.unique_list(new_sample["event_list"]), 0.6)
        new_event_list = []
        for event in event_list:
            new_arg_list = [{"role": arg["type"], "argument": arg["text"]} for arg in event["argument_list"]]
            new_event_list.append({
                "event_type": event["event_type"],
                "arguments": new_arg_list,
            })
        new_sample["event_list"] = new_event_list
        new_data.append(new_sample)
    return new_data


def convert_cmeee():
    data_in_dir = "../../data/ori_data/CMeEE_bk"
    train_path = os.path.join(data_in_dir, "CMeEE_train.json")
    valid_path = os.path.join(data_in_dir, "CMeEE_dev.json")
    test_path = os.path.join(data_in_dir, "CMeEE_test.json")
    train_data = load_data(train_path)
    valid_data = load_data(valid_path)
    test_data = load_data(test_path)

    def trans(data):
        new_data = []
        for sample in tqdm(data, desc="transform CMeEE"):
            text = sample["text"]
            new_sample = {
                "text": text,
            }
            if "entities" in sample:
                new_entity_list = []
                for ent in sample["entities"]:
                    char_sp = [ent["start_idx"], ent["end_idx"] + 1]
                    if char_sp[0] == char_sp[1] or ent["entity"] == "":
                        continue
                    assert text[char_sp[0]:char_sp[1]] == ent["entity"]
                    new_entity_list.append({
                        "text": ent["entity"],
                        "char_span": char_sp,
                        "type": ent["type"],
                    })
                new_sample["entity_list"] = new_entity_list
            new_data.append(new_sample)
        return new_data

    train_data = trans(train_data)
    valid_data = trans(valid_data)
    test_data = trans(test_data)
    return train_data, valid_data, test_data


def trans2cmeee_format():
    in_path = "../../data/res_data/CMeEE/ner+SNER+TSNER/3sax9k60/model_state_dict_5_66.62/test_data.json"
    out_path = "../../data/res_data/CMeEE/ner+SNER+TSNER/3sax9k60/model_state_dict_5_66.62/pred_data_formatted.json"
    data = load_data(in_path)
    new_data = []
    for sample in data:
        new_sample = {
            "text": sample["text"],
            "entities": [],
        }
        for ent in sample["entity_list"]:
            new_sample["entities"].append({
                "start_idx": ent["char_span"][0],
                "end_idx": ent["char_span"][1] - 1,
                "type": ent["type"],
                "entity": ent["text"]
            })
        new_data.append(new_sample)
    json.dump(new_data, open(out_path, "w", encoding="utf-8"), ensure_ascii=False, indent=2)


def convert_tfboys_baselines2normal_format(dataset, baseline_name):
    ac_path = "../../../data/tfboys_baselines/{}/{}/ac_prediction.json".format(dataset, baseline_name)
    ed_path = "../../../data/tfboys_baselines/{}/{}/ed_prediction.json".format(dataset, baseline_name)
    ed_data = load_data(ed_path)
    ac_data = load_data(ac_path)
    sent_id2event_pred = {}
    sent_id2event_gold = {}

    print(">>>>>>>>>>> baseline: {} >>>>>>>>>>>>>>".format(baseline_name))
    print("ed_pred: {}".format(len(ed_data["prediction"])))
    print("ed_ground: {}".format(len(ed_data["ground_truth"])))
    print("ac_pred: {}".format(len(ac_data["prediction"])))
    print("ac_ground: {}".format(len(ac_data["ground_truth"])))

    for pred in ed_data["prediction"]:
        sent_id2event_pred.setdefault(pred[0], {})
        tri_event_str = "{},{},{}".format(pred[1], pred[2], pred[3])
        sent_id2event_pred[pred[0]][tri_event_str] = {
            "trigger": "unk",
            "trigger_tok_span": [pred[1], pred[2]],
            "event_type": pred[3],
            "argument_list": [],
        }

    for pred in ac_data["prediction"]:
        tri_event_str = "{},{},{}".format(pred[2], pred[3], pred[1])
        if pred[0] not in sent_id2event_pred or tri_event_str not in sent_id2event_pred[pred[0]]:
            continue

        sent_id2event_pred[pred[0]][tri_event_str]["argument_list"].append({
            "tok_span": [pred[4], pred[5]],
            "type": pred[6],
        })

    for gold in ed_data["ground_truth"]:
        sent_id2event_gold.setdefault(gold[0], {})
        tri_event_str = "{},{},{}".format(gold[1], gold[2], gold[3])
        sent_id2event_gold[gold[0]][tri_event_str] = {
            "trigger": "unk",
            "trigger_tok_span": [gold[1], gold[2]],
            "event_type": gold[3],
            "argument_list": [],
        }

    for gold in ac_data["ground_truth"]:
        tri_event_str = "{},{},{}".format(gold[2], gold[3], gold[1])
        if gold[0] not in sent_id2event_gold or tri_event_str not in sent_id2event_gold[gold[0]]:
            continue

        sent_id2event_gold[gold[0]][tri_event_str]["argument_list"].append({
            "tok_span": [gold[4], gold[5]],
            "type": gold[6],
        })

    sent_id2events_pred = {}
    sent_id2events_gold = {}

    for sent_id, event in sent_id2event_pred.items():
        sent_id2events_pred.setdefault(sent_id, []).extend(event.values())

    for sent_id, event in sent_id2event_gold.items():
        sent_id2events_gold.setdefault(sent_id, []).extend(event.values())

    pred_data, gold_data = [], []
    for sent_id, gold_events in sent_id2events_gold.items():
        gold_data.append({"id": sent_id, "event_list": gold_events})
        pred_events = sent_id2events_pred.get(sent_id, [])
        pred_data.append({"id": sent_id, "event_list": pred_events})
        if sent_id in sent_id2events_pred:
            del sent_id2events_pred[sent_id]

    for sent_id, pred_events in sent_id2events_pred.items():
        pred_data.append({"id": sent_id, "event_list": pred_events})
        gold_events = sent_id2events_gold.get(sent_id, [])
        gold_data.append({"id": sent_id, "event_list": gold_events})

    def convert_sent_id(sent_id):
        return re.sub('(.*?)\-(.*)', r"\1_\2", sent_id)

    for sample in pred_data:
        sample["id"] = convert_sent_id(sample["id"])
    for sample in gold_data:
        sample["id"] = convert_sent_id(sample["id"])
    return pred_data, gold_data


def convert_ace05_lu(data):
    new_data = []
    for k, article in tqdm(data.items()):
        trigger_bio_tags = [lab.split("_")[0] for lab in article["event_label_list"]]
        trigger_bio_str = "".join(trigger_bio_tags)
        text = " ".join(article["word_list"])

        # find all events
        event_list = []
        for m in re.finditer("BI*", trigger_bio_str):
            trigger_wd_span = [m.span()[0], m.span()[1]]
            trigger_type = article["event_label_list"][trigger_wd_span[0]].split("_")[1]
            even_word_idx_list = [m.span()[0], m.span()[1]]
            trigger_start = trigger_wd_span[0]
            arguments = article["event_argument_list"][trigger_start]
            arguments_new = []
            for arg in arguments:
                arg_start = trigger_start + arg["start"]
                arg_end = trigger_start + arg["end"]
                even_word_idx_list.append(arg_start)
                even_word_idx_list.append(arg_end)
                arg_text = " ".join(article["word_list"][arg_start:arg_end])
                try:
                    assert arg_text == arg["text"].lower()
                except Exception:
                    print("extr_text: {} != ori_text: {}".format(arg_text, arg["text"].lower()))

                arguments_new.append({
                    "text": arg_text,
                    "wd_span": [arg_start, arg_end],
                    "type": arg["type"],
                })
            even_word_idx_list = sorted(even_word_idx_list)
            event = {
                "event_span": [even_word_idx_list[0], even_word_idx_list[-1]],
                "trigger": " ".join(article["word_list"][trigger_wd_span[0]:trigger_wd_span[1]]),
                "event_type": trigger_type,
                "trigger_wd_span": trigger_wd_span,
                "argument_list": arguments_new,
            }
            event_list.append(event)

        for sent_idx, sen_ext in enumerate(article["sentence_extents"]):
            word_list = article["word_list"][sen_ext[0]:sen_ext[1]]
            ner_tag_list = article["entity_label_list"][sen_ext[0]:sen_ext[1]]
            pos_tag_list = article["pos_tag_list"][sen_ext[0]:sen_ext[1]]
            dep_list = article["dependencies"][sen_ext[0]:sen_ext[1]]
            sent = " ".join(word_list)

            tok2char_span = WhiteWordTokenizer.get_tok2char_span_map(word_list)

            event_list_sent = []
            for event_ in event_list:
                event = copy.copy(event_)
                if event["event_span"][0] >= sen_ext[0] and event["event_span"][1] <= sen_ext[1]:
                    for arg in event["argument_list"]:
                        arg["wd_span"] = [arg["wd_span"][0] - sen_ext[0], arg["wd_span"][1] - sen_ext[0]]
                        char_span_list = tok2char_span[arg["wd_span"][0]:arg["wd_span"][1]]
                        arg["char_span"] = [char_span_list[0][0], char_span_list[-1][1]]
                        # try:
                        assert sent[arg["char_span"][0]:arg["char_span"][1]] == arg["text"]
                        # except Exception:
                        #     print("debug!")

                    del event["event_span"]

                    event["trigger_wd_span"] = [event["trigger_wd_span"][0] - sen_ext[0],
                                                event["trigger_wd_span"][1] - sen_ext[0]]
                    char_span_list = tok2char_span[event["trigger_wd_span"][0]:event["trigger_wd_span"][1]]
                    event["trigger_char_span"] = [char_span_list[0][0], char_span_list[-1][1]]
                    assert sent[event["trigger_char_span"][0]:event["trigger_char_span"][1]] == event["trigger"]

                    event_list_sent.append(event)
            new_data.append({
                "id": "{}_{}".format(article["docid"], sent_idx),
                "text": sent,
                "word_list": word_list,
                "word2char_span": tok2char_span,
                "ner_tag_list": ner_tag_list,
                "pos_tag_list": pos_tag_list,
                "dependency_list": dep_list,
                "event_list": event_list_sent,
            })
    return new_data


def convert_ace05_dygie():
    stanza_nlp = stanza.Pipeline(lang='en',
                                 processors='tokenize,mwt,pos,lemma,depparse',
                                 tokenize_pretokenized=True)

    # 'AFP_ENG_20030401.0476.7'

    def convert_format(data):
        sample_list = []
        for doc in data:
            doc_stz_res = stanza_nlp(doc["sentences"])
            for sent_id, word_list in tqdm(enumerate(doc["sentences"]), desc="convert ace05_dygie"):
                sent_stz_res = doc_stz_res.sentences[sent_id]

                sent_start_idx = doc["_sentence_start"][sent_id]
                word2char_span = WhiteWordTokenizer.get_tok2char_span_map(word_list)
                sample = {
                    "id": "{}.{}".format(doc["doc_key"], sent_id),
                    "text": " ".join(word_list),
                    "word_list": word_list,
                    "word2char_span": word2char_span,
                    "pos_tag_list": [w.xpos for w in sent_stz_res.words],
                    "dependency_list": [[w.head - (wid + 1) if w.head != 0 else 0, w.deprel]
                                        for wid, w in enumerate(sent_stz_res.words)],
                }

                def wsp2csp(wsp):
                    char_sp_slice = word2char_span[wsp[0]:wsp[1]]
                    return [char_sp_slice[0][0], char_sp_slice[-1][1]]

                ent_list = []
                ner_tag_list = ["O" for _ in word_list]
                for ent in doc["ner"][sent_id]:
                    word_sp = [ent[0] - sent_start_idx, ent[1] + 1 - sent_start_idx]
                    ent_type = ent[2]
                    for word_idx in range(word_sp[0], word_sp[1]):
                        htag = "B" if word_idx == word_sp[0] else "I"
                        ner_tag_list[word_idx] = "{}-{}".format(htag, ent_type)

                    char_sp = wsp2csp(word_sp)
                    ent_txt_ext = sample["text"][char_sp[0]:char_sp[1]]
                    assert ent_txt_ext == " ".join(word_list[word_sp[0]:word_sp[1]])
                    ent_list.append({
                        "text": ent_txt_ext,
                        "type": ent_type,
                        "char_span": char_sp,
                    })
                sample["ner_tag_list"] = ner_tag_list
                sample["entity_list"] = ent_list

                rel_list = []
                for rel in doc["relations"][sent_id]:
                    sub_wd_sp = [rel[0] - sent_start_idx, rel[1] + 1 - sent_start_idx]
                    obj_wd_sp = [rel[2] - sent_start_idx, rel[3] + 1 - sent_start_idx]
                    sub_ch_sp = wsp2csp(sub_wd_sp)
                    obj_ch_sp = wsp2csp(obj_wd_sp)
                    sub_txt_ext = sample["text"][sub_ch_sp[0]:sub_ch_sp[1]]
                    assert sub_txt_ext == " ".join(word_list[sub_wd_sp[0]:sub_wd_sp[1]])
                    obj_txt_ext = sample["text"][obj_ch_sp[0]:obj_ch_sp[1]]
                    assert obj_txt_ext == " ".join(word_list[obj_wd_sp[0]:obj_wd_sp[1]])
                    rel_list.append({
                        "subject": sub_txt_ext,
                        "object": obj_txt_ext,
                        "predicate": rel[4],
                        "subj_char_span": sub_ch_sp,
                        "obj_char_span": obj_ch_sp,
                    })
                sample["relation_list"] = rel_list

                event_list = []
                for event in doc["events"][sent_id]:
                    event_type = event[0][1]
                    tri_wsp = [event[0][0] - sent_start_idx, event[0][0] + 1 - sent_start_idx]
                    tri_csp = wsp2csp(tri_wsp)
                    tri_txt_ext = sample["text"][tri_csp[0]:tri_csp[1]]
                    assert tri_txt_ext == " ".join(word_list[tri_wsp[0]:tri_wsp[1]])
                    event_dict = {
                        "event_type": event_type,
                        "trigger_char_span": tri_csp,
                        "trigger": tri_txt_ext,
                    }
                    arg_list = []
                    for arg in event[1:]:
                        arg_wsp = [arg[0] - sent_start_idx, arg[1] + 1 - sent_start_idx]
                        arg_csp = wsp2csp(arg_wsp)
                        arg_txt_ext = sample["text"][arg_csp[0]:arg_csp[1]]
                        assert arg_txt_ext == " ".join(word_list[arg_wsp[0]:arg_wsp[1]])
                        arg_list.append({
                            "text": arg_txt_ext,
                            "type": arg[2],
                            "char_span": arg_csp,
                        })
                    event_dict["argument_list"] = arg_list
                    event_list.append(event_dict)
                sample["event_list"] = event_list
                sample_list.append(sample)
        return sample_list

    path_format = "../../data/ori_data/ace2005_dygiepp/{}/{}.json"
    for dataset_type in {"default_settings", "span_extent", "span_times_values", "span_times_values_pro"}:
        train_data = load_data(path_format.format(dataset_type, "train"))
        valid_data = load_data(path_format.format(dataset_type, "dev"))
        test_data = load_data(path_format.format(dataset_type, "test"))

        new_train, new_valid, new_test = convert_format(train_data), convert_format(valid_data), convert_format(test_data)
        sv_path_format = "../../data/normal_data/ace2005_dygiepp_{}/{}.json"
        save_as_json_lines(new_train, sv_path_format.format(dataset_type, "train_data"))
        save_as_json_lines(new_valid, sv_path_format.format(dataset_type, "valid_data"))
        save_as_json_lines(new_test, sv_path_format.format(dataset_type, "test_data"))


def rm_triggers(data):
    for sample in data:
        new_event_list = []
        for event in sample["event_list"]:
            if len(event["argument_list"]) == 0:
                continue
            new_event = {}
            for k, v in event.items():
                if k == "trigger_type":
                    new_event["event_type"] = v
                elif "trigger" in k:  # skip all trigger related keys
                    pass
                else:
                    new_event[k] = v
            new_event_list.append(new_event)
        sample["event_list"] = new_event_list
    return data


def convert_fewfc_format(data):
    def mysub(txt):
        return re.sub("ï¿½", "_", txt)

    new_data = []
    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> change format
    for sample in data:
        new_sample = {
            "id": sample["id"],
            "text": mysub(sample["content"]),
            "event_list": [],
        }
        for event in sample["events"]:
            if sample["id"] == 'fe0b55bd924aec7a8c05413408ea285d' and event["trigger"]["word"] == "ç»­å":
                event = {
                    'type': 'è¡ä»½è¡æè½¬è®©',
                    'trigger': {'span': [95, 97], 'word': 'ç»­å'},
                    'args': {
                        'sub-org': [{'span': [78, 83], 'word': ',å·¥é¶å½é'}],
                        'date': [{'span': [73, 78], 'word': '7æ18æ¥'}],
                        'money': [{'span': [85, 91], 'word': 'è¿2.5æ¸¯å'}],
                        'number': [{'span': [98, 104], 'word': '121.5ä¸'}]
                    }
                }

            new_event = {
                "event_type": event["type"],
                "trigger": mysub(event["trigger"]["word"]),
                "trigger_char_span": event["trigger"]["span"],
                "argument_list": [],
            }

            for role, args in event["args"].items():
                for arg in args:
                    if role == "number" and arg["word"] == "0.34%" and sample[
                        "id"] == '710cc2007052c48d2ee0d24e85f44977':
                        new_event["argument_list"].append({
                            "text": "0.16%",
                            "char_span": [117, 122],
                            "type": "proportion",
                        })
                        continue
                    if role == "number" and arg["word"] == "æ¶ä»£åé«" and sample[
                        "id"] == '3731f3afe87152dd45d0bd0919bc75eb':
                        new_event["argument_list"].append({
                            "text": mysub(arg["word"]),
                            "char_span": arg["span"],
                            "type": "target-company",
                        })
                        new_event["argument_list"].append({
                            "text": "65%",
                            "char_span": [15, 18],
                            "type": "proportion",
                        })
                        continue
                    new_event["argument_list"].append({
                        "text": mysub(arg["word"]),
                        "char_span": arg["span"],
                        "type": role,
                    })
            new_sample["event_list"].append(new_event)
        new_data.append(new_sample)

    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>> generate entity list
    # arg_type2ent_type = {
    #     "collateral": "collateral",
    #     "proportion": "proportion",
    #     "obj-org": "organization",
    #     "sub-org": "organization",
    #     "number": "number",
    #     "date": "date",
    #     "target-company": "organization",
    #     "sub": "organization",
    #     "obj": "organization",
    #     "share-org": "proportion",
    #     "money": "money",
    #     "title": "title",
    #     "sub-per": "person",
    #     "obj-per": "person",
    #     "share-per": "proportion",
    #     "institution": "institution",
    #     "way": "way",
    #     "amount": "money",
    # }
    # for sample in new_data:
    #     ent_list = []
    #     ent_text_set = set()
    #     for event in sample["event_list"]:
    #         for arg in event["argument_list"]:
    #             ent_list.append({
    #                 "text": arg["text"],
    #                 "char_span": arg["char_span"],
    #                 "type": arg_type2ent_type[arg["type"]],
    #             })
    #             ent_text_set.add(arg["text"])
    #     ent_list = utils.unique_list(ent_list)
    #     sample["entity_list"] = ent_list
    #

    # codes = ChineseWordTokenizer.tokenize_plus(sample["text"], ent_text_set)
    # sample["word2char_span"] = codes["word2char_span"]
    # sample["word_list"] = codes["word_list"]
    # char2word_span = utils.get_char2tok_span(sample["word2char_span"])
    #
    # ner_tag_list = ["O" for _ in sample["word_list"]]
    # for ent in ent_list:
    #     word_slice = char2word_span[ent["char_span"][0]:ent["char_span"][1]]
    #     word_sp = [word_slice[0][0], word_slice[-1][1]]
    #     ent_type = ent["type"]
    #     for w_idx in range(word_sp[0], word_sp[1]):
    #         h_tag = "B" if w_idx == word_sp[0] else "I"
    #         ner_tag_list[w_idx] = "{}-{}".format(h_tag, ent_type)
    # sample["ner_tag_list"] = ner_tag_list

    return new_data


def convert_few_fc():
    train_path = "../../data/ori_data/few_fc_bk/train.json"
    valid_path = "../../data/ori_data/few_fc_bk/dev.json"
    test_path = "../../data/ori_data/few_fc_bk/test.json"
    train_data = load_data(train_path)
    valid_data = load_data(valid_path)
    test_data = load_data(test_path)

    # >>>>>>>>>>>>>>>>>>>>> trans
    new_train = convert_fewfc_format(train_data)
    new_valid = convert_fewfc_format(valid_data)
    new_test = convert_fewfc_format(test_data)

    Preprocessor.add_ch_parsed_res(new_train + new_valid + new_test)

    train_sv = "../../data/ori_data/few_fc/train_data.json"
    valid_sv = "../../data/ori_data/few_fc/valid_data.json"
    test_sv = "../../data/ori_data/few_fc/test_data.json"

    save_as_json_lines(new_train, train_sv)
    save_as_json_lines(new_valid, valid_sv)
    save_as_json_lines(new_test, test_sv)
    # # >>>>>>>>>>>>>>>>>>>> one trigger to multiple event
    # one_tri_mul_event_samples = []
    # one_tri_mul_insts_in_same_event_type_samples = []
    # for sample in new_test:
    #     trigger_list = [str(event["trigger_char_span"]) for event in sample["event_list"]]
    #     tri_etype_list = [str(event["trigger_char_span"] + [event["event_type"]]) for event in sample["event_list"]]
    #     if len(set(trigger_list)) < len(trigger_list):
    #         one_tri_mul_event_samples.append(sample)
    #     if len(set(tri_etype_list)) < len(tri_etype_list):
    #         one_tri_mul_insts_in_same_event_type_samples.append(sample)
    #
    # otm_same_type_len = len(one_tri_mul_insts_in_same_event_type_samples)
    # otm_same_type_rate = otm_same_type_len / len(new_test)
    # otm_len = len(one_tri_mul_event_samples)
    # otm_rate = otm_len / len(new_test)
    # print(otm_len, otm_rate, otm_same_type_len, otm_same_type_rate)

    # otm_sv_path = "../../data/ori_data/few_fc/test_otm.json"
    # otm_same_type_sv_path = "../../data/ori_data/few_fc/test_otm_st.json"
    # save_as_json_lines(one_tri_mul_event_samples, otm_sv_path)
    # save_as_json_lines(one_tri_mul_insts_in_same_event_type_samples, otm_same_type_sv_path)


def convert_duuie():
    dir = "../../data/ori_data/duuie/"
    train_data_path = dir + "train.json"
    val_data_path = dir + "val.json"
    train_data = load_data(train_data_path)
    val_data = load_data(val_data_path)
    word_tokenizer = ChineseWordTokenizer()

    train_data_norm = []
    for sample in tqdm(train_data):
        new_sample = {
            "id": "train_{}".format(len(train_data_norm)),
            "text": sample["text"],
        }
        assert len(sample["text"]) == len(sample["tokens"]) and "".join(sample["tokens"]) == sample["text"]
        entity_list = []
        for ent in sample["entity"]:
            new_ent = {
                "text": ent["text"],
                "type": ent["type"],
                "char_span": utils.ids2span(ent["offset"]),
            }
            assert new_ent["text"] == new_sample["text"][new_ent["char_span"][0]:new_ent["char_span"][1]]
            entity_list.append(new_ent)
        new_sample["entity_list"] = entity_list

        rel_list = []
        for rel in sample["relation"]:
            assert len(rel["args"]) == 2
            new_rel = {
                "subject": rel["args"][0]["text"],
                "predicate": rel["type"],
                "object": rel["args"][1]["text"],
                "subj_char_span": utils.ids2span(rel["args"][0]["offset"]),
                "obj_char_span": utils.ids2span(rel["args"][1]["offset"]),
            }
            assert new_rel["subject"] == new_sample["text"][new_rel["subj_char_span"][0]:new_rel["subj_char_span"][1]]
            assert new_rel["object"] == new_sample["text"][new_rel["obj_char_span"][0]:new_rel["obj_char_span"][1]]
            rel_list.append(new_rel)
        new_sample["relation_list"] = rel_list

        event_list = []
        for event in sample["event"]:
            assert "text" in event
            new_event = {
                "event_type": event["type"],
                "trigger": event["text"],
                "trigger_char_span": utils.ids2span(event["offset"]),
            }
            assert new_event["trigger"] == new_sample["text"][new_event["trigger_char_span"][0]:new_event["trigger_char_span"][1]]

            arg_list = []
            for arg in event["args"]:
                new_arg = {
                    "type": arg["type"],
                    "text": arg["text"],
                    "char_span": utils.ids2span(arg["offset"]),
                }
                assert new_arg["text"] == new_sample["text"][
                                               new_arg["char_span"][0]:new_arg["char_span"][1]]
                arg_list.append(new_arg)
            new_event["argument_list"] = arg_list
            event_list.append(new_event)
        new_sample["event_list"] = event_list

        codes = word_tokenizer.tokenize_plus(new_sample["text"], span_list=utils.get_all_possible_char_spans(new_sample),
                                             tokenize_func=utils.get_lac("seg").run)
        new_sample = {**new_sample, **codes}
        train_data_norm.append(new_sample)

    return train_data_norm


if __name__ == "__main__":
    train_data = convert_duuie()
    # # ================= trans ace 2005 =============================
    # in_fold_name = "ace2005_35_bk"
    # ori_train_data = json.load(open("../../data/ori_data/{}/train_data.json".format(in_fold_name), "r", encoding="utf-8"))
    # ori_valid_data = json.load(open("../../data/ori_data/{}/dev_data.json".format(in_fold_name), "r", encoding="utf-8"))
    # ori_test_data = json.load(open("../../data/ori_data/{}/test_data.json".format(in_fold_name), "r", encoding="utf-8"))
    # new_train_data = convert_ace05_lu(ori_train_data)
    # new_valid_data = convert_ace05_lu(ori_valid_data)
    # new_test_data = convert_ace05_lu(ori_test_data)

    # out_fold_name = "ace2005_35"
    # json.dump(new_train_data, open("../../data/ori_data/{}/train_data.json".format(out_fold_name), "w", encoding="utf-8"))
    # json.dump(new_valid_data, open("../../data/ori_data/{}/valid_data.json".format(out_fold_name), "w", encoding="utf-8"))
    # json.dump(new_test_data, open("../../data/ori_data/{}/test_data.json".format(out_fold_name), "w", encoding="utf-8"))



    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> analysis >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
    # gold_id2events_dict = {sample["id"]:sample["event_list"] for sample in gold_data}
    # tbee_id2events_dict = {sample["id"]: sample["event_list"] for sample in tbee_pred_data}
    # tfree_id2events_dict = {}
    # mul_tri_id2events_dict = {}
    # id2res = {}
    # for sample in tfee_pred_data:
    #     id2res[sample["id"]] = {
    #         "text": sample["text"],
    #         "tf_pred": sample["event_list"],
    #         "tb_pred": tbee_id2events_dict[sample["id"]],
    #         "gold": gold_id2events_dict[sample["id"]],
    #     }
    #     for event in sample["event_list"]:
    #         triggers = event.get("trigger_list", [])
    #         if len(triggers) == 0 and len(event["argument_list"]) > 0:
    #             tfree_id2events_dict[sample["id"]] = {
    #                 "text": sample["text"],
    #                 "pred": sample["event_list"],
    #                 "gold": gold_id2events_dict[sample["id"]],
    #             }
    #         elif len(triggers) > 1:
    #             mul_tri_id2events_dict[sample["id"]] = {
    #                 "text": sample["text"],
    #                 "pred": sample["event_list"],
    #                 "gold": gold_id2events_dict[sample["id"]],
    #             }
    #
    # json.dump(tfree_id2events_dict, open("../../data/res_data/analysis/tfboys/tri_free.json", "w", encoding="utf-8"), ensure_ascii=False)
    # json.dump(mul_tri_id2events_dict, open("../../data/res_data/analysis/tfboys/mul_tri.json", "w", encoding="utf-8"), ensure_ascii=False)
    # >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

    # # # ============================ rm triggers in ace05 ======================================
    # train_path = "../../data/preprocessed_data/ace2005_dygiepp_default_settings/train_data.json"
    # dev_path = "../../data/preprocessed_data/ace2005_dygiepp_default_settings/valid_data.json"
    # test_path = "../../data/preprocessed_data/ace2005_dygiepp_default_settings/test_data.json"
    # train_data = load_data(train_path)
    # valid_data = load_data(dev_path)
    # test_data = load_data(test_path)
    # sv_dir = "../../data/preprocessed_data/ace2005_dygiepp_default_settings_tf"
    # if not os.path.exists(sv_dir):
    #     os.mkdir(sv_dir)
    #
    # new_train_data = rm_triggers(train_data)
    # new_valid_data = rm_triggers(valid_data)
    # new_test_data = rm_triggers(test_data)
    # train_sv_path = os.path.join(sv_dir, "train_data.json")
    # dev_sv_path = os.path.join(sv_dir, "valid_data.json")
    # test_sv_path = os.path.join(sv_dir, "test_data.json")
    # save_as_json_lines(new_train_data, train_sv_path)
    # save_as_json_lines(new_valid_data, dev_sv_path)
    # save_as_json_lines(new_test_data, test_sv_path)

    pass
