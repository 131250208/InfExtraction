import json
import os
from InfExtraction.modules.preprocess import Preprocessor, WhiteWordTokenizer, ChineseWordTokenizer
from InfExtraction.modules.utils import load_data, save_as_json_lines, merge_spans
from InfExtraction.modules import utils
from tqdm import tqdm
import random
from tqdm import tqdm
from pprint import pprint
import copy
import re
import jieba
import string
from pattern.en import lexeme, lemma
import itertools
import matplotlib.pyplot as plt
import time


def trans_genia():
    data_in_dir = "../../data/ori_data/genia_bk"
    data_out_dir = "../../data/ori_data/genia"
    in_file2out_file = {
        "train_dev.genia.jsonlines": "train_data.json",
        "test.genia.jsonlines": "test_data.json",
    }

    # load data
    filename2data = {}
    for in_filename, out_filename in in_file2out_file.items():
        int_path = os.path.join(data_in_dir, in_filename)
        out_path = os.path.join(data_out_dir, out_filename)
        with open(int_path, "r", encoding="utf-8") as file_in:
            data = [json.loads(line) for line in file_in]
            out_data = []
            for batch in tqdm(data, desc="transforming"):
                ners = batch["ners"]
                sentences = batch["sentences"]
                for idx, words in enumerate(sentences):
                    text = " ".join(words)
                    tok2char_span = WhiteWordTokenizer.get_tok2char_span_map(words)
                    ent_list = []
                    for ent in ners[idx]:
                        ent_text = " ".join(words[ent[0]:ent[1] + 1])
                        char_span_list = tok2char_span[ent[0]:ent[1] + 1]
                        char_span = [char_span_list[0][0], char_span_list[-1][1]]
                        norm_ent = {"text": ent_text,
                                    "type": ent[2],
                                    "char_span": char_span}
                        assert ent_text == text[char_span[0]:char_span[1]]
                        ent_list.append(norm_ent)
                    sample = {
                        "text": text,
                        "word_list": words,
                        "entity_list": ent_list,
                    }
                    out_data.append(sample)

        json.dump(out_data, open(out_path, "w", encoding="utf-8"), ensure_ascii=False)


def clean_entity(ent):
    ent = re.sub("ï¿½", "", ent)
    ent = ent.strip("\xad")
    return ent.strip()


def trans_daixiang_data(path, data_type=None):
    with open(path, "r", encoding="utf-8") as file_in:
        lines = [line.strip("\n") for line in file_in]
        data = []
        for i in range(0, len(lines), 3):
            sample = lines[i: i + 3]
            text = sample[0]
            word_list = text.split(" ")
            annstr = sample[1]
            ent_list = []
            word2char_span = WhiteWordTokenizer.get_tok2char_span_map(word_list)

            # entities
            for ann in annstr.split("|"):
                if ann == "":
                    continue
                offsets, ent_type = ann.split(" ")
                offsets = [int(idx) for idx in offsets.split(",")]
                assert len(offsets) % 2 == 0
                for idx, pos in enumerate(offsets):
                    if idx % 2 != 0:
                        offsets[idx] += 1

                extr_segs = []
                char_span = []
                tok_span = []
                for idx in range(0, len(offsets), 2):
                    wd_sp = [offsets[idx], offsets[idx + 1]]
                    ch_sp_list = word2char_span[wd_sp[0]:wd_sp[1]]
                    ch_sp = [ch_sp_list[0][0], ch_sp_list[-1][1]]

                    seg_wd = " ".join(word_list[wd_sp[0]: wd_sp[1]])
                    seg_ch = text[ch_sp[0]:ch_sp[1]]
                    assert seg_ch == seg_wd

                    char_span.extend(ch_sp)
                    tok_span.extend(wd_sp)
                    extr_segs.append(seg_ch)
                ent_txt_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(char_span, text, "en")
                ent_txt = " ".join(extr_segs)

                assert ent_txt == ent_txt_extr
                ent = {
                    "text": ent_txt,
                    "type": ent_type,
                    "char_span": char_span,
                    "tok_span": tok_span,
                }
                ent_list.append(ent)

            # merge continuous spans
            for ent in ent_list:
                ori_char_span = ent["char_span"]
                merged_span = merge_spans(ori_char_span)
                ent_ori_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(ori_char_span, text, "en")
                ent_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(merged_span, text, "en")
                ent["char_span"] = merged_span
                assert ent_ori_extr == ent_extr == ent["text"]

            new_sample = {
                "text": sample[0],
                "word_list": word_list,
                "word2char_span": word2char_span,
                "entity_list": ent_list,
            }
            if data_type is not None:
                new_sample["id"] = "{}_{}".format(data_type, len(data))

            data.append(new_sample)

    return data


def postprocess_duee():
    res_data_path = "../../data/res_data/test_data.json"
    out_path = "../../data/res_data/duee.json"
    test_data = load_data(res_data_path)
    test_data2submit = []
    for sample in test_data:
        sample2submit = {
            "id": sample["id"],
            "text": sample["text"],
            "event_list": []
        }
        for event in sample["event_list"]:
            event2submit = {
                "event_type": event["trigger_type"],
                "trigger": event["trigger"],
                "trigger_start_index": event["trigger_char_span"][0],
                "arguments": [],
            }
            for arg in event["argument_list"]:
                event2submit["arguments"].append({
                    "argument": arg["text"],
                    "role": arg["type"],
                    "argument_start_index": arg["char_span"][0],
                })
            sample2submit["event_list"].append(event2submit)
        test_data2submit.append(sample2submit)
    save_as_json_lines(test_data2submit, out_path)


def preprocess_oie4():
    data_in_dir = "../../data/ori_data/oie4_bk"
    data_out_dir = "../../data/ori_data/oie4"
    if not os.path.exists(data_out_dir):
        os.makedirs(data_out_dir)

    train_filename = "openie4_labels"
    valid_filename = "dev.tsv"
    test_filename = "test.tsv"
    train_path = os.path.join(data_in_dir, train_filename)
    valid_path = os.path.join(data_in_dir, valid_filename)
    test_path = os.path.join(data_in_dir, test_filename)

    train_save_path = os.path.join(data_out_dir, "train_data.json")
    valid_save_path = os.path.join(data_out_dir, "valid_data.json")
    test_save_path = os.path.join(data_out_dir, "test_data.json")

    train_data = []
    with open(train_path, "r", encoding="utf-8") as file_in:
        text = None
        words = None
        tag_lines = []
        for line in tqdm(file_in, desc="loading data"):
            line = line.strip("\n")
            if re.search("ARG|REL|NONE", line) is not None:
                tag_lines.append(line.split(" "))
            else:
                if text is not None:
                    train_data.append({
                        "text": text,
                        "word_list": words,
                        "tag_lines": tag_lines
                    })
                    tag_lines = []
                text = line
                words = line.split(" ")
        if text is not None:
            train_data.append({
                "text": text,
                "word_list": words,
                "tag_lines": tag_lines
            })

    for sample in tqdm(train_data, desc="transforming data"):
        open_spo_list = []
        word_list = sample["word_list"]
        tok2char_span = WhiteWordTokenizer.get_tok2char_span_map(word_list)
        text = sample["text"]

        for tags in sample["tag_lines"]:
            for tag_id in range(-3, 0):
                if tags[tag_id] != "NONE":
                    assert tags[tag_id] == "REL"
                    tags[tag_id] = "ADD"
            type2indices = {}
            for idx, tag in enumerate(tags):
                if tag == "NONE":
                    continue
                if tag not in type2indices:
                    type2indices[tag] = []
                type2indices[tag].append(idx)

            spo = {"predicate": {"text": "",
                                 "complete": "",
                                 "predefined": False,
                                 "prefix": "",
                                 "suffix": "",
                                 "char_span": [0, 0],
                                 },
                   "subject": {"text": "", "char_span": [0, 0]},
                   "object": {"text": "", "char_span": [0, 0]},
                   "other_args": []}
            add_text = None
            other_args = []
            for type_, ids in type2indices.items():
                wd_spans = []
                pre = -10
                for pos in ids:
                    if pos - 1 != pre:
                        wd_spans.append(pre + 1)
                        wd_spans.append(pos)
                    pre = pos
                wd_spans.append(pre + 1)
                wd_spans = wd_spans[1:]

                ch_spans = Preprocessor.tok_span2char_span(wd_spans, tok2char_span)
                arg_text = Preprocessor.extract_ent_fr_toks(wd_spans, word_list, "en")
                arg_text_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(ch_spans, text, "en")
                assert arg_text_extr == arg_text

                type_map = {
                    "REL": "predicate",
                    "ARG1": "subject",
                    "ARG2": "object",
                    "ADD": "add",
                    "TIME": "time",
                    "LOC": "location",
                }
                if type_ in {"REL", "ARG1", "ARG2"}:
                    spo[type_map[type_]] = {
                        "text": arg_text,
                        "char_span": ch_spans,
                        "word_span": wd_spans,
                    }
                elif type_ in {"TIME", "LOC"}:
                    other_args.append({
                        "type": type_map[type_],
                        "text": arg_text,
                        "char_span": ch_spans,
                    })
                else:
                    add_info_map = {
                        "[unused1]": "be-none",
                        "[unused2]": "be-of",
                        "[unused3]": "be-from",
                    }
                    add_text = add_info_map[arg_text]

            if "predicate" not in spo:
                if add_text == "be-none":
                    spo["predicate"] = {
                        "predefined": True,
                        "text": "be",
                        "complete": "be",
                        "prefix": "",
                        "suffix": "",
                        "char_span": [0, 0]
                    }
                else:
                    spo["predicate"] = {
                        "predefined": True,
                        "text": "",
                        "complete": "DEFAULT",
                        "prefix": "",
                        "suffix": "",
                        "char_span": [0, 0]
                    }
                    raise Exception
            else:
                spo["predicate"]["prefix"] = ""
                spo["predicate"]["suffix"] = ""
                spo["predicate"]["predefined"] = False
                if add_text is not None:
                    spo["predicate"]["prefix"] = "be"
                    if add_text == "be-of":
                        spo["predicate"]["suffix"] = "of"
                    if add_text == "be-from":
                        spo["predicate"]["suffix"] = "from"
                spo["predicate"]["complete"] = " ".join([spo["predicate"]["prefix"],
                                                         spo["predicate"]["text"],
                                                         spo["predicate"]["suffix"]]).strip()
                spo["other_args"] = other_args
                open_spo_list.append(spo)

        word_list = word_list[:-3]
        text = " ".join(word_list)
        sample["word2char_span"] = tok2char_span[:-3]
        sample["text"] = text
        sample["word_list"] = word_list
        sample["open_spo_list"] = open_spo_list
        for spo in open_spo_list:
            for key, val in spo.items():
                if key == "other_args":
                    for arg in spo[key]:
                        arg_text_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(arg["char_span"], text, "en")
                        assert arg_text_extr == arg["text"]
                else:
                    arg_text_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(val["char_span"], text, "en")
                    assert arg_text_extr == val["text"]

    # valid and test
    def get_val_test_data(path):
        fix_map = {
            "the unique limestone limestone of the mountains": "the unique limestone of the mountains",
            "do n't": "don't",
            "did n't": "didn't",
        }

        with open(path, "r", encoding="utf-8") as file_in:
            lines = []
            for line in file_in:
                for key, val in fix_map.items():
                    line = re.sub(key, val, line)
                splits = line.strip("\n").split("\t")
                # puncs = re.escape(re.sub("\.", "", string.punctuation))
                new_splits = []
                # ç¬¦å·ä¸åè¯ç¨ç©ºæ ¼éå¼
                for sp in splits:
                    sp = re.sub("([A-Za-z]+|[0-9\.]+)", r" \1 ", sp)
                    sp = re.sub("\s+", " ", sp)
                    sp = re.sub("n ' t", "n 't", sp)
                    sp = re.sub("' s", "'s", sp)
                    sp = sp.strip()
                    new_splits.append(sp)
                lines.append(new_splits)

            text2anns = {}
            for line in lines:
                text = line[0]
                if text not in text2anns:
                    text2anns[text] = []
                spo = {"predicate": {"text": line[1],
                                     "complete": line[1],
                                     "predefined": False,
                                     "prefix": "",
                                     "suffix": "",
                                     "char_span": [0, 0],
                                     },
                       "subject": {"text": line[2], "char_span": [0, 0]},
                       "object": {"text": "", "char_span": [0, 0]},
                       "other_args": []}

                if len(line) >= 4:
                    if "C :" not in line[3]:
                        spo["object"] = {"text": line[3], }
                if len(line) >= 5:
                    if "C :" not in line[4]:
                        arg = re.sub("T : |L : ", "", line[4])
                        spo["other_args"].append({"text": arg, "type": "time/loc_1"})
                if len(line) == 6:
                    if "C :" not in line[5]:
                        arg = re.sub("T : |L : ", "", line[5])
                        spo["other_args"].append({"text": arg, "type": "time/loc_2"})
                text2anns[text].append(spo)

        data = []
        for text, anns in text2anns.items():
            data.append({
                "text": text,
                "open_spo_list": anns,
            })

        # spans
        predefined_p_set = {"belong to",
                            "come from",
                            "have a", "have",
                            "will be",
                            "exist",
                            "be", "be a", "be in", "be on", "be at", "be of", "be from", "be for", "be with"}
        prefix_set = {
            "be", "will", "will be", "have", "have no", "must", "do not", "that",
        }
        suffix_set = {"in", "by", "of", "to", "from", "at"}
        samples_w_tl = []

        def my_lexeme(ori_word):
            lexeme_ws = lexeme(ori_word)
            lexeme_ws += [ori_word[0].upper() + ori_word[1:]]
            lexeme_ws += [ori_word.lower()]
            lexeme_ws += [ori_word.upper()]

            if ori_word[-2:] == "ly":
                lexeme_ws += [ori_word[:-2]]
            if re.match("[A-Z]", ori_word[0]) is not None:
                lexeme_ws += [ori_word + "'s"]
            if ori_word == "pursued":
                lexeme_ws += ["Pursuit"]
            if ori_word == "approve":
                lexeme_ws += ["approval"]
            if ori_word == "goes":
                lexeme_ws += ["exit onto"]
            return lexeme_ws

        def try_best2get_spans(target_str, text):
            candidate_spans, add_text = Preprocessor.search_char_spans_fr_txt(target_str, text, "en")
            spans = candidate_spans[0]
            fin_spans = None
            if add_text.strip("_ ") == "" and len(spans) != 0:  # if exact match
                fin_spans = spans
            else:
                pre_add_text = add_text
                # find words need to alter
                words2lexeme = re.findall("[^_\s]+", add_text)
                # lexeme all words
                words_list = [my_lexeme(w) for w in words2lexeme]
                # enumerate all possible alternative words
                alt_words_list = [[w] for w in words_list[0]] if len(words_list) == 1 else itertools.product(
                    *words_list)

                match_num2spans = {}
                max_match_num = 0
                for alt_words in alt_words_list:
                    chs = list(target_str)
                    add_text_cp = pre_add_text[:]
                    for wid, alt_w in enumerate(alt_words):
                        # search the span of the word need to alter
                        m4alt = re.search("[^_\s]+", add_text_cp)
                        sp = m4alt.span()
                        if alt_w == m4alt.group():  # same word, skip
                            continue
                        # alter the word
                        chs[sp[0]:sp[1]] = list(alt_w)
                        # mask the positions, will be ignore when getting m4alt next time
                        add_text_cp_ch_list = list(add_text_cp)
                        add_text_cp_ch_list[sp[0]:sp[1]] = ["_"] * len(alt_w)
                        add_text_cp = "".join(add_text_cp_ch_list)
                    # alternative text
                    alt_txt = "".join(chs)

                    # try to get spans
                    candidate_spans, add_text = Preprocessor.search_char_spans_fr_txt(alt_txt, text, "en")
                    spans = candidate_spans[0]
                    # cal how many words are matched this time
                    match_num = len(re.findall("_+", add_text)) - len(re.findall("_+", pre_add_text))
                    if match_num > 0:  # some words matched
                        match_num2spans[match_num] = spans
                        max_match_num = max(max_match_num, match_num)
                if max_match_num > 0:  # if there are any successful cases
                    fin_spans = match_num2spans[max_match_num]  # use the longest match

            if fin_spans is None or len(fin_spans) == 0:  # if still can not match, take partial match instead
                candidate_spans, add_text = Preprocessor.search_char_spans_fr_txt(target_str, text, "en")
                fin_spans = candidate_spans[0]
            return fin_spans

        for sample in tqdm(data, "add char span to val/test"):
            text = sample["text"]
            for spo in sample["open_spo_list"]:
                if len(spo) >= 4:
                    samples_w_tl.append(spo)
                for key, val in spo.items():
                    if key == "predicate":
                        predicate = spo["predicate"]["text"]
                        p_words = predicate.split()
                        p_lemma_words = [lemma(w) for w in p_words]
                        p_lemma = " ".join(p_lemma_words)

                        if p_lemma in predefined_p_set:
                            spo["predicate"]["predefined"] = True
                            spo["predicate"]["text"] = ""
                            spo["predicate"]["complete"] = p_lemma
                            spo["predicate"]["char_span"] = [0, 0]
                            continue

                        candidate_spans, add_text = Preprocessor.search_char_spans_fr_txt(predicate, text, "en")
                        spans = candidate_spans[0]
                        if add_text.strip("_ ") == "" and len(spans) != 0:
                            spo["predicate"]["char_span"] = spans
                            continue

                        # take prefix and suffix out
                        if re.search("[A-Za-z0-9]$", add_text):
                            for suffix in sorted(suffix_set, key=lambda a: len(a), reverse=True):
                                if re.search(" {}$".format(suffix), p_lemma):
                                    spo["predicate"]["text"] = " ".join(
                                        spo["predicate"]["text"].split()[:len(p_words) - len(suffix.split())])
                                    spo["predicate"]["suffix"] = suffix
                                    break
                        if re.search("^[A-Za-z0-9]", add_text):
                            for prefix in sorted(prefix_set, key=lambda a: len(a), reverse=True):
                                if re.search("^{} ".format(prefix), p_lemma):
                                    spo["predicate"]["text"] = " ".join(
                                        spo["predicate"]["text"].split()[len(prefix.split()):])
                                    spo["predicate"]["prefix"] = prefix
                                    break

                    elif key != "other_args":
                        arg = spo[key]
                        if arg is not None:
                            arg["char_span"] = try_best2get_spans(arg["text"], text)
                            seg_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(arg["char_span"], text, "en")
                            # if seg_extr != arg["text"]:
                            #     print(sample["text"])
                            #     print("target_seg: {}".format(arg["text"]))
                            #     print("extr_seg: {}".format(seg_extr))
                            #     pprint(spo)
                            #     print("===============")
                    else:
                        for arg in spo[key]:
                            arg["char_span"] = try_best2get_spans(arg["text"], text)
                            seg_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(arg["char_span"], text, "en")
                            # if seg_extr != arg["text"]:
                            #     print(sample["text"])
                            #     print("target_seg: {}".format(arg["text"]))
                            #     print("extr_seg: {}".format(seg_extr))
                            #     pprint(spo)
                            #     print("===============")
        return data

    valid_data = get_val_test_data(valid_path)
    test_data = get_val_test_data(test_path)
    save_as_json_lines(train_data, train_save_path)
    save_as_json_lines(valid_data, valid_save_path)
    save_as_json_lines(test_data, test_save_path)
    return train_data, valid_data, test_data


def trans2dai_dataset():
    '''
    change our data format to daixiang data format
    :return:
    '''
    in_data_dir = "../../data/normal_data/share_14_uncbase"
    out_data_dir = "../../data/ori_data/share_14_uncbase"
    if not os.path.exists(out_data_dir):
        os.makedirs(out_data_dir)

    test_data_path = os.path.join(in_data_dir, "test_data.json")
    train_data_path = os.path.join(in_data_dir, "train_data.json")
    valid_data_path = os.path.join(in_data_dir, "valid_data.json")
    test_out_path = os.path.join(out_data_dir, "test.txt")
    valid_out_path = os.path.join(out_data_dir, "dev.txt")
    train_out_path = os.path.join(out_data_dir, "train.txt")

    def trans2daixiang_subwd(in_path, out_path):
        data = load_data(in_path)
        with open(out_path, "w", encoding="utf-8") as out_file:
            for sample in data:
                ent_list = []
                for ent in sample["entity_list"]:
                    ent_subwd_sp = [str(pos) if idx % 2 == 0 else str(pos - 1) for idx, pos in
                                    enumerate(ent["wd_span"])]
                    ent_list.append(",".join(ent_subwd_sp) + " " + ent["type"])
                text = sample["text"]
                ann_line = "|".join(ent_list)
                out_file.write("{}\n".format(text))
                out_file.write("{}\n".format(ann_line))
                out_file.write("\n")

    trans2daixiang_subwd(test_data_path, test_out_path)
    trans2daixiang_subwd(train_data_path, train_out_path)
    trans2daixiang_subwd(valid_data_path, valid_out_path)


def preprocess_saoke(data_path="../../data/ori_data/saoke_bk/saoke.json"):
    data = load_data(data_path)
    # fix data
    pred_fix_map = {
        'å°ç ç©¶é¢åççéç¹æ¾å¨': 'å°ç ç©¶é¢åçéç¹æ¾å¨',
        "NOTæXç¨å¨å¦ä½è¸¢çä¸": "éæXç¨å¨å¦ä½è¸¢çä¸",
        "è¢«Xè£ä¸ºâ ä¸çåªåY âçæ¹æ³": "è¢«Xè£ä¸ºâä¸çåªåYâçæ¹æ³",
        "å³å°å¨Xä¸¾è¡ä¸¾è¡": "å³å°å¨Xä¸¾è¡",
        "å¦é³åè¬æçæç": "å¦é³åè¬æç",
        "ä¸æ­ä¸æ­é¢ç¥": "ä¸æ­é¢ç¥",
        "æäºXè¯ºè´å°åå¹³å¥æ": "æäºXè¯ºè´å°åå¹³å¥",
        "åºå®ä½¿ç¨ä½¿ç¨": "åºå®ä½¿ç¨",
        "å¬è§Xå¬è§": "å¬è§",
        "æ¸éæ§æ§è½": "æ¸éæ§è½",
        "å°A/Væ°æ®ä¼ è¾ä¼ è¾ç»": "å°A/Væ°æ®ä¼ è¾ç»",
        'èå¥èå¥': 'èå¥',
        'å æ»æ»é¢ç§¯': 'å æ»é¢ç§¯',
        "ä¸»è¦ä»¥Xä¸»è¦ä»¥": "ä¸»è¦ä»¥",
        'æ ¹ç¶èè': 'æ ¹ç¶è',
        "å®ä¸ºä¸º": "å®ä¸º",
        'ç¬¬ä¸ç¬é£é©æèµèµé': 'ç¬¬ä¸ç¬é£é©æèµé',
        'æ éXç§ç': 'æ é¡»Xç§ç',
        "[è¸ååè¿ç­]åç­é¢": "[è¸å|è¿ç­]åç­é¢",
        "[é«ç¨³äº§åç°å°é¢ç§¯|äººåé«ç¨³äº§åç°å°é¢ç§¯": "[é«ç¨³äº§åç°å°é¢ç§¯|äººåé«ç¨³äº§åç°å°é¢ç§¯]",
        "[åå¼±ORæ¶å¤±]": "[åå¼±|æ¶å¤±]",
        "æ´ä½[è§å|è¥å»º|": "æ´ä½[è§å|è¥å»º]",
        "äºåå¥èµ°|": "äºåå¥èµ°",
        "æè®¢|ç»ç»å®æ½]": "[æè®¢|ç»ç»å®æ½]",
        "è§å®|": "è§å®",
        "ä»¥âè£æ°å¤§å®ç¥ å¾â|": "ä»¥âè£æ°å¤§å®ç¥ å¾â",
        "å¥å¨||": "å¥å¨",
        "è½¬ç§»|å¹è®­": "[è½¬ç§»|å¹è®­]",
        "è¢«å½å¡é¢ç¡®ç¡®å®ä¸º": "è¢«å½å¡é¢ç¡®å®ä¸º",
        "èä¼|": "èä¼",
        "ç±Xç±": "ç±Xåºç",
        "ä»¥Xä»¥": "ä»¥Xä¸ºç®æ ",
        "å¯ä»¥åXå¯ä»¥å": "å¯ä»¥åX",
        "å¯ä½¿Xä½ä¸|": "å¯ä½¿Xä½ä¸",
        "ä¸ç´è´åäºä¸ºXæä¾æå¡]": "ä¸ç´è´åäºä¸ºXæä¾[è®¾æ½|æå¡]",
        "ä»¥è¾¾å°çææXçææ": "ä»¥è¾¾å°Xçææ",
        "ä¸ºX[åå¦ãæ®æ]": "ä¸ºX[åå¦|æ®æ]",
        "æå©Xå¿«é[æå»ºè°æ´]Y": "æå©Xå¿«é[æå»º|è°æ´]Y",
        "ä¾æ³çç£æ£æ¥Xè´¯å½»æ§è¡å®å¨çäº§æ³è§æåµ|è®¾å¤è®¾æ½å®å¨æåµ]": "ä¾æ³çç£æ£æ¥Xè´¯å½»æ§è¡[å®å¨çäº§[æ³å¾|æ³è§]æåµ|å®å¨çäº§æ¡ä»¶|è®¾å¤è®¾æ½å®å¨æåµ]",
        "è´åäºå¸®å©Xè§£å³æè²ãèä¸ç­ç­å·¥ä½æ¹æ¹é¢é¢æéå°çé¾é¢]": "è´åäºå¸®å©Xè§£å³[è¡£|é£|ä½|è¡|å¨±ä¹|ææ|æè²|èä¸]çé¾é¢",
        "ä¸ºXæä¾ä¸ä¸ªçæ§éæ©çå¹³å°|": "ä¸ºXæä¾ä¸ä¸ª[å®¢è§è¯ä¼°|çæ§éæ©]çå¹³å°",
        "ä¼Xå¸¦æ¥[å²å»åå½±å]": "ä¼ç»Xå¸¦æ¥[å²å»|å½±å]",
        "ä»¥Xèµ¢å¾äº[Y]": "ä»¥Xèµ¢å¾äºY",
        "ä¾æ³æå®Xè´è´£Yç[ä¸å®¡|[åºå±æ³é¢|æ£å¯é¢äºå®¡æ¡ä»¶]çå®¡çåæ³å¾çç£å·¥ä½": "ä¾æ³æå®Xè´è´£Yç[ä¸å®¡|åºå±[æ³é¢|æ£å¯é¢]äºå®¡]æ¡ä»¶ç[å®¡ç|æ³å¾çç£å·¥ä½]",
        "å X sockrrlates": "åXsockrrlates",
        "ç¨Xååè¡ä¸­å¿ç³æ¥[ä¸­å¥ç­çº§|æç¥¨äºº[å§å|ä½å|èº«ä»½è¯å·ç ]": "ç¨Xååè¡ä¸­å¿ç³æ¥[ä¸­å¥ç­çº§|æç¥¨äºº[å§å|ä½å|èº«ä»½è¯å·ç ]]",
        "å¯¹Xå¯å®ç°å¨é¨[ä¸»å¤§å¤æ°çå¶çèè´[ä¸»|å¯]æåä½æç[ä¸æ¬¡ææ¾|ä¸æ¬¡å®æY]": "å¯¹Xå¯å®ç°å¨é¨[ä¸»|å¯]æåä½æç[ä¸æ¬¡ææ¾|ä¸æ¬¡å®æY]",
        "ä¸ºXå¥ç®äºäºä¸ä¸ªåä¸ä¸ªè§è²": "ä¸ºXå¥ç®äºä¸ä¸ªåä¸ä¸ªè§è²",
        "æ¯ä¿æ¤Xåé­[[å°é|æ°´ç¾|ç«ç¾]ç­ç¯å¢äºæ]|äººä¸ºæä½[å¤±è¯¯|éè¯¯]|åç§è®¡ç®æºç¯ç½ªè¡ä¸º]å¯¼è´çç ´åè¿ç¨": "æ¯ä¿æ¤Xåé­[[å°é|æ°´ç¾|ç«ç¾]ç­ç¯å¢äºæ|äººä¸ºæä½[å¤±è¯¯|éè¯¯]|åç§è®¡ç®æºç¯ç½ªè¡ä¸º]å¯¼è´çç ´åè¿ç¨",
        "ä»èåè½»ORé¿åXçåè¿«": "ä»è[åè½»|é¿å]Xç[æ©æ¦|åè¿«]",
        "è®©Xä¹è½æ¥è§¦[æéè¯æ±ORå¿ å­èä¹æäº]": "è®©Xä¹è½æ¥è§¦[æéè¯æ±|å¿ å­èä¹æäº]",
        "è´è´£å¯¹Xç[å®¡æ¹åæ¾åç®¡ç]": "è´è´£å¯¹Xç[å®¡æ¹åæ¾|ç®¡ç]",
        "åçXç[è°ä»»|è½¬ä»»|å®¡æ¹|å·¥ä½]": "åçXç[è°ä»»|è½¬ä»»|å®¡æ¹å·¥ä½]",
        "èµX[å£°å¦å¥é·ï¼å¿å¦çæ¶ï¼éµå¦è±ªæ­]": "èµX[å£°å¦å¥é·|å¿å¦çæ¶|éµå¦è±ªæ­]",
        "[æ¥èµä¸ºåè´¾|[é©¬è´©|å± å®°]ä¹ç±»]": "æ¥èµä¸º[åè´¾|é©¬è´©|å± å®°]",
        "åçXç[è°ä»»|è½¬ä»»|å®¡æ¹|å·¥ä½": "åçXç[è°ä»»|è½¬ä»»|å®¡æ¹|å·¥ä½]",
        "ä½ç°äº[ç[è±ªå|æ§æ|èé|åé æ§]æ¼ç»çæè´": "ä½ç°äºXç[è±ªå|æ§æ|èé|åé æ§]æ¼ç»çæè´",
        "å¾åäºX[ç[å±±æ°´é£å|äººæåå²|èªç¶å°ç|æ°ä¿é£æ]": "å¾åäºXç[å±±æ°´é£å|äººæåå²|èªç¶å°ç|æ°ä¿é£æ]",
        "[äº[ä¸ç¦|ä¸å¡]ä¹é´ç[çå¯¼|åå|è¯´æ]|[[å¤ºå¶å©|ä¸å¶å©|ä¸å¶å©]çå å¿å©å¯¼çæè²å·¥ä½]": "[äº[ä¸ç¦|ä¸å¡]ä¹é´ç[çå¯¼|åå|è¯´æ]|[å¤ºå¶å©|ä¸å¶å©]çå å¿å©å¯¼çæè²å·¥ä½]",
    }

    subj_fix_map = {
        '[ç¤¾ä¼æ»éæ±|ç¤¾ä¼æ»ä¾ç»|å¨åºç¤¾ä¼èµé]': '[ç¤¾ä¼[æ»éæ±|æ»ä¾ç»]|å¨åºç¤¾ä¼èµé]',
        "_ä¸é¨æ²¿æµ·æ¸åº": "ä¸é¨æ²¿æµ·æ¸åº",
        "_å¨ç": "å¨ç",
        "[æµæ¥ä¼ åªéå¢è¡ä»½æéå¬å¸ååäº¬åå¥¥æç©ºç§æåå±æéå¬å¸]": "[æµæ¥ä¼ åªéå¢è¡ä»½æéå¬å¸|åäº¬åå¥¥æç©ºç§æåå±æéå¬å¸]",
        '[udpåtcp]åè®®å¨å®ç°æ°æ®ä¼ è¾æ¶çå¯é æ§': '[udp|tcp]åè®®å¨å®ç°æ°æ®ä¼ è¾æ¶çå¯é æ§',
        "|æ°å|é«å³°|ç´«å²«]": "[æ°å|é«å³°|ç´«å²«]",
        '[é¥¥é¥¿ç¾ç]': '[é¥¥é¥¿|ç¾ç]',
        "ThinkPad T430s 2352A31ç¬è®°æ¬ThinkPad T430s 2352A31ç¬è®°æ¬": "ThinkPad T430s 2352A31ç¬è®°æ¬",
        "å­å­": "å­",
        "ãæ­»äº¡æ¸¸è¡ããæ­»äº¡æ¸¸è¡ã": "ãæ­»äº¡æ¸¸è¡ã",
        "[åä¹¡æ®é[ä¸­å°å­¦åå­¦åæè²]å¸èµéç½®": "åä¹¡æ®é[ä¸­å°å­¦|å­¦åæè²]å¸èµéç½®",
        "ç¥åç¥å": "ç¥å",
        "å½å[æ±½æ²¹|æ´æ²¹]åæ²¹ä»·æ ¼": "å½å[æ±½|æ´æ²¹]åæ²¹ä»·æ ¼",
        "[å³å¨è¡æ¿ä¸»ç®¡é¨é¨|å³å¨è¡æ¿ä¸»ç®¡é¨é¨å§æçä¸é¨æºæ]": "å³å¨è¡æ¿ä¸»ç®¡é¨é¨[|å§æçä¸é¨æºæ]",
        "è®¸å¤[é®é¢|é®é¢è§£å³ä¹é]": "è®¸å¤é®é¢[|è§£å³ä¹é]",
        "[å½å¡é¢æ°´è¡æ¿ä¸»ç®¡é¨é¨|å½å¡é¢æ°´è¡æ¿ä¸»ç®¡é¨é¨ææçæµåç®¡çæºæ]": "å½å¡é¢æ°´è¡æ¿ä¸»ç®¡é¨é¨[|ææçæµåç®¡çæºæ]",
        "[é¿æ¯æ|é¿æ¯æç¸å³äººå]": "é¿æ¯æ[|ç¸å³äººå]",
        "[å¤©çº¿ä¸å¾è§|å¤©çº¿æ¹ä½è§|å¤©çº¿é«åº¦]": "[å¤©çº¿[ä¸å¾è§|æ¹ä½è§]|å¤©çº¿é«åº¦]",
        "[åçº§æ°´è¡æ¿ä¸»ç®¡é¨é¨|åçº§æ°´è¡æ¿ä¸»ç®¡é¨é¨ææçæå³é¨é¨|æµåæºæ]": "[åçº§æ°´è¡æ¿ä¸»ç®¡é¨é¨[|ææçæå³é¨é¨]|æµåæºæ]",
        'äºèäºè': 'äºè',
        '[èµäº§åè´åº]ç[è´§å¸ç»æåå©çç»æ]': '[èµäº§|è´åº]ç[è´§å¸ç»æ|å©çç»æ]',
        'âæ´çâ': "âæ´çâ",
        '[æ¥æ³¢|æ¥æ³¢]': 'æ¥æ³¢',
        "[åä½å¨åå¼çå­¦çå®¿è|ä¼é£èªçå¼çå­¦çå®¿è|ä¼ ç»å¼çå­¦çå®¿è|ååºå¼çç¬ç«å°é¢]": "[[åä½å¨åå¼ç|ä¼é£èªçå¼ç]å­¦çå®¿è|ä¼ ç»å¼çå­¦çå®¿è|ååºå¼çç¬ç«å°é¢]",
        "[åè¶çè´¨é|åè¿ççå¿µ|ä¼è´¨çæå¡|å¸åºçéæ±|ä¼ä¸çåç|ä¼ä¸çç¥ååº¦]": "[åè¶çè´¨é|åè¿ççå¿µ|ä¼è´¨çæå¡|å¸åºçéæ±|ä¼ä¸ç[åç|ç¥ååº¦]]",
        "[å¤§åè¿éå¼å¯¼æå¡æ³|éè¦æå®¢ç»è®°æå¡æ³|ç¹æ®éè¦é¢çº¦æå¡æ³|åè¯­å¼æå¡æ³|ç½ç»å¼çéæå¡æ³]": "[å¤§åè¿éå¼å¯¼æå¡æ³|éç¹æå®¢ç»è®°æå¡æ³|ç¹æ®éè¦é¢çº¦æå¡æ³|åè¯­å¼æå¡æ³|ç½ç»å¼çéæå¡æ³]",
        "[ææ³½æ°|ææ³½æ°]": "[æåè¾¾|ææ³½æ°]",
        "âéå¤æ±å½¢å¼èµäº§ââäººæ°å¸ââéå¤æ±å½¢å¼èµäº§ââäººæ°å¸âåæ¹": "âéå¤æ±å½¢å¼èµäº§ââäººæ°å¸âåæ¹",
        "[çæµæ°æ®|çæµèµæ|æ±¡æºèµæ]": "[çæµ[æ°æ®|èµæ]|æ±¡æºèµæ]",
        "[è¯ç¹å¿|è¯ç¹å¸|å°çº§å¸]": "[è¯ç¹[å¿|å¸]|å°çº§å¸]",
        "æ ¹ç¶èè": "æ ¹ç¶è",
        "ååºäººæ°æ¿åºæå®æºææºæ": "ååºäººæ°æ¿åºæå®æºæ",
        "å·¥å": "åå·¥",
        'ååå¼ºå¤«å¦|ç«¯æ¨æ¨±å­|åå¶ä»åæ°å®¶ææå]ä¸åä½äºº': '[ååå¼ºå¤«å¦|ç«¯æ¨æ¨±å­|å¶ä»åæ°å®¶ææå]',
        "[ä¸»é¢æ²ãåé¶ã||ææ²ãæ²éã]": "[ä¸»é¢æ²ãåé¶ã|ææ²ãæ²éã]",
        "[BodoL innhoff|BodoL innhoffåäº]": "BodoL innhoff[åäº|]",
        "[å¼ è|å¼ èä¼ä¼´ä»¬]": "å¼ è[ä¼ä¼´ä»¬|]",
        "[æ²é|æ²éçæçå·¨é­]": "æ²é[çæçå·¨é­|]",
        "[æ¾ä¸æ­¦å¨ç[åæ°å£«åµ|å¹³æ°]": "æ¾ä¸æ­¦å¨ç[åæ°å£«åµ|å¹³æ°]",
        "[è±å­æ¯|çµè¯|å¹³é¢çµè§|ä¿é©ç®±|ä¹¦æ¡|ææ°|æ´è¡£å®¤|æ²å|æ¨è´¨/é¶æ¨å°æ¿|è¡£æ/è¡£æ©±|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|æµ´ç¼¸ORæ·æµ´|å¤éæå¡]": "[è±å­æ¯|çµè¯|å¹³é¢çµè§|ä¿é©ç®±|ä¹¦æ¡|ææ°|æ´è¡£å®¤|æ²å|æ¨è´¨/é¶æ¨å°æ¿|è¡£æ/è¡£æ©±|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|æµ´ç¼¸|æ·æµ´|å¤éæå¡]",
        "[ãç·åããå¥³åã|ãè·³æ å¸¸ã]ç[ææ|åºæ¿]]": "[ãç·åã|ãå¥³åã|ãè·³æ å¸¸ã]ç[ææ|åºæ¿]",
        "[ç|å¸|å°]çº§æ°´è¡æ¿ä¸»ç®¡é¨é¨]": "[ç|å¸|å°]çº§æ°´è¡æ¿ä¸»ç®¡é¨é¨",
        "[æ²ä¸é­å ¡äº|ç»æå¨åä»¤ç³»å][æ²ä¸é­å ¡äº|ç»æå¨åä»¤ç³»å]": "[æ²ä¸é­å ¡äº|ç»æå¨åä»¤ç³»å]",
        "[å®¹é|åå¼|ç©ºè½½çµæµ|ç©ºè½½æè|ç­è·¯ï¼è´è½½ï¼æè|é»æçµåå®¹é|åå¼|ç©ºè½½çµæµ|ç©ºè½½æè|ç­è·¯ï¼è´è½½ï¼æè|é»æçµå]": "[å®¹é|åå¼|ç©ºè½½çµæµ|ç©ºè½½æè|ç­è·¯ï¼è´è½½ï¼æè|é»æçµå]",
        "[[ãæ±ç é±¼è°±ã|ãéé±¼åã|ãéé±¼é¥²å»æ³ã]": "[ãæ±ç é±¼è°±ã|ãéé±¼åã|ãéé±¼é¥²å»æ³ã]",
        "[Denise Gimpel|Lyce.jankowski|Lyce.jankowski]": "[Denise Gimpel|Dr.james S.Edgren|Lyce.jankowski]",
        "[å¼ å®¶ååç|ä¸æ°å®åç|èµµåç¶åç|æ¹ç»ªé¾åç|å®¢æ·ä»£è¡¨]|": "[å¼ å®¶ååç|ä¸æ°å®åç|èµµåç¶åç|æ¹ç»ªé¾åç|å®¢æ·ä»£è¡¨]",
        "1å[å¹´é¾è¾å¤§çå¿ç«¥|æäºº]|ä¸å¼ å åºæ¶è´¹": "1å[å¹´é¾è¾å¤§çå¿ç«¥|æäºº]ä¸å¼ å åºæ¶è´¹",
        "[[å¬å¼|çè¯]è°è®ºéè¦è¯¾é¢|ä¸æ­ææèªå·±æèè½å]]": "[[å¬å¼|çè¯]è°è®ºéè¦è¯¾é¢|ä¸æ­ææèªå·±æèè½å]",
        "[ä¸­åæå|æ±æ·®æå|ééµæå|å´æå]": "[ä¸­å|æ±æ·®|ééµ|å´]æå",
        "[éæ­£å®|éè£ç»|éå­é]ç­": "[éæ­£å®|éå­é|éè£ç»]ç­",
        "[éæ­£å®|éè£ç»|éå­é]": "[éæ­£å®|éå­é|éè£ç»]",
        "[æ|é£å ä¸ªå£«åµ]": "[é£å ä¸ªå£«åµ|æ]",
        "[ä»|ä¸­çä»ä¹å©|äºä»£åå]": "[ä»|äºä»£åå|ä¸­çä»ä¹å©]",
        '[[åºé³|å®å|åæ°´|å®å·|æ­£å®]é¾æ°]|[è£åé¾æ°]]': '[[åºé³|å®å|åæ°´|å®å·|æ­£å®]é¾æ°|è£åé¾æ°]',
        "[ææç|ä¹¡ç»ç­¹è´¹|éç»ç­¹è´¹]": "[ææç|[ä¹¡|é]ç»ç­¹è´¹]",
        "[èèå¬ä¸»|æ¨åå½é|ä»åº·ç§æ]]": "[èèå¬ä¸»|æ¨åå½é|ä»åº·ç§æ]",
        "å¶[çµæ´»|æ§ä»·æ¯|æå¡ç»éª|å¯¹æ¬åæ¶è´¹è|å¯¹æ¬åæ¶è´¹èæ´å¯]": "å¶[çµæ´»|æ§ä»·æ¯|æå¡ç»éª|å¯¹æ¬åæ¶è´¹èæ´å¯]",
        "æ²¹ç»[ãçº¢è¡£å°å¥³ã|ãç´«ç½|å°ã]|": "æ²¹ç»[ãçº¢è¡£å°å¥³ã|ãç´«ç½å°ã]",
        "[ç¢³çº¤ç»´ORç¡¼çº¤ç»´å¢å¼ºçç¯æ°§æ èåºå¤åææ|éå±åºå¤åææ]": "[[ç¢³çº¤ç»´|ç¡¼çº¤ç»´]å¢å¼ºç[ç¯æ°§æ èåºå¤åææ|éå±åºå¤åææ]]",
        "[åæå¤§å¦|å¤ªå¤æ±|æ­å§é¢": "[åæå¤§å¦|å¤ªå¤æ±|æ­å§é¢]",
        "[æä¿äºº|è¢«ä¿é©äºº|åçäºº": "[æä¿äºº|è¢«ä¿é©äºº|åçäºº]",
        "[é´è|é³èä½è´¨ä¹äºº": "[é´è|é³è]ä½è´¨ä¹äºº",
        "[å±åä¼ä¸æå|åºæ¬åå": "[å±åä¼ä¸æå|åºæ¬åå]",
        "è­æ°§å°æµå¼æ··åæèè£ç½®|ç¾å½é¶æ°å¬å¸åæ¸éè£ç½®|çº¤ç»´æ´»æ§ç­é«æå¸é]": "[è­æ°§å°æµå¼æ··åæèè£ç½®|ç¾å½é¶æ°å¬å¸åæ¸éè£ç½®|çº¤ç»´æ´»æ§ç­é«æå¸é]",
        "è®°|ä¼ é|å½æ¡£]å·¥ä½": "[æ¶åç»è®°|ä¼ é|å½æ¡£]å·¥ä½",
        "è©èé¸ç|è°ç|é¢æ¤ç|å³èç|é£æ¹¿|å¤±ç |æ°å]": "[è©èé¸ç|è°ç|é¢æ¤ç|å³èç|é£æ¹¿|å¤±ç |æ°å]",
        "ä¿é|æå¡]": "[ä¿é|æå¡]",
        "[å¡è¿ªæåæ±½è½¦åå¬å¸|åè±æ¯åå¬å¸": "[å¡è¿ªæåæ±½è½¦åå¬å¸|åè±æ¯åå¬å¸]",
        "[ç¤¾å¢|å¤§è¿é«æ ¡å¤©æåä¼èç": "[ç¤¾å¢|å¤§è¿é«æ ¡å¤©æåä¼èç]",
        "é¥®é£æåç¹è°æ¹æ³]": "[é¥®é£æå|ç¹è°æ¹æ³]",
        "[æ¬ç½ç«": "æ¬ç½ç«",
        "ãåä¼¯èç¹ç§é¦2ã|ãé¾å¤åºã]ç­": "[ãåä¼¯èç¹ç§é¦2ã|ãé¾å¤åºã]ç­",
        "å·«å¯æ²³æ¼æµ|åå¨è§é³é|åéæé¹|èå±±ç|å¤ç§æ|å«ä¸å±±|é¹ææ¥¸èªç¶ä¿æ¤åº|ä¾å®åç§ç¢|ç¦å»ºæçç©ç¾¤|åå¨æ°´ä¸åº¦ååº|ä¹ä»°|ä¹å|è¿ææ°ææ|ä¹è¸èæåè¿éå]": "[å·«å¯æ²³æ¼æµ|åå¨è§é³é|åéæé¹|èå±±ç|å¤ç§æ|å«ä¸å±±|é¹ææ¥¸èªç¶ä¿æ¤åº|ä¾å®åç§ç¢|ç¦å»ºæçç©ç¾¤|åå¨æ°´ä¸åº¦ååº|ä¹ä»°|ä¹å|è¿ææ°ææ|ä¹è¸èæåè¿éå]",
        "ãè¡æ¿ç®¡çä¸æ¿åºè¡ä¸ºç ç©¶ã|ãç¤¾ä¼ä¸»ä¹å¸åºç»æµè®ºã|ãå¯æç»­åå¸ååå±ç ç©¶--ä¸­å½åå·çå®è¯åæã]": "[ãè¡æ¿ç®¡çä¸æ¿åºè¡ä¸ºç ç©¶ã|ãç¤¾ä¼ä¸»ä¹å¸åºç»æµè®ºã|ãå¯æç»­åå¸ååå±ç ç©¶--ä¸­å½åå·çå®è¯åæã]",
        "[é»é³|èè¹|æ°´è|å°é¾è¾|æ°´èé¼ |éè|è¾è": "[é»é³|èè¹|æ°´è|å°é¾è¾|æ°´èé¼ |éè|è¾è]",
        "ãåä¼¯èç¹ç§é¦2ã|ãé¾å¤åºã]": "[ãåä¼¯èç¹ç§é¦2ã|ãé¾å¤åºã]",
        "åºæ¬ç¹è´¨|ç»éªæè®­": "[åºæ¬ç¹è´¨|ç»éªæè®­]",
        "[åçç½ãåçææ¦´å¼¹]": "[åçç½|åçææ¦´å¼¹]",
        "[åçç½ãåçææ¦´å¼¹]ç­": "[åçç½|åçææ¦´å¼¹]",
        "ä¼|": "ä¼",
        "[ç»ä¸é¨ç½²å¤§å¹åº¦è£å]": "ç»ä¸é¨ç½²å¤§å¹åº¦è£å",
        "[æ·±å³å¸åå¹¿ä¸çæè²ç£å¯¼å®¤]": "æ·±å³å¸åå¹¿ä¸çæè²ç£å¯¼å®¤",
        "_å¢å": "å¢å",
        "[ç»¼åç®¡çæ¡£æ¡è°é]å·¥ä½": "[ç»¼åç®¡ç|æ¡£æ¡è°é]å·¥ä½",
        "å¾æ­¥ç»å±±|å²©éçé|æå²©æ¢æ´[]": "[å¾æ­¥ç»å±±|å²©éçé|æå²©æ¢æ´]",
        "[è®²è§£+ç»é£+æå·§è§£æ]": "[è®²è§£|ç»é£|æå·§è§£æ]",
        "[å¬å¸åäºº]çä»·å¼": "[å¬å¸|äºº]çä»·å¼",
        "æè¿·|å¤±è´¥": "æè¿·å¤±è´¥",
        "æ´ä¸ªæ¬§æ´²äººå£èå¹´å|æ´ä¸ªç¦å©ç¤¾ä¼[]": "[æ´ä¸ªæ¬§æ´²äººå£èå¹´å|æ´ä¸ªç¦å©ç¤¾ä¼]",
        "[åºæ¿æ§åèèæ§]": "[åºæ¿æ§|èèæ§]",
        "[äººäºé¨å¨å½äººææµå¨ä¸­å¿ä¸é¦é½ç»æµè´¸æå¤§å­¦é¦ç»ææ¯å¹è®­ä¸­å¿]": "[äººäºé¨å¨å½äººææµå¨ä¸­å¿|é¦é½ç»æµè´¸æå¤§å­¦é¦ç»ææ¯å¹è®­ä¸­å¿]",
        "ãå²å­¦ç ç©¶ã|ãé©¬åæä¸»ä¹ç ç©¶ã|ãå½ä»£ä¸çä¸ç¤¾ä¼ä¸»ä¹ã|ãç§å­¦ç¤¾ä¼ä¸»ä¹ã|ãåææ¥æ¥ãï¼çè®ºçï¼|ãä¸­å½æè²æ¥ãï¼çè®ºçï¼": "[ãå²å­¦ç ç©¶ã|ãé©¬åæä¸»ä¹ç ç©¶ã|ãå½ä»£ä¸çä¸ç¤¾ä¼ä¸»ä¹ã|ãç§å­¦ç¤¾ä¼ä¸»ä¹ã|ãåææ¥æ¥ãï¼çè®ºçï¼|ãä¸­å½æè²æ¥ãï¼çè®ºçï¼]",
        "[ç¥è¯åæè½å]èªç©ºæå¡ä¸é¨äººæ": "[ç¥è¯å|æè½å]èªç©ºæå¡ä¸é¨äººæ",
        "[å¯è±¡ç­åå­¦åç»è®¡åå­¦]ççè®º": "[å¯è±¡ç­åå­¦|ç»è®¡åå­¦]ççè®º",
        "GERMARTâ|âåçç¹â": "[âGERMARTâ|âåçç¹â]",
        "æ¶å°ä½ä¸å¼ æ¬|æºæä½ä¸éè": "[æ¶å°ä½ä¸å¼ æ¬|æºæä½ä¸éè]",
        "_ä¸é¢çéåºç°çæ¡ä¾": "ä¸é¢çéåºç°çæ¡ä¾",
        "ç¤¾å¢|ä¸å¤§è¿å¶ä»é«æ ¡": "[ç¤¾å¢|å¤§è¿å¶ä»é«æ ¡]",
        "_æ¯äº²": "æ¯äº²",
        "_ç¶äº²": "ç¶äº²",
        "[å½å®å± âä¸è­¦æ¿ç½²]": "[âå½å®å± â|è­¦æ¿ç½²]",
        "[å«ééå»ç]": "[å«é|éå»ç]",
        "_æ²¿æµ·": "æ²¿æµ·",
        "è¿ªè¾¾X": "è¿ªè¾¾",
        "[å¤§ä¸­å°åå¸åå°åé]": "[å¤§ä¸­å°åå¸|å°åé]",
        "å¬å¸|": "[å¬å¸|äº§å]",
    }

    obj_fix_map = {
        "[]æ¥æ¬å­¦æ¯ä¼è®®|å æ¿å¤§çå®¶å­¦ä¼ç­": "[æ¥æ¬å­¦æ¯ä¼è®®|å æ¿å¤§çå®¶å­¦ä¼]ç­",
        "[æ±ªå½©éä¸å¯ä¸åå¯è²è³ä¸é¡¾æ¥]": "[æ±ªå½©é|å¯ä¸å|å¯è²è³|é¡¾æ¥]",
        'é£ç§å¥ç¹çåéº»çæè§|': 'é£ç§å¥ç¹çåéº»çæè§',
        '[è¯ç©åè³é£ç»å]': '[è¯ç©|è³é£]',
        'ç³å¤´|èæ æ¢': '[ç³å¤´|èæ æ¢]',
        'Xåæ°å¨é¨åºå': 'åæ°å¨é¨åºå',
        '[å¼ä¸ç³è¯·ä¹¦åæ³çéªèµè¯æ]': 'å¼ä¸ç³è¯·ä¹¦',
        'å½éåç¾åå¹´||å½éåç¾æ¥': '[å½éåç¾åå¹´|å½éåç¾æ¥]',
        "[å¯è±¡ç­åå­¦åç»è®¡åå­¦]ççè®º": "[å¯è±¡ç­åå­¦|ç»è®¡åå­¦]ççè®º",
        "[èçº¦èµæºåä¿æ¤ç¯å¢]çåå±æ¨¡å¼": "[èçº¦èµæº|ä¿æ¤ç¯å¢]çåå±æ¨¡å¼",
        "[åéååæ]": "[åé|åæ]",
        "æ²¡æ[äº²ç¼|è¡ç¼|æ²¡å°ç¼]": "æ²¡æ[äº²ç¼|è¡ç¼|å°ç¼]",
        '[é³å°|DVDæ­æ¾æº|å«æé¢é|å¹³é¢çµè§|ç©ºè°|ä¹¦æ¡|å®¢åè§|æ´è¡£æº|æ²å|è¡£æ|è¡£æ©±|æ·æµ´|æµ´ç¼¸|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|å°ç®±|å¾®æ³¢ç|å¨æ¿|ç¨é¤åº|çµç§æ°´å£¶|å¨æ¿ç¨å·|é¤æ¡]ç­è®¾æ½': '[é³å°|DVDæ­æ¾æº|å«æé¢é|å¹³é¢çµè§|ç©ºè°|ä¹¦æ¡|å®¢åè§|æ´è¡£æº|æ²å|è¡£æ|è¡£æ©±|æ·æµ´|æµ´ç¼¸|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|å°ç®±|å¾®æ³¢ç|ç¨é¤åº|å¨æ¿|çµç§æ°´å£¶|å¨æ¿ç¨å·|é¤æ¡]ç­è®¾æ½',
        '[å¯¹åº|åä¸ç§é¢è²|åä¸ç§å½¢ç¶]ç': '[å¯¹åº|åä¸ç§[é¢è²|å½¢ç¶]]ç',
        'å®æåå­¦ååºææ¶èçè¯åéè¯åé': 'å®æåå­¦ååºææ¶èçè¯åé',
        '[çº¢æ°´æ³¡|ç½æ°´æ³¡|èæ°´æ³¡|çº¢ç½æ°´æ³¡|é»ç½æ°´éåéæ°´æ³¡|çº¢é¡¶æ°´æ³¡|å¢¨æ°´æ³¡|äºè±æ°´æ³¡|ç´«èæ°´æ³¡]': '[çº¢æ°´æ³¡|ç½æ°´æ³¡|èæ°´æ³¡|çº¢ç½æ°´æ³¡|é»ç½æ°´æ³¡|éåéæ°´æ³¡|çº¢é¡¶æ°´æ³¡|å¢¨æ°´æ³¡|äºè±æ°´æ³¡|ç´«èæ°´æ³¡]',
        "å°æææ³¨è¯´æ": "å°ææ³¨è¯´æ",
        "åç¨çç[è®¡ååé¡¹ç®|ç»è®°é¡¹ç®]": "åç¨ç[è®¡ååé¡¹ç®|ç»è®°é¡¹ç®]",
        "AutoCAD2006ä¸­æçç[æä»¶æä½|ç»å¾è®¾ç½®|ç»å¶äºç»´å¾å½¢|ç¼è¾å¾å½¢å¯¹è±¡çåºæ¬å½ä»¤|å¾æ¡çå¡«åæ¹æ³åå¶è®¾ç½®|å¾å±ãåä»¥åå±æ§çå®ä¹|ä¸ç»´å¾å½¢åå»º|æ¸²æä¸çè²|å¾å½¢æå°]": "AutoCAD 2006ä¸­æçç[æä»¶æä½|ç»å¾è®¾ç½®|ç»å¶äºç»´å¾å½¢|ç¼è¾å¾å½¢å¯¹è±¡çåºæ¬å½ä»¤|å¾æ¡çå¡«åæ¹æ³åå¶è®¾ç½®|å¾å±ãåä»¥åå±æ§çå®ä¹|ä¸ç»´å¾å½¢åå»º|æ¸²æä¸çè²|å¾å½¢æå°]",
        "å®æ´å®¢æ·çå½å¨æçç[åç|åå±]": "å®æ´å®¢æ·çå½å¨æç[åç|åå±]",
        "æ»å»è°è|ä¸»åè°": "[æ»å»è°è|ä¸»åè°]",
        "[ç§å­¦å®¶åèåå½ç²®åç»ç»åä¸çå«çç»ç»]": "[ç§å­¦å®¶åèåå½ç²®åç»ç»|ä¸çå«çç»ç»]",
        "[ä¸æ¡§çè³äºæ¡§]çå¤§æµ·è¹": "[ä¸æ¡§|äºæ¡§]çå¤§æµ·è¹",
        "[æ°´çµåæçµæ°åå¿åå°æ°´çµä»£çæå·¥ç¨å»ºè®¾]": "[æ°´çµåæçµæ°åå¿|å°æ°´çµä»£çæå·¥ç¨å»ºè®¾]",
        "[âåè°ãä¸ªæ§ãæ±çãåæ°â]çåå­¦çå¿µ": "[âåè°|ä¸ªæ§|æ±ç|åæ°â]çåå­¦çå¿µ",
        "[å¿è¦æ§åç°å®å·®è·]": "[å¿è¦æ§|ç°å®å·®è·]",
        "[é¥®é£æåç¹è°æ¹æ³]": "[é¥®é£æå|ç¹è°æ¹æ³]",
        "[ææ­10å¬æ¤æ¡æ­4å¬æ¤]": "[ææ­10å¬æ¤|æ¡æ­4å¬æ¤]",
        "[æµ·åçä½è²æ»ä¼åæµ·åçæ°æ¿å]": "[æµ·åçä½è²æ»ä¼|æµ·åçæ°æ¿å]",
        "ææå¸¦å¤´ä½ç¨|å¢éåä½ç²¾ç¥[]": "[ææå¸¦å¤´ä½ç¨|å¢éåä½ç²¾ç¥]",
        "[å°å¯ç±åç¾è¤¶è£]": "[å°å¯ç±|ç¾è¤¶è£]",
        "çº¦å å¨_çé¢ç§¯ç1/2": "çº¦å å¨çé¢ç§¯ç1/2",
        "[åºæ¿æ§åèèæ§]": "[åºæ¿æ§|èèæ§]",
        "æé«ç[ç¥ååº¦åå½±åå]": "æé«ç[ç¥ååº¦|å½±åå]",
        "å½ä¼ç­|å¤®ä¼ç­|å½è¿æ°é": "[å½ä¼ç­|å¤®ä¼ç­|å½è¿æ°é]",
        "[è¿å¨ååå]ä¹ä¸­": "[è¿å¨|åå]ä¹ä¸­",
        "[å«ééå»ç]": "[å«é|éå»ç]",
        "å½å®¶[äº§ä¸æ¿ç­åæ¹é©æªæ½]": "å½å®¶[äº§ä¸æ¿ç­|æ¹é©æªæ½]",
        "[æ£è±ä¼ä¸åå±è§åååºåæ§æ£è±äº¤æå¸åºåå±è§å]": "[æ£è±ä¼ä¸åå±è§å|åºåæ§æ£è±äº¤æå¸åºåå±è§å]",
        "[è¿·æå­¤ç¬]": "[è¿·æ|å­¤ç¬]",
        "è±å½|ç¾å½": "[è±å½|ç¾å½]",
        "[ç¡«åç¿åæ°§åç¿]": "[ç¡«åç¿|æ°§åç¿]",
        "[çº¯ååç­æ´»]çç²èçæ¯åä¹èè¡¨é¢æåçæ··åæ¶²": "[çº¯å|ç­æ´»]çç²èçæ¯åä¹èè¡¨é¢æåçæ··åæ¶²",
        "[åºå±åç»ç»å»ºè®¾åååæè²ç®¡ç]å·¥ä½": "[åºå±åç»ç»å»ºè®¾|ååæè²ç®¡ç]å·¥ä½",
        "[åé¸æ¥çå­£è]": "åé¸æ¥çå­£è",
        "[é»é±¼|å¸¦é±¼|é²³é±¼|è¾ç±»|è¹ç±»åè´è»ç±»]": "[é»é±¼|å¸¦é±¼|é²³é±¼|è¾ç±»|è¹ç±»|è´è»ç±»]",
        "[]åæ¡¥å¤§å­¦ææ|è±åçå®¶å­¦ä¼ä¼å|è±é­èªç¶åå²åç©é¦èµæ·±å çç©å­¦ä¸å®¶": "[åæ¡¥å¤§å­¦ææ|è±åçå®¶å­¦ä¼ä¼å|è±é­èªç¶åå²åç©é¦èµæ·±å çç©å­¦ä¸å®¶]",
        "å­¦æ ¡æç|å­¦çå°±ä¸]ç­ç§ç§å°é¾": "[å­¦æ ¡æç|å­¦çå°±ä¸]ç­ç§ç§å°é¾",
        "æ´é¢å¥¶|ä¿æ¹¿æ°´|ç¼é¨ç²¾æ²¹|é¢é¨ç²¾æ²¹|ç¼è´´è|é¢è´´è]": "[æ´é¢å¥¶|ä¿æ¹¿æ°´|ç¼é¨ç²¾æ²¹|é¢é¨ç²¾æ²¹|ç¼è´´è|é¢è´´è]",
        "[åæ§|åé£|åçºª|ç": "[åæ§|åé£|åçºª]ç",
        "[åç±»åå·¥ç¨ååå·¥æ¹é¢çç»éª|": "[åç±»åå·¥ç¨ååå·¥æ¹é¢çç»éª|ç²¾æ¹çéå±æ·¬ç«ææ¯]",
        "å·«å¯æ²³æ¼æµ|åå¨è§é³é|åéæé¹|èå±±ç|å¤ç§æ|å«ä¸å±±|é¹ææ¥¸èªç¶ä¿æ¤åº|ä¾å®åç§ç¢|ç¦å»ºæçç©ç¾¤|åå¨æ°´ä¸åº¦ååº|ä¹ä»°|ä¹å|è¿ææ°ææ|ä¹è¸èæåè¿éå]": "[å·«å¯æ²³æ¼æµ|åå¨è§é³é|åéæé¹|èå±±ç|å¤ç§æ|å«ä¸å±±|é¹ææ¥¸èªç¶ä¿æ¤åº|ä¾å®åç§ç¢|ç¦å»ºæçç©ç¾¤|åå¨æ°´ä¸åº¦ååº|ä¹ä»°|ä¹å|è¿ææ°ææ|ä¹è¸èæåè¿éå]",
        "ãåä¼¯èç¹ç§é¦2ã|ãé¾å¤åºã]ç­": "[ãåä¼¯èç¹ç§é¦2ã|ãé¾å¤åºã]ç­",
        "[é»ç¼å|ç¼é¨ç±çº¹": "[é»ç¼å|ç¼é¨ç±çº¹]",
        "[æ´é¢å¥¶|ä¿æ¹¿æ°´|ç¼é¨ç²¾æ²¹|é¢é¨ç²¾æ²¹|ç¼è´´è|é¢è´´è]": "æ´é¢å¥¶|ä¿æ¹¿æ°´|ç¼é¨ç²¾æ²¹|é¢é¨ç²¾æ²¹|ç¼è´´è|é¢è´´è]",
        "[åºæ¬ç¹è´¨|ç»éªæè®­": "[åºæ¬ç¹è´¨|ç»éªæè®­]",
        "[ééå¼æ°´|å°è®¸ç½ç³": "[ééå¼æ°´|å°è®¸ç½ç³]",
        "èº«ä½å¥½|åæ°å¤§]ç": "[èº«ä½å¥½|åæ°å¤§]çéå¹´äºº",
        "[ç®¡çå­¦ç»æµå­¦|äººåèµæºç®¡çæ¹é¢": "[ç®¡çå­¦|ç»æµå­¦|äººåèµæºç®¡ç]æ¹é¢",
        "[èå°|æåº|å¬ç¤¾å¤§ä¼åºå°|": "[èå°|æåº|å¬ç¤¾å¤§ä¼åºå°]",
        "âå¿åä¸­åç±å½ç±å®¶ç¬çå¤©ä¸é£äºäºââæ°¸èåæä¸ºä¾ ä¸ºä¹ä¹¦åå¸¸ä¼´å½å£«åâ]": "[âå¿åä¸­åç±å½ç±å®¶ç¬çå¤©ä¸é£äºäºâ|âæ°¸èåæä¸ºä¾ ä¸ºä¹ä¹¦åå¸¸ä¼´å½å£«åâ]",
        "æ£èè«æç|æ£é¾è«ç]ç­": "[æ£èè«æç|æ£é¾è«ç]ç­",
        "[å¶å¯¹åºçææå|ä¸ä¸ä¸ªææåä¸­å¯¹åºä½": "[å¶å¯¹åºçææå|ä¸ä¸ä¸ªææåä¸­å¯¹åºä½]",
        "[ææçææ·±å|æ²éèä¸æ²å|ææ¬èä¸èæ³": "[ææçææ·±å|æ²éèä¸æ²å|ææ¬èä¸èæ³]",
        "[å¡è¿ªæåæ±½è½¦åå¬å¸|åè±æ¯åå¬å¸": "[å¡è¿ªæåæ±½è½¦åå¬å¸|åè±æ¯åå¬å¸]",
        "å¾æº|èå|æ¨¡å¼åæ¢]æé®": "[å¾æº|èå|æ¨¡å¼åæ¢]æé®",
        "|å¸å§|æ¿åºç´å±äºä¸åä½|åé¨é¨æå±äºä¸åä½]çæºææ¹é©æ¹æ¡": "[å¸å§|æ¿åºç´å±äºä¸åä½|åé¨é¨æå±äºä¸åä½]çæºææ¹é©æ¹æ¡",
        "âç©ºçâ|âå®éçâ]æè¯ç¶æ": "[âç©ºçâ|âå®éçâ]æè¯ç¶æ",
        "é¿ä¾æå°|è¾å±±]": "[é¿ä¾æå°|è¾å±±]",
        "è¿äºç­å¿åä¸æ¬é¡¹å·¥ä½çä¸­å¤äººå£«]": "è¿äºç­å¿åä¸æ¬é¡¹å·¥ä½çä¸­å¤äººå£«",
        "[åä¹¡æ°´å©è®¾æ½å»ºè®¾è½åæ°´èµæºå©ç¨æ°´å¹³ä¸ä¿éè½å|": "[åä¹¡æ°´å©è®¾æ½å»ºè®¾è½å|æ°´èµæºå©ç¨æ°´å¹³ä¸ä¿éè½å]",
        "Java|iOS]å¼åå¤å¹´": "[Java|iOS]å¼åå¤å¹´",
        "[ä¸å®çæ©æ¦å|ä¸éçè´¨æ": "[ä¸å®çæ©æ¦å|ä¸éçè´¨æ]",
        "[å¨æ°12è²é¢æå¢¨æ°´ç³»ç»|": "[å¨æ°12è²é¢æå¢¨æ°´ç³»ç»|è¶é«å¯åº¦æå°å¤´ææ¯âFINEâ]",
        "[éå¤§å³ç­|éè¦é¨ç½²|ãæ¿åºå·¥ä½æ¥åã|éç¹å·¥ä½": "[éå¤§å³ç­|éè¦é¨ç½²|ãæ¿åºå·¥ä½æ¥åã|éç¹å·¥ä½]",
        "è±è¯­ï¼Marc Edworthy]]]": "è±è¯­ï¼Marc Edworthy",
        "[å­©å­ä»¬èª": "å­©å­ä»¬èªå·±",
        "å®æ§èµæçå¤å«åæ|å®éèµæçå¤å«åæ[": "[å®æ§èµæçå¤å«åæ|å®éèµæçå¤å«åæ]",
        "é³åéè´­|å¿«ééè´­]åå": "[é³å|å¿«é]éè´­åå",
        "|åç±»æå­¦å®ä¹ |å®éªå®¤|å¤åè½æ¥åå|è®¡ç®æºç½ç»ä¸­å¿|æ ¡å­ç½]": "[åç±»æå­¦å®ä¹ |å®éªå®¤|å¤åè½æ¥åå|è®¡ç®æºç½ç»ä¸­å¿|æ ¡å­ç½]",
        "æ»è¾æ¶¦èº|è¡¥èæç®]": "[æ»è¾æ¶¦èº|è¡¥èæç®]",
        "å¬å®é¨|ä¸å½±è±ç]ç­åä½": "[å¬å®é¨|ä¸å½±è±ç]ç­åä½",
        "ç©å¿½èå®|è¿æ³ä¹±çºª]ç": "[ç©å¿½èå®|è¿æ³ä¹±çºª]ç",
        "Internetç½ç»|IVRè¯­é³ç³»ç»|WAPææºæºè½ä»¥|èªå¨å®ç¥¨æº]ç­å¤ç§æ¹å¼": "[Internetç½ç»|IVRè¯­é³ç³»ç»|WAPææºæºè½ä»¥|èªå¨å®ç¥¨æº]ç­å¤ç§æ¹å¼",
        'ãè¡æ¿ç®¡çä¸æ¿åºè¡ä¸ºç ç©¶ã|ãç¤¾ä¼ä¸»ä¹å¸åºç»æµè®ºã|ãå¯æç»­åå¸ååå±ç ç©¶--ä¸­å½åå·çå®è¯åæã]ç­': '[ãè¡æ¿ç®¡çä¸æ¿åºè¡ä¸ºç ç©¶ã|ãç¤¾ä¼ä¸»ä¹å¸åºç»æµè®ºã|ãå¯æç»­åå¸ååå±ç ç©¶--ä¸­å½åå·çå®è¯åæã]ç­',
        'è©èé¸ç|è°ç|é¢æ¤ç|å³èç|é£æ¹¿|å¤±ç |æ°å]': '[è©èé¸ç|è°ç|é¢æ¤ç|å³èç|é£æ¹¿|å¤±ç |æ°å]',
        'âä¸è½¬æ¬â|âä¸æ¥æ¬â]æ¹å¼': '[âä¸è½¬æ¬â|âä¸æ¥æ¬â]æ¹å¼',
        '[èå¤´è|æ°´éæµå¤´é»åçãéé¥­çãåçä¸ä¸ãéåçãééçå¤´': '[èå¤´è|æ°´éæµå¤´|é»åç|éé¥­ç|åçä¸ä¸|éåç|ééçå¤´]',
        'è¾åºç[åä¸ªéåä¸¤ä¸ªå¼ååº]': 'è¾åºç[åä¸ªé|ä¸¤ä¸ªå¼ååº]',
        '[é¨éåæ²|æ¥ç§åè¶³|æ°åæ¸©å': '[é¨éåæ²|æ¥ç§åè¶³|æ°åæ¸©å]',
        '[å¬å¸è£äº|å¯æ»è£|å¯¼èªäºä¸é¨æ»ç»ç': '[å¬å¸è£äº|å¯æ»è£|å¯¼èªäºä¸é¨æ»ç»ç]',
        'ä¼ æå¨ä¿¡å·è°ç|æ°æ®è½¬æ¢åå¤ç]è§£å³æ¹æ¡æ¹é¢': '[ä¼ æå¨ä¿¡å·è°ç|æ°æ®è½¬æ¢åå¤ç]è§£å³æ¹æ¡æ¹é¢',
        'ç«æ¡çç£|ä¾¦æ¥çç£|åç½æ§è¡çç£]äºé¡¹': '[ç«æ¡çç£|ä¾¦æ¥çç£|åç½æ§è¡çç£]äºé¡¹',
        'æ­¦å¨è£å¤ç§ç çäº§è®¸å¯è¯|ååç§ç çäº§æ¿æåä½ä¿å¯èµæ ¼]å®¡æ¥': '[æ­¦å¨è£å¤ç§ç çäº§è®¸å¯è¯|ååç§ç çäº§æ¿æåä½ä¿å¯èµæ ¼]å®¡æ¥',
        'å°¼æ³å°|é¡é|ä¸ä¸¹|å­å æ|å°åº¦|ä¸­å½å¤§éçè¥¿èèªæ²»åº]ç­å°': '[å°¼æ³å°|é¡é|ä¸ä¸¹|å­å æ|å°åº¦|ä¸­å½å¤§éçè¥¿èèªæ²»åº]ç­å°',
        "ç¾å½ç½å¾·å²å¤§å­¦ç": 'ç¾å½ç½å¾·å²å¤§å­¦',
        'è­æ°§å°æµå¼æ··åæèè£ç½®|ç¾å½é¶æ°å¬å¸åæ¸éè£ç½®|çº¤ç»´æ´»æ§ç­é«æå¸é]ç­': '[è­æ°§å°æµå¼æ··åæèè£ç½®|ç¾å½é¶æ°å¬å¸åæ¸éè£ç½®|çº¤ç»´æ´»æ§ç­é«æå¸é]ç­',
        '[å¯ç±èæ¶å°]ç': '[å¯ç±|æ¶å°]ç',
        "ä½åæä¸»æçâè·¨è¶å¼åå±çâå®éªæ ¡": 'ä½åæä¸»æçâè·¨è¶å¼åå±âçå®éªæ ¡',
        "ä¸­é«ç«¯è½¯ä½å®¶å·ä¸­é«ç«¯è½¯ä½å®¶å·": "ä¸­é«ç«¯è½¯ä½å®¶å·",
        "æ°æ°æä¸»ä¹": "æ°æä¸»ä¹",
        "ä¸æçä¸æç": "ä¸æç",
        "è®¸å¤[é®é¢|é®é¢è§£å³ä¹é]": "è®¸å¤é®é¢[|è§£å³ä¹é]",
        "[å°æ¹¾å¤§å­¦å²å­¦ç³»ä¸»ä»»|æ¯å©æ¶é²æ±¶å¤§å­¦å®¢åº§ææ|è·å°è±é¡¿å¤§å­¦å®¢åº§ææ]": "[å°æ¹¾å¤§å­¦[å²å­¦ç³»ä¸»ä»»|å²å­¦ç ç©¶ææé¿]|[æ¯å©æ¶é²æ±¶å¤§å­¦|è·å°è±é¡¿å¤§å­¦]å®¢åº§ææ]",
        "[å®£ä¼ å¹²äºå¯ç§é¿|å¿æèå¯ä¸»å¸­|å¿æèä¸»å¸­|é»åå¸ä½å®¶åä¼å¯ä¸»å¸­|ä¸­å½éä¿æèºç ç©¶ä¼çäº|æ¹åçæ°é´æèºå®¶åä¼å¯ä¸»å¸­]": "[å®£ä¼ å¹²äºå¯ç§é¿|å¿æè[å¯ä¸»å¸­|ä¸»å¸­]|é»åå¸ä½å®¶åä¼å¯ä¸»å¸­|ä¸­å½éä¿æèºç ç©¶ä¼çäº|æ¹åçæ°é´æèºå®¶åä¼å¯ä¸»å¸­]",
        "[ç½ç»åºç¨æ§è½å é|å®å¨åå®¹ç®¡ç|å®å¨äºä»¶ç®¡ç|ç¨æ·ç®¡ç|ç½ç»èµæºç®¡ç|ç½ç»èµæºä¼å|æ¡é¢ç³»ç»ç®¡ç]": "[ç½ç»åºç¨æ§è½å é|å®å¨åå®¹ç®¡ç|å®å¨äºä»¶ç®¡ç|ç¨æ·ç®¡ç|ç½ç»èµæº[ç®¡ç|ä¼å]|æ¡é¢ç³»ç»ç®¡ç]",
        "[ç è´¨å²©ç è´¨å²©|ç¢³é¸çå²©|çº¢å²©|ç¬¬åçºªæ¾æ£å ç§¯ç©]": "[ç è´¨å²©|ç¢³é¸çå²©|çº¢å²©|ç¬¬åçºªæ¾æ£å ç§¯ç©]",
        "[èå¡å|èå¡éè¿å°å¸¦]": "èå¡[å|éè¿å°å¸¦]",
        "[è±å½æµªæ¼«ä¸»ä¹é£æ¯ç»å®¶|èåçæ°´å½©ç»å®¶|èåçæ²¹ç»å®¶]": "[è±å½æµªæ¼«ä¸»ä¹é£æ¯ç»å®¶|èåç[æ°´å½©ç»å®¶|æ²¹ç»å®¶]]",
        "å¦ä½æè½ä½¿[ç»ç»çåééä¸­éä¸­å®ç°ä¸åçæ æææ|æææ§å¶åå°é£é©]": "å¦ä½æè½ä½¿[ç»ç»çåééä¸­å®ç°ä¸åçæ æææ|æææ§å¶åå°é£é©]",
        "[ææ°ç ç©¶|ææ°è®¾è®¡|ææ°å¶é ]ç": "ææ°[ç ç©¶|è®¾è®¡|å¶é ]ç",
        "[å¹³ç´ è°å»|äºå¥åº·ç¶æ|æ¢æ§ç¾ç|èå¼±æ§ç¾ç|çåæ¢å¤]ç­äººç¾¤": "[å¹³ç´ è°å»|äºå¥åº·ç¶æ|[æ¢æ§|èå¼±æ§]ç¾ç|çåæ¢å¤]ç­äººç¾¤",
        "[åå§å¸¸å§|æ¹åå¤§å­¦å¯æ ¡é¿|é©¬åæä¸»ä¹å­¦é¢åå£«çå¯¼å¸|é©¬åæä¸»ä¹å­¦é¢ææ|æ¹åçæçªåºè´¡ç®ä¸­éå¹´ä¸å®¶]": "[æ¹åå¤§å­¦[åå§å¸¸å§|å¯æ ¡é¿]|é©¬åæä¸»ä¹å­¦é¢[åå£«çå¯¼å¸|ææ]|æ¹åçæçªåºè´¡ç®ä¸­éå¹´ä¸å®¶]",
        "[å®éªå®¤|é³ä¹å®¤å®éªå®¤|é³ä¹å®¤|ç§ææ´»å¨å®¤|å­¦çæºæ¿|å¤åè½æå®¤|å­¦çå¬å¯|ç§ææ´»å¨å®¤|å­¦çæºæ¿|å¤åè½æå®¤|å­¦çå¬å¯]": "[å®éªå®¤|é³ä¹å®¤|ç§ææ´»å¨å®¤|å­¦çæºæ¿|å¤åè½æå®¤|å­¦çå¬å¯]",
        "æå½[éèé«å|éèé«åæ¯é»å°åº]": "æå½éèé«å[|æ¯é»å°åº]",
        "[é£æºç¨çæ|ç«ç®­ç¨æ¨è¿å|åç§æ¶¦æ»å|åç§æ¶²åæ²¹]": "[é£æºç¨çæ|ç«ç®­ç¨æ¨è¿å|åç§[æ¶¦æ»å|æ¶²åæ²¹]]",
        "[å±åç½ä¿¡æ¯ä¼ é|å¹¿åç½ä¿¡æ¯ä¼ é]": "[å±åç½|å¹¿åç½]ä¿¡æ¯ä¼ é",
        "[ãå¬å¸æ³ã|æå³æ³å¾|æå³æ³è§]çè§å®": "[ãå¬å¸æ³ã|æå³[æ³å¾|æ³è§]çè§å®]",
        "[[èªå·±|æ­£éä»¬]çåç§|èªå·±ççæ]": "[æ­£éä»¬çåç§|èªå·±ççæ]",
        "[å¹è®­å­¦åç®¡ç||å¹è®­å­¦åç®¡çåç¤¾åºåº·å¤åè°åæè¯ä¸å²]": "[å¹è®­å­¦åç®¡ç|ç¤¾åºåº·å¤åè°åæè¯ä¸å²]",
        "[ä¸æµ·å¸ä½è²æ»ä¼ç¬¬ä¸å±å§åä¼å¸¸å§|ä¸æµ·å¸ä½è²æ»ä¼ç¬¬ä¸å±å§åä¼å¯ä¸»å¸­]": "ä¸æµ·å¸ä½è²æ»ä¼ç¬¬ä¸å±å§åä¼[å¸¸å§|å¯ä¸»å¸­]",
        "[äººç±»åç³20ä½ä»¶|ç³å¶åä¸ä½ä»¶|éª¨è§å¨åç³|åºä¹³å¨ç©åç³]": "[äººç±»åç³20ä½ä»¶|ç³å¶åä¸ä½ä»¶|[éª¨è§å¨|åºä¹³å¨ç©]åç³]",
        "[ç±è®¾åºçå¸æ¿åºè¡ä½¿çç»æµç¤¾ä¼ç®¡çæé|[çæ¿åº|çæ¿åºé¨é¨]ä¸æ¾ç»[çè¾å¸æ¿åº|çè¾å¸æ¿åºé¨é¨]çç»æµç¤¾ä¼ç®¡çæé": "[ç±è®¾åºçå¸æ¿åºè¡ä½¿çç»æµç¤¾ä¼ç®¡çæé|çæ¿åº[|é¨é¨]ä¸æ¾ç»çè¾å¸æ¿åº[|é¨é¨]çç»æµç¤¾ä¼ç®¡çæé]",
        "[çè¾å¸æ¿åº|çè¾å¸æ¿åºé¨é¨]": "[çè¾å¸æ¿åº[|é¨é¨]]",
        "[ç°å­|åéäºåä¸å¹´ææ|åº·çåä¸å¹´ææ]": "[ç°å­[åéäºåä¸å¹´|åº·çåä¸å¹´]ææ]",
        '[åç±»æ¿å±å»ºç­|æ¿å±å»ºç­éå±è®¾æ½|å¸æ¿è®¾æ½æéè®¾é²]': '[åç±»æ¿å±å»ºç­[|éå±è®¾æ½]|å¸æ¿è®¾æ½æéè®¾é²]',
        '[å¤§åè¿éå¼å¯¼æå¡æ³|éè¦æå®¢ç»è®°æå¡æ³|ç¹æ®éè¦é¢çº¦æå¡æ³|åè¯­å¼æå¡æ³|ç½ç»å¼çéæå¡æ³]': '[å¤§åè¿éå¼å¯¼æå¡æ³|éç¹æå®¢ç»è®°æå¡æ³|ç¹æ®éè¦é¢çº¦æå¡æ³|åè¯­å¼æå¡æ³|ç½ç»å¼çéæå¡æ³]',
        "[èåæ°å­¦å®¶|æ²å°å¤«å¥å¾ä¸»|é¿è´å°å¥å¾ä¸»]": "[èåæ°å­¦å®¶|[æ²å°å¤«å¥|é¿è´å°å¥]å¾ä¸»]",
        "[åç­ç»è|èç­ç»è|çè|éµæ¯èèæ ª|ç¸å³åè§£é¶]": "[[åç­|èç­]ç»è|çè|éµæ¯èèæ ª|ç¸å³åè§£é¶]",
        "[å·äººæ°æ¿åº|å¸äººæ°æ¿åº|å°åºè¡æ¿å¬ç½²ç¿å±±ä¼ä¸ä¸»ç®¡é¨é¨]ä¼åæå³[é¨é¨|åä½]": "[[å·|å¸]äººæ°æ¿åº|å°åºè¡æ¿å¬ç½²ç¿å±±ä¼ä¸ä¸»ç®¡é¨é¨]ä¼åæå³[é¨é¨|åä½]",
        "ä¸éä¸éè·¯": "ä¸éè·¯",
        "æ¨ç®æåé¢å¯ä¸ä¸å¤çç¸éçéééé": "æ¨ç®æåé¢å¯ä¸ä¸å¤çç¸éçéé",
        "[éé¸¡|éé¸¡]": "[éé¸¡|éé¸­]",
        "ä¸æ¹[æ¹éª¨|è§å¨]": "ä¸æ¹[éª¨|è§å¨]",
        "ä»¤äººæ¦ç¶å¿å¨çè½½ä½è½½ä½": "ä»¤äººæ¦ç¶å¿å¨çè½½ä½",
        "[å¢ä½æ¯èµæ»åç¬¬ä¸|ä¸ªäººç¬¬äºå|ä¸ªäººç¬¬ä¸å]": "[å¢ä½æ¯èµæ»åç¬¬ä¸|ä¸ªäºº[ç¬¬äºå|ç¬¬ä¸å]]",
        '[ä»ä¹å¯ä»¥å|ä»ä¹ä¸å¯ä»¥å]': '[ä»ä¹å¯ä»¥å|ä¸å¯ä»¥å]',
        '[âç¬¬äºå±ä¸­å½è¯å§èºæ¯èâ|âç¬¬å«å±ä¸­å½æå§èâ|âå¨å½å°æ¹ææ²ä¼ç§å§ç®è¯æ¯å±æ¼â|çæèºç²¾åå·¥ç¨ä¸ç­å¥çæèºç²¾åå·¥ç¨ä¸ç­å¥]': '[âç¬¬äºå±ä¸­å½è¯å§èºæ¯èâ|âç¬¬å«å±ä¸­å½æå§èâ|âå¨å½å°æ¹ææ²ä¼ç§å§ç®è¯æ¯å±æ¼â|çæèºç²¾åå·¥ç¨ä¸ç­å¥]',
        'å¿çº§éç¹æç©ä¿æ¤åä½å¿çº§éç¹æç©ä¿æ¤åä½': 'å¿çº§éç¹æç©ä¿æ¤åä½',
        'ç¡é¸|åçç¡«é¸|æ°¯ç£ºé¸ãä¹ç¯äºèºãä¹ç¯äºèºãæ°¢æ°§åé ': '[ç¡é¸|åçç¡«é¸|æ°¯ç£ºé¸|ä¹ç¯äºèº|ä¹ç¯äºèº|æ°¢æ°§åé ]',
        'å¨å½å°çéå®å¬å¸|ç¹çº¦ç»éå¤æå¡äººå': 'å¨å½å°ç[éå®å¬å¸|ç¹çº¦ç»éå¤æå¡äººå]',
        'äºææ¥å¤|è´£ä»»è¿½ç©¶]è½å®æåµ': '[äºææ¥å¤|è´£ä»»è¿½ç©¶]è½å®æåµ',
        'ç°æç[é²ç«å¢|å®é²æ£æµ|å¥ä¾µæ£æµ|è´è½½åè¡¡|é¢å®½ç®¡ç|ç½ç»é²æ¯]ç­[è®¾å¤|ç½ç»]é®é¢': 'ç°æç[é²ç«å¢|[å®é²|å¥ä¾µ]æ£æµ|è´è½½åè¡¡|é¢å®½ç®¡ç|ç½ç»é²æ¯]ç­[è®¾å¤|ç½ç»]é®é¢',
        '[æ¥å¥ç¡¬ä»¶|è½¯ä»¶æä½çæ¹å¼]': '[æ¥å¥[ç¡¬ä»¶|è½¯ä»¶]æä½çæ¹å¼]',
        '[ä¾¯æ¹å]éè´æ§]å´åºç®]': '[ä¾¯æ¹å|éè´æ§|å´åºç®]',
        '[èåè¡åç­å¿é¡»è£å¤|éæ©åºåæ¥æ|å³å®æ¹å|è·¯çº¿å|åæ¬å¯è½çæ¿ä»£çº¿è·¯|åå¤å¥½å°å¾|æGPS|éæ©è¡¥ç»å°ç¹|äºåé®å¯å¾é¢çè¡¥åç»å»|é¢è®¢èµ·ç»ç¹çäº¤é|ä½å®¿]ç­': '[èåè¡åç­å¿é¡»è£å¤|éæ©åºåæ¥æ|å³å®[æ¹å|è·¯çº¿|å¯è½çæ¿ä»£çº¿è·¯]|åå¤å¥½[å°å¾|GPS]|éæ©è¡¥ç»å°ç¹|äºåé®å¯å¾é¢çè¡¥åç»å»|é¢è®¢èµ·ç»ç¹ç[äº¤é|ä½å®¿]]ç­',
        'IEC61032å¾7è¯å·11|GB/T16842è¯å·11|GB8898|IEC60065|IEC60598|GB7000|IEC60335|GB4706]ç­æ åè¦æ±': '[IEC61032å¾7è¯å·11|GB/T16842è¯å·11|GB8898|IEC60065|IEC60598|GB7000|IEC60335|GB4706]ç­æ åè¦æ±',
        'ä½ä¸ºå´å»ºæ¿å±ä¹ç¨ç': 'ä½ä¸ºå´å»ºæ¿å±ä¹ç¨',
        'é¡¹ç®ææ |å·¥ç¨è´¨éçç£ç§æä¿¡æ¯]å·¥ä½': '[é¡¹ç®ææ |å·¥ç¨è´¨éçç£|ç§æä¿¡æ¯]å·¥ä½',
        "[è±å­æ¯|çµè¯|å¹³é¢çµè§|ä¿é©ç®±|ä¹¦æ¡|ææ°|æ´è¡£å®¤|æ²å|æ¨è´¨/é¶æ¨å°æ¿|è¡£æ/è¡£æ©±|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|æµ´ç¼¸ORæ·æµ´|å¤éæå¡]": "[è±å­æ¯|çµè¯|å¹³é¢çµè§|ä¿é©ç®±|ä¹¦æ¡|ææ°|æ´è¡£å®¤|æ²å|æ¨è´¨/é¶æ¨å°æ¿|è¡£æ/è¡£æ©±|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|æµ´ç¼¸|æ·æµ´|å¤éæå¡]",
        "[è·å¾[çå­|åå±|å£®å¤§|ä¸ºå¨ç¤¾ä¼æå¡]": "[è·å¾[çå­|åå±|å£®å¤§]|ä¸ºå¨ç¤¾ä¼æå¡]",
        "[[[åºç¨çç©ææ¯|åå·¥|è¯ç©å¶å|]ç¸å³ä¸ä¸å¨æ¥å¶å­¦ç|å¹è®­æºæå­¦ç]": "[[åºç¨çç©ææ¯|åå·¥|è¯ç©å¶å|ç¸å³ä¸ä¸][å¨æ¥å¶å­¦ç|å¹è®­æºæå­¦ç]]",
        "[å¸å§åé¨é¨ä¹é´|æ¿åºåé¨é¨ä¹é´|å¸å§åé¨é¨ä¸æ¿åºåé¨é¨ä¹é´|å¸ç´é¨é¨ä¸ä¹¡ä¹é´|å¸ç´é¨é¨ä¸éä¹é´]çèè´£åå·¥]": "[å¸å§åé¨é¨ä¹é´|æ¿åºåé¨é¨ä¹é´|å¸å§åé¨é¨ä¸æ¿åºåé¨é¨ä¹é´|å¸ç´é¨é¨ä¸[ä¹¡|é]ä¹é´]çèè´£åå·¥",
        "[æ±æ²³|æºªæµ|æ¹æ³|æ°´å¡|æµ·å²¸]ç­æ°´åå²¸è¾¹|å¶æµæ°´å¤]": "[[æ±æ²³|æºªæµ|æ¹æ³|æ°´å¡|æµ·å²¸]ç­æ°´åå²¸è¾¹|å¶æµæ°´å¤]",
        "[æ·±åæ¹é©|æ©å¤§å¼æ¾|å¼è¿å½åå¤[èµé|ææ¯|äººæ]": "[æ·±åæ¹é©|æ©å¤§å¼æ¾|å¼è¿å½åå¤[èµé|ææ¯|äººæ]]",
        "[å¾·å½EVITAââ¡å¨èªå¨å¼å¸æº|CSIâ507SDå¤åæ°çæ¤ä»ª|æ¥æ¬ç¾è½èªå¨çµå¼åº|æ´åææ¯å®¤]|": "[å¾·å½EVITAââ¡å¨èªå¨å¼å¸æº|CSIâ507SDå¤åæ°çæ¤ä»ª|æ¥æ¬ç¾è½èªå¨çµå¼åº|æ´åææ¯å®¤]",
        "[åä¸­|å°å­¦]|æè²": "[åä¸­|å°å­¦]æè²",
        "[éå±ORç¡¬çº¸|å¡æ]": "[éå±|ç¡¬çº¸|å¡æ]",
        "ç¢±[ç¢±2|10|25]mg": "ç¢±[2|10|25]mg",
        "[å¸åº|ç¤¾ä¼çäº§]|": "[å¸åº|ç¤¾ä¼çäº§]",
        "é¿å·è©[ç®å°|åè°]|": "é¿å·è©[ç®å°|åè°]",
        "[æå­|éæ]|[å°è¾¹æ ä¸|[æ¿åå±å|åº­é¢ä¸­]çæ ä¸]": "[æå­|[éæ|å°è¾¹]æ ä¸|[æ¿åå±å|åº­é¢ä¸­]çæ ä¸]",
        "[é©¬åæä¸»ä¹[æ°æè§|å®æè§|åç[æ°æå®æçè®º|æ¿ç­|æ³å¾æ³è§]]": "[é©¬åæä¸»ä¹[æ°æè§|å®æè§]|åç[æ°æå®æçè®º|æ¿ç­|æ³å¾æ³è§]]",
        "åå·¥å§å³äºåç[ææ³|ç»ç»]ä½é£å»ºè®¾|ååæè²è®¡å|ä¸­å¿ç»å­¦ä¹ è®¡å]": "[åå·¥å§å³äºåç[ææ³|ç»ç»]ä½é£å»ºè®¾|ååæè²è®¡å|ä¸­å¿ç»å­¦ä¹ è®¡å]",
        "[[å½å|çå]åå¸[é³å|æèºç]äººå£«": "[å½å|çååå¸][é³å|æèºç]äººå£«",
        "[åè½¬è¤¶æ²|å¹³å§è¤¶æ²ORéæ©æ­å±æ¨è¦æé ä½]": "[åè½¬è¤¶æ²|å¹³å§è¤¶æ²|éæ©æ­å±æ¨è¦æé ä½]",
        "[[ææº|PDA|MP4|ç¬è®°æ¬çµè|å°åæ¥æ¶ç»ç«¯]": "[ææº|PDA|MP4|ç¬è®°æ¬çµè|å°åæ¥æ¶ç»ç«¯]",
        "[å¤§åå¸|ä¸­åå¸|å°åå¸|å°åé]": "[[å¤§|ä¸­|å°]åå¸|å°åé]",
        "[æ°¯æ°°èé¯ç­|ç¹è°±å]æ··åå·é¾]": "[æ°¯æ°°èé¯ç­æè«å|ç¹è°±åæ··åå·é¾]",
        "[æ±æå«çé¢éå»º|æ±æå«çé¢å åºé¡¹ç®|æ±æå°å­¦éå»ºé¡¹ç®|æ±æå¹¼å¿å­éå»ºé¡¹ç®]": "[æ±æå«çé¢[éå»º|å åº]é¡¹ç®|æ±æ[å°å­¦|å¹¼å¿å­]éå»ºé¡¹ç®]",
        "[è´¢æ¿ã[éè|å¶ä»ç»æµ|ç¤¾ä¼åå±]": "[è´¢æ¿|éè|å¶ä»ç»æµ|ç¤¾ä¼åå±]çæåµ",
        "[å¦é¨å¸è¹ä¸åä¼|æ°´è·¯è¿è¾è¡æ¿ä¸»ç®¡é¨é¨ORå¶ä»æå³é¨é¨]": "[å¦é¨å¸è¹ä¸åä¼|æ°´è·¯è¿è¾è¡æ¿ä¸»ç®¡é¨é¨|å¶ä»æå³é¨é¨]",
        "[æ¶è´¹èä¸ä¼ä¸æå³ç[ç»æµ|æ¿æ²»|ç¤¾ä¼|æ¥å¸¸æ´»å¨]èå´åç[è¡ä¸º|éè¦|æåº¦|å¨æº]ç­": "æ¶è´¹èä¸ä¼ä¸æå³ç[ç»æµ|æ¿æ²»|ç¤¾ä¼|æ¥å¸¸]æ´»å¨èå´åç[è¡ä¸º|éè¦|æåº¦|å¨æº]",
        "[[åäº¬|ä¸æµ·|å¤©æ´¥]ç­å°æ¹æ¿åº]|[ä¸­å¤®æ¿åº[æ°æ¿|ç¤¾ä¼ç¦å©|å¤èµç®¡ç]ç­é¨é¨çä¸é¡¹å¨è¯¢é¡¹ç®çé¡¾é®]]": "[[åäº¬|ä¸æµ·|å¤©æ´¥]ç­å°æ¹æ¿åº|ä¸­å¤®æ¿åº][æ°æ¿|ç¤¾ä¼ç¦å©|å¤èµç®¡ç]ç­é¨é¨çä¸é¡¹å¨è¯¢é¡¹ç®çé¡¾é®",
        "åæ¬[ä¸­å½[å¸é¿æµè¯|åå¸å¼ååºæèµç¯å¢è¯ä¼°]ç­å¨åçéè¦é¡¹ç®]]": "ä¸­å½[å¸é¿æµè¯|åå¸å¼ååºæèµç¯å¢è¯ä¼°]ç­éè¦é¡¹ç®",
        "[æºå³è¡æ¿äºå¡ç®¡ç|å¯¹å¤[èç»|åè°|æ¥å¾]|ä¼è®®çç»ç»å®æ|å¯¹å¤ä¿¡æ¯åå¸]]": "[æºå³è¡æ¿äºå¡ç®¡ç|å¯¹å¤[èç»åè°|æ¥å¾]|ä¼è®®çç»ç»å®æ|å¯¹å¤ä¿¡æ¯åå¸]",
        "[DESCåå®¹ä¸°å¯|æ´»å¨å¨é¢]": "[åå®¹ä¸°å¯|æ´»å¨å¨é¢]",
        "[å¦é¨ç¥éå¨æ­¦å¹³çæ´»ç»é¢æ°åå¹´æ¥æ­¦å¹³å¦é¨é´çäº¤æµæ´»å¨|æ°åå¹´æ¥æ­¦å¹³å¦é¨é´çäº¤æµæ´»å¨]": "[å¦é¨ç¥éå¨æ­¦å¹³çæ´»ç»é¢|æ°åå¹´æ¥æ­¦å¹³å¦é¨é´çäº¤æµæ´»å¨]",
        "[å½å[æ­¦æ¯ç|æè²ç]ç[ä¸å®¶|ææ|å­¦è]": "å½å[æ­¦æ¯ç|æè²ç]ç[ä¸å®¶|ææ|å­¦è]",
        "[è°èæ§|æ··åæ§]è¿è§]": "[è°èæ§|æ··åæ§]è¿è§",
        "[åä¸ªä¸åº|ä¸ä¸ç»]ç[éå¥|é¶å¥|éå¥|ä¼ç§èè¹æ°ä½åå¥]å±ä¸åäºä¸ª": "[[ä¸åº|ä¸ä¸ç»]ç[é|é¶|é]å¥|ä¼ç§èè¹æ°ä½åå¥]",
        "[æä½èççç»ç¨åº¦|è¿æä½èççç»ç¨åº¦|è¿éæ¬¡æ°|ç©¿åºéä¸ç©¿åºç¹è¸èåçº¿ä½çéè§åº¦|èºæ°è¿]ç­å ç´ éæ¬¡æ°ãç©¿åºéä¸ç©¿åºç¹è¸èåçº¿ä½çéè§åº¦åèºæ°è¿ç­å ç´ ": "[æä½èççç»ç¨åº¦|è¿éæ¬¡æ°|ç©¿åºéä¸ç©¿åºç¹è¸èåçº¿ä½çéè§åº¦|èºæ°è¿]ç­å ç´ ",
        "ç´æµ1mAçµå[ï¼U1mAï¼|0.75 U1mA|0.75 U1mA]": "[ç´æµ1mAçµåï¼U1mAï¼|0.75 U1mAä¸æ³æ¼çµæµ]",
        "æ®ç¾äººæ¥å[åº·å¤è®­ç»|æºææå»|èä¸å¹è®­|å°±ä¸æå¯¼]|[å¼å±æå|ä½è²æ´»å¨]]": "æ®ç¾äºº[æ¥å[åº·å¤è®­ç»|æºææå»|èä¸å¹è®­|å°±ä¸æå¯¼]|å¼å±[æå|ä½è²]æ´»å¨]",
        "[ç¾åº¦å¬å¸çæç¥è§å|ç¾åº¦å¬å¸çè¿è¥ç®¡ç]": "ç¾åº¦å¬å¸ç[æç¥è§å|è¿è¥ç®¡ç]",
        "[å·éå¬è·¯|æç»µé«éå¬è·¯|ç»µå¹¿é«éå¬è·¯]åæ¥ææéè·¯|ææ¸éè·¯|ææ¸é«éå¬è·¯]": "[å·éå¬è·¯|æç»µé«éå¬è·¯|ç»µå¹¿é«éå¬è·¯|ææéè·¯|ææ¸éè·¯|ææ¸é«éå¬è·¯]",
        "[[åå·¥|å¶é|é¢å|ç®¡é]ç­å¤å¿ä¸å¯å°çåä»¶": "[åå·¥|å¶é|é¢å|ç®¡é]ç­å¤å¿ä¸å¯å°çåä»¶",
        "[å·å¤[ç®¡ç|ç»æµ|æ³å¾|äººåèµæºç®¡ç]ç­æ¹é¢çç¥è¯åè½å]|[è½å¨[äºä¸åä½|æ¿åºé¨é¨]ä»äº[äººåèµæºç®¡ç]|[[æå­¦|ç§ç ]æ¹é¢å·¥ä½ç]å·¥åç®¡çå­¦ç§é«çº§ä¸é¨äººæ": "[å·å¤[ç®¡ç|ç»æµ|æ³å¾|äººåèµæºç®¡ç]ç­æ¹é¢çç¥è¯åè½å|è½å¨[äºä¸åä½|æ¿åºé¨é¨]ä»äº[äººåèµæºç®¡ç|æå­¦|ç§ç ]æ¹é¢å·¥ä½]çå·¥åç®¡çå­¦ç§é«çº§ä¸é¨äººæ",
        "[[éå±±|ä¸­å½é¶è|BBTVç¾è§é|ç§å¤§è®¯é£]ç­åä½ä¼ä¼´]|å¨çè½¯ä»¶å¼åå¤§èµç­è½¯ä»¶å¼åå¹³å°|éå¸å¨ççç åæºæ|æ¶²æ¶é¢æ¿ç­ä¸æ¸¸èµæºä¿é]": "[[éå±±|ä¸­å½é¶è|BBTVç¾è§é|ç§å¤§è®¯é£]ç­åä½ä¼ä¼´|å¨çè½¯ä»¶å¼åå¤§èµç­è½¯ä»¶å¼åå¹³å°|éå¸å¨ççç åæºæ|æ¶²æ¶é¢æ¿ç­ä¸æ¸¸èµæºä¿é]",
        "[[HTTP|TCP|UDPï¼SUDP|RUDPï¼|ç½å³ç©¿éæ¨¡ç»|å¨çIPè¡¨]": "[HTTP|TCP|UDP|SUDP|RUDP|ç½å³ç©¿éæ¨¡ç»|UDPç©¿é|RPNPç©¿é|å¨çIPè¡¨]",
        "å¨å¸[ç»æµ|[èµæº|ç¯å¢]åè°åå±": "å¨å¸[ç»æµ|èµæº|ç¯å¢]åè°åå±",
        "[æ±ç¯|æ±é¶]ç»åçä¹¦æ³ååè½¨è¿¹|[æ±ç¯|æ±é¶|å«åä½]çç¾çé­å]": "[[æ±ç¯|æ±é¶]ç»åçä¹¦æ³ååè½¨è¿¹|[æ±ç¯|æ±é¶|å«åä½]çç¾çé­å]",
        "[å¨å¸åæ¿ç¾¤æºå³ç´å±äºä¸åä½|åé¨é¨æå±äºä¸åä½]ãæºæç¼å¶ç®¡çè¯ã|æ°å¢äººåç[æ§å¶|åç¼]ç­æç»­": "[å¨å¸åæ¿ç¾¤æºå³[ç´å±äºä¸åä½|åé¨é¨æå±äºä¸åä½]ãæºæç¼å¶ç®¡çè¯ã|æ°å¢äººåç[æ§å¶|åç¼]æç»­]",
        "[[âç¨³ç²®|å¢çª|å´ç|æ©ç»|ä¿æ|å¼ºå·¥â]æ»ä½æè·¯": "[ç¨³ç²®|å¢çª|å´ç|æ©ç»|ä¿æ|å¼ºå·¥]æè·¯",
        "[[è½»æ¾|æäº®|åç¡®|åæ¶¦]": "[è½»æ¾|æäº®|åç¡®|åæ¶¦]",
        "[ä¿é©ç®±|ç¨æ|ä¹¦æ¡|ç¨è¡£è®¾å¤|ææ°|æ·æµ´|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´ç¼¸ORæ·æµ´|çµè§|çµè¯|æçº¿é¢é|è¿·ä½ å§|å°ç®±]": "[ä¿é©ç®±|ç¨æ|ä¹¦æ¡|ç¨è¡£è®¾å¤|ææ°|æ·æµ´|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´ç¼¸|æ·æµ´|çµè§|çµè¯|æçº¿é¢é|è¿·ä½ å§|å°ç®±]",
        "[åå°æµæ¼äºé¡¹|æ¿äº§æµæ¼äºé¡¹|è½¦è¾æµæ¼äºé¡¹|è®¾å¤æµæ¼äºé¡¹]": "[åå°|æ¿äº§|è½¦è¾|è®¾å¤]æµæ¼äºé¡¹",
        "[âä¸èµä¼ä¸â|ææ¸¸å¼åé¡¹ç®æä»¶çå®¡æ¥|æ¥æ¹|åè°æå¡]]": "[âä¸èµä¼ä¸â|ææ¸¸å¼åé¡¹ç®æä»¶ç[å®¡æ¥|æ¥æ¹|åè°æå¡]]",
        "ä¸é¨[åå§|ç±æçORå§æç]": "ä¸é¨[åå§|ç±æç|å§æç]",
        "[å£°é³|æ­å±çåå®¹|æ­å±èçé£åº¦ãä»ªè¡¨ãæ°è´¨æ­å±èçé£åº¦ãä»ªè¡¨ãæ°è´¨]": "[å£°é³|æ­å±çåå®¹|æ­å±èç[é£åº¦|ä»ªè¡¨|æ°è´¨]]",
        "[ä¸å¡å¹è®­||çè®ºè°ç |å®£ä¼ |ä¿¡æ¯|åºå±é¢å¤æ¡ææç[æ¶é|æ¥é|å¤åªä½ç¤ºè¯]å·¥ä½": "[ä¸å¡å¹è®­|çè®ºè°ç |å®£ä¼ |ä¿¡æ¯|åºå±é¢å¤æ¡ææç[æ¶é|æ¥é|å¤åªä½ç¤ºè¯]å·¥ä½]",
        "[éåºè½æºç»æä¸­[æ¸æ´è½æº|å¯åçè½æº|æ°è½æº]": "éåºè½æºç»æä¸­[æ¸æ´è½æº|å¯åçè½æº|æ°è½æº]",
        "[æ§è¡STS - 41Cæ¡|å¨å¤ªç©ºä¸­è®°å½äº168ä¸ªå°æ¶ç]ç": "[æ§è¡STS - 41Cæ¡|å¨å¤ªç©ºä¸­è®°å½äº168ä¸ªå°æ¶ç]",
        "[åºæ¬å·ç |ç¹å«å·ç ]|": "[åºæ¬å·ç |ç¹å«å·ç ]",
        "[[è¯å¥½|æ­£ç¡®]ç[åå£°æ¹æ³|åå£°æå·§]": "[[è¯å¥½|æ­£ç¡®]ç[åå£°æ¹æ³|åå£°æå·§]]",
        "ä¸»æºçææç¨æ·ç[æ³¨åå|çå|æåç»å½æ¶é´|ä½¿ç¨shellç±»å]ç­]": "ä¸»æºçææç¨æ·ç[æ³¨åå|çå|æåç»å½æ¶é´|ä½¿ç¨shellç±»å]ç­",
        "[å½å®¶å¤§æ¿æ¹é|[æ¿æ²»|ç»æµ|ç¤¾ä¼çæ´»]]ä¸­çéè¦é®é¢]": "[å½å®¶å¤§æ¿æ¹é|[æ¿æ²»|ç»æµ|ç¤¾ä¼çæ´»]ä¸­çéè¦é®é¢]",
        "[è®¡ç®æºç§å­¦ä¸ææ¯|è½¯ä»¶å·¥ç¨è®¡ç®æºç§å­¦ä¸ææ¯ãè½¯ä»¶å·¥ç¨ãç½ç»å·¥ç¨ãçµå­ä¿¡æ¯å·¥ç¨ãéä¿¡å·¥ç¨ãèªå¨åãä¿¡æ¯ç®¡çä¸ä¿¡æ¯ç³»ç»ç­7ä¸ªæ¬ç§ä¸ä¸|ç½ç»å·¥ç¨|çµå­ä¿¡æ¯å·¥ç¨|éä¿¡å·¥ç¨|èªå¨å|ä¿¡æ¯ç®¡çä¸ä¿¡æ¯ç³»ç»]ç­7ä¸ªæ¬ç§ä¸ä¸": "[è®¡ç®æºç§å­¦ä¸ææ¯|è½¯ä»¶å·¥ç¨|ç½ç»å·¥ç¨|çµå­ä¿¡æ¯å·¥ç¨|éä¿¡å·¥ç¨|èªå¨å|ä¿¡æ¯ç®¡çä¸ä¿¡æ¯ç³»ç»]",
        "[æå±å°æ°æ°æ|åéå°æ°æ°æ]|å°æ°æ°æ[å¦å¥³|å¿ç«¥]ä¿æ¤ç­æå³äºå®": "[æ£æå±å°æ°æ°æ|åéå°æ°æ°æ|å°æ°æ°æ[å¦å¥³|å¿ç«¥]ä¿æ¤]",
        "[ä¸­åæå|æ±æ·®æå|ééµæå|å´æå]": "[ä¸­å|æ±æ·®|ééµ|å´]æå",
        "å¤§é[åè´¹|æ­¦å¨å¼¹è¯]|": "å¤§é[åè´¹|æ­¦å¨å¼¹è¯]",
        "[è°æ´|ç¼å²ORçº¿æ§åå¤ç]": "[è°æ´|ç¼å²|çº¿æ§åå¤ç]",
        "[é¡¹ç®å»ºè®¾ç[ç¨åº|è´¨é|å®å¨|è¿åº¦|èµéä½¿ç¨ï¼ç»ç®ï¼|å³ç®|ç«£å·¥]ç­å¨è¿ç¨": "é¡¹ç®å»ºè®¾ç[ç¨åº|è´¨é|å®å¨|è¿åº¦|èµéä½¿ç¨ï¼ç»ç®ï¼|å³ç®|ç«£å·¥]ç­å¨è¿ç¨",
        "[æ±æ|éå¿äºº|ä¸­å±åå|[ä¸­å½æå§å®¶åä¼ä¼å|æ²³åçæå§å®¶åä¼çäº|æ²³åçèºæ¯åä½ä¸­å¿ç¹çº¦å¯¼æ¼|ä¸é¨å³¡å¸æå§å®¶åä¼ä¸»å¸­|ä¸é¨å³¡å¸æåå±èºæ¯ç§ç§é¿]": "[æ±æ|éå¿äºº|ä¸­å±åå|ä¸­å½æå§å®¶åä¼ä¼å|æ²³åçæå§å®¶åä¼çäº|æ²³åçèºæ¯åä½ä¸­å¿ç¹çº¦å¯¼æ¼|ä¸é¨å³¡å¸æå§å®¶åä¼ä¸»å¸­|ä¸é¨å³¡å¸æåå±èºæ¯ç§ç§é¿]",
        "å¨çæå¤§è§æ¨¡ç[æç´¢å¼æ[è¥é|ä¼å]ä¸ä¸ä¼è®®æç´¢å¼ææç¥å¤§ä¼": "å¨çæå¤§è§æ¨¡çæç´¢å¼æ[è¥é|ä¼å]ä¸ä¸ä¼è®®æç´¢å¼ææç¥å¤§ä¼",
        "[ãé­æ³å°å¥³å¥å¶StrikerS THE COMICSãç¬¬ä¸å·åè¡æ¬æ¼«ç»ï¼æ¥æï¼|ãé­æ³å°å¥³å¥å¶StrikerS THE COMICSãçç¹ä½ä¸­æç]": "ãé­æ³å°å¥³å¥å¶StrikerS THE COMICSã[ç¬¬ä¸å·åè¡æ¬æ¼«ç»ï¼æ¥æï¼|ç¹ä½ä¸­æç]",
        "[æ°æ[å¤ç±|æç©]ç[æ¢æ|æ¶é|æ´ç|åºçè§å]ç­å·¥ä½": "[æ°æ[å¤ç±|æç©]ç[æ¢æ|æ¶é|æ´ç|åºçè§å]ç­å·¥ä½]",
        "[å¤«å¦»ä¹é´ç¸æ¿¡ä»¥æ²«|[ç¶|æ¯]å¥³ä¹é´è¡èç¸è¢­ç[ç¹ç¹æ»´æ»´|ç»å¸ç»é¢]": "[å¤«å¦»ä¹é´ç¸æ¿¡ä»¥æ²«|[ç¶|æ¯]å¥³ä¹é´è¡èç¸è¢­ç[ç¹ç¹æ»´æ»´|ç»å¸ç»é¢]]",
        "[é´å¹³ã[é´ä¸|é´å»|é´å¥|é³å¹³|é³ä¸|é³å»|é³å¥]": "[é´å¹³|é´ä¸|é´å»|é´å¥|é³å¹³|é³ä¸|é³å»|é³å¥]",
        "[The Patriotic Front | PF]": "[The Patriotic Front|PF]",
        "[åå¬åºæ|ç»è¥åºå°]ç[äº§æè¯æ|ç§æ1å¹´ä»¥ä¸çç§èµåå|": "[åå¬åºæ|ç»è¥åºå°]ç[äº§æè¯æ|ç§æ1å¹´ä»¥ä¸çç§èµåå|åæ³çéªèµè¯æ]",
        "å¨è¾¹[6ä¸ªå¿ï¼å¸ï¼|55ä¸ªä¹¡é|600å¤ä¸ªå¤©ç¶æ]": "å¨è¾¹[6ä¸ªå¿ï¼å¸ï¼|55ä¸ªä¹¡é|500å¤ä¸ªå¤©ç¶æ]",
        "é£ç©ç[åæ°|äºå³|å½ç»|é´é³]å±æ§ç­|äººä½çççå¯åç¸å³ç[çè®º|ç»éª]]": "[é£ç©ç[åæ°|äºå³|å½ç»|é´é³]å±æ§|äººä½çççå¯åç¸å³ç[çè®º|ç»éª]]",
        "å[å·¥å§|ç®¡å§ä¼]ç[å°ç« ç®¡ç|æºè¦ä¿å¯]å·¥ä½]": "å[å·¥å§|ç®¡å§ä¼]ç[å°ç« ç®¡ç|æºè¦ä¿å¯]å·¥ä½",
        "[è¿åæ¼èºç|æåå¤«ç|è·å¾æ´å¤§çä¸ªäººåå±ï¼æä¸ºææçè³åå¤«å·¨æ|è·å¾æ´å¤§çä¸ªäººåå±|æä¸ºææçè³åå¤«å·¨æ]": "[è¿åæ¼èºç|æåå¤«ç|è·å¾æ´å¤§çä¸ªäººåå±|æä¸ºææçè³åå¤«å·¨æ]",
        "[ãç¬¬äºé¡¹ä¿®ç¼ãççè®º|ãç¬¬äºé¡¹ä¿®ç¼ãçå¯æä½æ§]": "ãç¬¬äºé¡¹ä¿®ç¼ãç[çè®º|å¯æä½æ§]",
        "[æºå³|äºä¸åä½äººå|ä¼ä¸ç®¡çäººå|ä¸ä¸ææ¯äººå]ç»è®¡|[æºå³|äºä¸]åä½å·¥èµç»è®¡|äººäºä¿¡æ¯ç®¡ç]å·¥ä½": "[[æºå³|äºä¸åä½äººå|ä¼ä¸ç®¡çäººå|ä¸ä¸ææ¯äººå]ç»è®¡|[æºå³|äºä¸]åä½å·¥èµç»è®¡|äººäºä¿¡æ¯ç®¡çå·¥ä½]",
    }
    place_fix_map = {
        "å¨èå·|[å·¥ä¸å­åº|é«æ°åº]": "å¨èå·[å·¥ä¸å­åº|é«æ°åº]",
    }
    qua_fix_map = {
        "å¨ä¸ä¸(ä¼è´¨è¯¾)æ¯èµä¸­": "å¨ä¸ä¸ï¼ä¼è´¨è¯¾ï¼æ¯èµä¸­",
        "å°±éåç¸å³èä¸(å·¥ç§)": "å°±éåç¸å³èä¸ï¼å·¥ç§ï¼",
        "å¨[å­¦ä¸ä½å­¦ææ³]ç­æ¹é¢": "å¨å­¦ä¸[|ä½å­¦ææ³]ç­æ¹é¢",
        "æ ¹æ®ãä¸­å±æ é³åºå§ãæ é³åºäººæ°æ¿åºå³äºå°åãæ é³åºäººæ°æ¿åºæºææ¹é©æ¹æ¡ãçéç¥ã(æ é³å§å[2010]14å·)ç²¾ç¥": "æ ¹æ®ãä¸­å±æ é³åºå§ãæ é³åºäººæ°æ¿åºå³äºå°åãæ é³åºäººæ°æ¿åºæºææ¹é©æ¹æ¡ãçéç¥ãï¼æ é³å§å[2010]14å·ï¼ç²¾ç¥",
        "æ[é¡¹ç®ææ¯æ°´å¹³çé«ä½|ç»æµæççå¤§å°|ç¤¾ä¼æççå¤§å°]": "æ[é¡¹ç®ææ¯æ°´å¹³çé«ä½|[ç»æµæç|ç¤¾ä¼æç]çå¤§å°]",
        "å¨[å®¶å­æ¸¸æ|å®¶å­æ¸¸æç»­ä½]ä¸­": "å¨å®¶å­æ¸¸æ[|ç»­ä½]ä¸­",
        "[å¨æ ¡é¢å¯¼çé«åº¦éè§|å¨ä½å¸ççå±ååªå]]": "å¨[æ ¡é¢å¯¼çé«åº¦éè§|å¨ä½å¸ççå±ååªå]ä¸",
        "å¨[é¶ä¸40åº¦é«æ¸©|è¶ä½æ¸©ç¯å¢]ä¸­|å¨[å¹²ç¥|æ½®æ¹¿|é£å°]ç­åä¸ªç¯å¢ä¸­": "[å¨[é¶ä¸40åº¦é«æ¸©|è¶ä½æ¸©ç¯å¢]ä¸­|å¨[å¹²ç¥|æ½®æ¹¿|é£å°]ç­åä¸ªç¯å¢ä¸­]",
        "ä»ç¾å½ç[æ°ä¸»|å®ªæ³]|ç¾å½ç¤¾ä¼çé®é¢|ç¾å½çç§»æ°åå²|ç¾å½äººççæ´»ä¹ æ¯]": "ä»[ç¾å½ç[æ°ä¸»|å®ªæ³]|ç¾å½ç¤¾ä¼çé®é¢|ç¾å½çç§»æ°åå²|ç¾å½äººççæ´»ä¹ æ¯]",
        "ä»¥[æå­|å½©å¾]]": "ä»¥[æå­|å½©å¾]",
        'ä¸è´¯ä¸è´¯': 'ä¸è´¯',
        "[": "_",
        "ä»¥æç§å­(2006)122å·æ": "ä»¥æç§å­ï¼2006ï¼122å·æ",
        "é¤äºç¼(æ§)éçä»¥åæäºç¹æ®ç¨éççåä»¥å¤": "é¤äºç¼ï¼æ§ï¼éçä»¥åæäºç¹æ®ç¨éççåä»¥å¤",
        "æºä¼ ç»(åä»£)ç»ç»ç ç©¶é¡¹ç®": "æºä¼ ç»ï¼åä»£ï¼ç»ç»ç ç©¶é¡¹ç®",
        "ä»¥()çåä¹": "ä»¥ï¼ï¼çåä¹",
        "å¨å¨å½äº¿åä¹¡(é)ç¤¾ä¼ç»æµåå±ç»éªäº¤æµä¼ä¸": "å¨å¨å½äº¿åä¹¡ï¼éï¼ç¤¾ä¼ç»æµåå±ç»éªäº¤æµä¼ä¸",
    }
    time_fix_map = {
        "å¨()å": "å¨ï¼ï¼å",
        "(12æ11æ¥)": "ï¼12æ11æ¥ï¼",
        "(12æ8æ¥)": "ï¼12æ8æ¥ï¼",
    }
    text_fix_map = {
        "AGM-114åå¦åå¯¼å¼¹å±åå±åºä¸¤ä»£ï¼ä¸ä»£äº982å¹´æäº§ã": "AGM-114åå¦åå¯¼å¼¹å±åå±åºä¸¤ä»£ï¼ä¸ä»£äº1982å¹´æäº§ã",
        "5æ21æ¥ï¼å§æçå¦»å­å¶èäºå¨ä¼æ¯æ¦å½å°å»é¢é¡ºå©äº§ä¸ä¸å¥³ã": "5æ21æ¥ï¼å§æçå¦»å­å¶èäºå¨ä¼æ¯é¡¿å½å°å»é¢é¡ºå©äº§ä¸ä¸å¥³ã",
        "ç§æè¾å¯¼åæ´»å¨è¾å¯¼ç»ç»å¹è®­ï¼ä½¿åæ ¡æå¸åçéæµã": "ç§æè¾å¯¼ååæ´»å¨è¾å¯¼ç»ç»å¹è®­ï¼ä½¿åæ ¡æå¸åçéæµã",
    }

    def clean_arg(txt):

        # txt = re.sub("\s*([\|\[\]])\s*", r"\1", txt)

        # rm blanks
        txt = txt.strip()
        txt = re.sub("([\u4e00-\u9fa5\|\[\]])\s+", r"\1", txt)
        txt = re.sub("\s+([\u4e00-\u9fa5\|\[\]])", r"\1", txt)

        # OR -> |
        txt = re.sub("([^a-zA-Z])OR([^a-zA-Z])", r"\1|\2", txt)

        # redundant characters
        txt = re.sub("\|+", "|", txt)
        txt = txt.strip("_")
        return txt

    for sample in data:
        text = sample["natural"]

        # if "å½éåç¾åå¹´" in sample["natural"] and "å½éåç¾æ¥" in sample["natural"]:
        #     print("!23")

        for spo in sample["logic"]:

            # fix subject
            if spo["subject"] in subj_fix_map:
                spo["subject"] = subj_fix_map[spo["subject"]]

            # fix predicate
            spo["predicate"] = spo["predicate"].strip()
            if spo["predicate"] in pred_fix_map:
                spo["predicate"] = pred_fix_map[spo["predicate"]]

            # fix object
            new_objs = []
            for obj in spo["object"]:
                if obj in obj_fix_map:
                    new_objs.append(obj_fix_map[obj])
                else:
                    new_objs.append(obj)
            spo["object"] = new_objs

            # fix place
            if spo["place"] in place_fix_map:
                spo["place"] = place_fix_map[spo["place"]]

            # fix time
            if spo["time"] in time_fix_map:
                spo["time"] = time_fix_map[spo["time"]]

            # fix qualifier
            if spo["qualifier"] in qua_fix_map:
                spo["qualifier"] = qua_fix_map[spo["qualifier"]]

            # fix text
            if text in text_fix_map:
                text = text_fix_map[text]
                sample["natural"] = text

            # special case
            if text == 'åå¦ç»£ç³å¤´ãèæ æ¢ç­ï¼çº¿ç²ï¼æéä¸å¿è¿äºååã' and spo["predicate"] == "æé" and spo["object"][0] == 'ä¸å¿è¿äºåå':
                spo["subject"] = "æé"
                spo["predicate"] = 'ä¸å¿è¿äºåå'

            if text == 'ä¿¡æ¯ç®¡çâæ½å·¥é¡¹ç®ç®¡çæ¯ä¸é¡¹å¤æçç°ä»£åçç®¡çæ´»å¨ï¼æ´è¦ä¾é å¤§éçä¿¡æ¯ä»¥åå¯¹å¤§éä¿¡æ¯çç®¡çï¼å¹¶åºç¨çµå­è®¡ç®æºè¿è¡è¾å©ã' and \
                    spo["object"][0] == 'å¤§éçä¿¡æ¯|å¯¹å¤§éä¿¡æ¯çç®¡ç':
                spo["subject"] = "æ½å·¥é¡¹ç®ç®¡ç"
                spo["object"][0] = '[å¤§éçä¿¡æ¯|å¯¹å¤§éä¿¡æ¯çç®¡ç]'

            if text == 'è¿ç§è§ç¹çå¤§å¤æ°æ¯å·æä½¿å½æçèµæ¬å®¶ï¼è¿æä¸»å¼ è¿è¡èªç±å¼æ°ä¸»æ¹é©çäººã' and spo["object"][0] == 'èµæ¬å®¶|':
                spo["object"][0] = '[èµæ¬å®¶|ä¸»å¼ è¿è¡èªç±å¼æ°ä¸»æ¹é©çäºº]'
            if text == 'è¿ç§è§ç¹çå¤§å¤æ°æ¯å·æä½¿å½æçèµæ¬å®¶ï¼è¿æä¸»å¼ è¿è¡èªç±å¼æ°ä¸»æ¹é©çäººã' and spo["predicate"] == 'ä¸»å¼ è¿è¡':
                spo["subject"] = 'äºº'

            if text == "åææèµåªéè¦ä¼ ç»åç¼©æºç©ºè°çä¸åï¼ä¸­æè¿è¡èçµéåªéè¦ä¼ ç»ç©ºè°ç[/b]1/8[/b]ââ[/b]1/10[/b]ï¼åæç»´æ¤è´¹ç¨ä½ã" and spo["predicate"] == "åªéè¦Xç[/b]1/8[/b]ââ[/b]1/10[/b]":
                text = "åææèµåªéè¦ä¼ ç»åç¼©æºç©ºè°çä¸åï¼ä¸­æè¿è¡èçµéåªéè¦ä¼ ç»ç©ºè°ç1/8ââ1/10ï¼åæç»´æ¤è´¹ç¨ä½ã"
                spo["predicate"] = "åªéè¦Xç1/8ââ1/10"

            if text == 'å¨çæ·ç±äººå£ä¸º70780918äººã' and spo["object"][0] == '79780918äºº':
                spo["object"][0] = '70780918äºº'

            if text == "å¶ä¸­ï¼60ã¡åä¸­å°æ·åæ´»è·ï¼å æ¯è¶è¿40%ï¼å¢å çº¦6%ï¼ç½æ¹ãçç°å æ¯è¶è¿5æï¼ç¦ç°å¨5æå·¦å³ã" and spo["time"] == "4æ":
                spo["time"] = "_"

            if text == "è¿æ­è½½äºMOTOBLURæå¡ï¼è¿æ¯ä¸ä¸ªèªæçäº¤äºèååºç¨ã" and spo["predicate"] == "MOTOBLURæå¡":
                spo["predicate"] = "æ­è½½äº"
                spo["object"] = ["MOTOBLURæå¡", ]
                spo["subject"] = "_"

            if text == "2017å¹´1æ3æ¥æé´ï¼ç±ä¸­å¤®çºªå§å®£ä¼ é¨ãä¸­å¤®çµè§å°èåå¶ä½ççµè§ä¸é¢çãæéè¿éèªèº«ç¡¬ãå°å¨ä¸­å¤®çµè§å°ç»¼åé¢éé¦æ­ã" and \
                    spo["subject"] == "ãæéè¿éèªèº«ç¡¬ãå°å¨Xé¦æ­" and spo["predicate"] == "ä¸­å¤®çµè§å°ç»¼åé¢é":
                spo["subject"] = "ãæéè¿éèªèº«ç¡¬ã"
                spo["predicate"] = "å°å¨Xé¦æ­"
                spo["object"][0] = "ä¸­å¤®çµè§å°ç»¼åé¢é"
                spo["time"] = "2017å¹´1æ3æ¥æé´"

            if text == "2ãæ¢äºæ´ï¼è¯äººæ¢åºä¹å­ï¼æ¢ç«æ¬çç¶äº²ï¼ä¹¾éå­å¹´ï¼1741ï¼ä¸¾äººï¼æ¬¡å¹´ä¸­è¿å£«ï¼æ­¥å¥ä»éï¼å®å¾å·ææã" \
                    and spo["time"] == "1742":
                spo["time"] = "1741"
            if text == "ä½æ³ï¼éç¨è±çä»ãç³ç²ãç½ç ç³åç²¾ç½é¢ç²æ··åçå¶èæã" and spo["subject"] == "éç¨Xæ··åçå¶èæ" \
                    and spo["predicate"] == "ç²¾ç½é¢ç²":
                spo["predicate"] = "éç¨Xæ··åçå¶èæ"
                spo["object"][0] = "[è±çä»|ç³ç²|ç½ç ç³|ç²¾ç½é¢ç²]"
                spo["subject"] = "_"

            if text == "1998å¹´å¸å§å®£ä¼ é¨ãå¸æèæäºâä¸é¨å³¡å¸åä½³æèºå·¥ä½èâç§°å·ï¼1999å¹´å¸æ¿åºæäºâä¸é¨å³¡å¸å³å¨æ¨¡èâç§°å·ã" and \
                    spo["subject"] == "[]" and spo["predicate"] == "_" and spo["qualifier"] == "_" and spo[
                "place"] == "_" \
                    and spo["time"] == "_" and spo["object"][0] == "_":
                spo["subject"] = "[å¸å§å®£ä¼ é¨|å¸æè]"
                spo["predicate"] = "æäºXç§°å·"
                spo["object"] = ["âä¸é¨å³¡å¸åä½³æèºå·¥ä½èâ", ]
                spo["time"] = "1998å¹´"
                sample["logic"].append({
                    "subject": "å¸æ¿åº",
                    "predicate": "æäºXç§°å·",
                    "object": ["âä¸é¨å³¡å¸å³å¨æ¨¡èâ", ],
                    "time": "1999å¹´",
                    "place": "_",
                    "qualifier": "_",
                })

            if spo["predicate"] == 'åºäºXå¯¹çå¯¹è±¡' and spo["object"][0] == "è¦ä¿æ¤ç":
                spo["predicate"] = 'åºäºXçå¯¹è±¡'
                spo["object"] = "è¦ä¿æ¤"

            if text == "è¿ç§æ¹æ³è½ç¶æ¯ä¼ä¸å¨è¥éç¯å¢ä¸­è¿è¡çï¼ä½åä¸æ¯çº¯èªç¶çï¼æ¯äººä»¬æ ¹æ®è°æ¥ç®çä¸»å¨å°ãæç®çå°æ½å ä¸äºå½±åï¼æä»¥ï¼è¿ç§æ¹æ³å¾å¾è½å¤æç§ç ç©¶ç®çåå¾æ¯è¾åç¡®ãææçèµæï¼æ¯åºç¨èå´æ¯è¾å¹¿æ³çæ¹æ³ã" and \
                    spo["subject"] == "å¨Xè¿è¡":
                spo["subject"] = "ä¼ä¸"

            if text == "è¥¿çº¢æ¿å¤æ±ï¼å¯ä»¥å©å°¿ï¼è¾ççäººä¹å®é£ç¨ã" and \
                    spo["predicate"] == "è¥¿çº¢æ¿" == spo["subject"]:
                spo["predicate"] = "å¤"
            if text == "å å¼ºåçº§æ®ç¾äººè¾å©å¨å·æå¡ä¸­å¿ï¼ç«ï¼å»ºè®¾ï¼æ¨è¿è¾å©å¨å·æå¡è¿ç¤¾åºãå°å®¶åº­ï¼ä¾åºéåè¾å©å¨å·9.2ä¸ä»¶ã" and \
                    spo["predicate"] == "è¾å©å¨å·æå¡" == spo["subject"]:
                spo["predicate"] = "å°"
            if text == "å¹¶ä¸ç±äºå¯¹å¼éå¾·å°çæ·±å¥äºè§£ï¼ä»æ¯è°é½æ´çå¾éã" and \
                    spo["predicate"] == "ä»" == spo["subject"]:
                spo["predicate"] = "æ¯è°é½æ´çå¾é"
            if text == "è¯¥æå°ä¹¡éè·¯ä¸ºåè·¯ï¼äº¤éä¸æ¹ä¾¿ï¼" and \
                    spo["predicate"] == "è¯¥æå°ä¹¡éè·¯" == spo["subject"]:
                spo["predicate"] = "ä¸º"
            if text == "èµµå·è¯´â¶ãå ä¸ºå®æä¸è¯å¨ã" and \
                    spo["predicate"] == "å®" == spo["subject"]:
                spo["predicate"] = "æXå¨"

            if text == "æ¬åï¼æªå¬å¼ï¼çåäºå¡æçæ¯ä¾ï¼æä¸èºäººåºæ¬é½ç¨èºåï¼" and \
                    spo["predicate"] == "é½ç¨" == spo["object"][0]:
                spo["object"][0] = "èºå"
            if text == "âè¯ä¿¡ä¸ºæ¬ï¼è´¨éä¸ºæ ¹âï¼åæ¬æä¸­ååå¤ä¸åçä¼ ç»ç¾å¾·ï¼" and \
                    spo["predicate"] == "ä¸º" == spo["object"][0]:
                spo["object"][0] = "æ ¹"
            if text == "å¨æ¥æ¬å´æ£æå±é¾çæ¶åï¼æ¯ä¾ç°å±¡æ¬¡æå«æ¥æ¬å´æ£çå°ä¸¥ï¼åææè¡¡å¤ç±æ£æï¼å¤æåæä¸­é©åå¢ã" and \
                    spo["predicate"] == "å±¡æ¬¡æå«" == spo["object"][0]:
                spo["object"][0] = "æ¥æ¬å´æ£çå°ä¸¥"
            if text == "å¨æ¥æ¬å´æ£æå±é¾çæ¶åï¼æ¯ä¾ç°å±¡æ¬¡æå«æ¥æ¬å´æ£çå°ä¸¥ï¼åææè¡¡å¤ç±æ£æï¼å¤æåæä¸­é©åå¢ã" and \
                    spo["predicate"] == "åå¸" == spo["object"][0]:
                spo["object"][0] = "_"
                spo["qualifier"] = "å¯é"
            if text == "å¨æ¥æ¬å´æ£æå±é¾çæ¶åï¼æ¯ä¾ç°å±¡æ¬¡æå«æ¥æ¬å´æ£çå°ä¸¥ï¼åææè¡¡å¤ç±æ£æï¼å¤æåæä¸­é©åå¢ã" and \
                    spo["predicate"] == "äººå£" == spo["object"][0]:
                spo["object"][0] = "_"
                spo["qualifier"] = "é¼ç"

            if text == "3ï¼æ­å±ä¸­ç¹å«å¼ºè°æ°æ¯çæ§å¶ï¼å¼ºè°ï¼è¿è´¯æ§ï¼åé³è²çä¼ç¾ï¼è¦æ±æ­å±ä¸­è¯­æ°å¯äºååï¼ææè¡¨è¾¾çæã" and \
                    spo["predicate"] == "è¦æ±X":
                spo["object"] = ["[è¯­æ°å¯äºåå|ææè¡¨è¾¾çæ]"]
            if text == "åç¦»åºç¨å±åé¢åå±æå©äºå¯¹é¢åæ¨¡åçæ½è±¡åä¸æ­ç²¾åï¼ä¹æå©å®æ½äººåå¿«éæå»ºæè°æ´äº§åï¼ä»¥æ»¡è¶³ä¼ä¸åå±ååçç®¡çéè¦ã" and \
                    spo["predicate"] == "æå©Xå¿«é[æå»º|è°æ´]Y":
                spo["object"] = ["å®æ½äººå", "äº§å"]
            if text == "æ±æ­¦å¸ç»æä¹ä¸å¹´ï¼å³å¬åå138å¹´ï¼âæ²³æ°´æº¢äºå¹³åï¼å¤§é¥¥ï¼äººç¸é£âçäºå®ï¼å·²åºç°äºå®æ¹çºªå½ã" and \
                    spo["predicate"] == "=å¬åå138å¹´":
                spo["predicate"] = "="
                spo["object"] = ["å¬åå138å¹´"]
            if text == "åå§å§åãå¯éé¿è°­æå°ï¼" and spo["subject"] == "è°­æå°" and spo["object"][0] == "[åå§å§å|å¯éé¿]":
                spo["predicate"] = "ISA"
            if text == "'èå¯¹äºåç¦æå¹³çäººæ¥è¯´ï¼æ´éè¦åçèèªæ¸¸ç§»çåçï¼å°èä¸ãåèãå°è¹çèèªéä¸­åè£¹å¨è¸é¨ï¼å°å¤§è¿æ ¹é¨ãå¤ä¾§çèèªä¸æåºå®å¨èé¨ï¼åé åºç²çæè´çèº«æã" and \
                    spo["predicate"] == "å°Xéä¸­Y":
                spo["predicate"] = "å°Xéä¸­åè£¹Y"
                spo["object"] = ['[èä¸|åè|å°è¹]çèèª', 'å¨è¸é¨']
            if text == 'å¿«æ¿èµ·æä¸­çæ­¦å¨åä½ çå°ä¼ä¼´åå æ å¤ºèµæºçææä¸­å§ï¼' and spo["predicate"] == "å¿«æ¿èµ·" == spo["object"][0]:
                spo["object"][0] = "æä¸­çæ­¦å¨"
            if text == "ä»¥æ±ææ¬èé¢åééä¸»å¹²éæ³°å·è·¯ä»¥åæ¯çº¿è¡éå»ºè®¾ä¸ºä¸»è¦åå®¹çç¬¬äºææ´å»ºé¡¹ç®ç¡®å®ï¼è®¾è®¡æ¹æ¡å·²ç»éè¿ä¸å®¶å®¡æ¥ï¼å·¥ç¨å¯æ11æä¸­æ¬å¼å·¥ã" \
                    and spo["subject"] == "ç¬¬äºææ´å»ºé¡¹ç®" == spo["object"][0]:
                spo["object"][0] = "_"
            if text == "ç±äºGPSææ¯æå·æçå¨å¤©åãé«ç²¾åº¦åèªå¨æµéçç¹ç¹ï¼ä½ä¸ºåè¿çæµéææ®µåæ°ççäº§åï¼å·²ç»èå¥äºå½æ°ç»æµå»ºè®¾ãå½é²å»ºè®¾åç¤¾ä¼åå±çåä¸ªåºç¨é¢åã" \
                    and spo["subject"] == "çäº§å" == spo["object"][0]:
                spo["object"][0] = "æ°ç"
            if text == "è¿ç§ææ³å½æ¶å¨ç©ççä¸ä½æ®éå­å¨ï¼èä¸ç±æ¥å·²ä¹ã" \
                    and spo["subject"] == "è¿ç§ææ³" == spo["object"][0]:
                spo["object"][0] = "[æ®éå­å¨|ç±æ¥å·²ä¹]"
            if text == "å ä¸ºç¼ºä¹å¹¼åï¼æä»¥å¨ä»é«åºå¥åçä¹å­é«éµç¬åçæå¯¼ä¸æ¯å¤©ç»åã" \
                    and spo["subject"] == "é«éµç¬" == spo["object"][0]:
                spo["object"][0] = "é«åºå¥åçä¹å­"
            if text == "å»ºç­å¤æ´éå¥ï¼æ¯æµæ±çå¾·æ¸å¿åä¿å­æå®æ´çå¤ä»£æ¡¥æ¢ï¼ç°ä¸ºççº§æç©ä¿æ¤åä½ã" \
                    and spo["subject"] == "å¾·æ¸å¿" == spo["object"][0]:
                spo["object"][0] = "æµæ±ç"
            if text == "ç°å¨ç¦»æå®¶å·²ç»ä¸è¿äºï¼ä½æ¯è¦å¨äºåéä¹åå°è¾¾ï¼å°±è¦æè¿è·¯ç©¿è¿ä¸ä¸ªå¤§åçåè½¦åºã" \
                    and spo["subject"] == "æå®¶" == spo["object"][0]:
                spo["subject"] = "_"
            if text == "è¥ç±äºå¡çæ¬èº«è´¨éé®é¢é æçæåï¼å·¥ä½äººåå¨ç¥¨é¢ä¸è¿è¡æ æ³¨ï¼ä¹å®¢å¨æææåç±è½¦ç«å·¥ä½äººåå¼å¯¼ä»ä¸ç¨ééè¿åºç«ã" \
                    and spo["subject"] == "è´¨éé®é¢" == spo["object"][0]:
                spo["object"][0] = "å¡çæ¬èº«"
            if text == "çæ°´å¹³ï¼ææçè¿äºçæå©äºèªç¶åå¨©çæ¯äº²åçäº§çå©´å¿ï¼å©´å¿çå¿è·³çå°ä¿æå¨ä¸ä¸ªæ­£å¸¸çèå´åï¼ç±äºå­å®«çéå½æ¶ç¼©åï¼åæ¶å ä¸è¯å¥½çæ°§æ°åè¡æ¶²ä¾åºç»åçæ¯äº²çè¯å¥½æè§ï¼å©´å¿ä¼æ¯è¾èéï¼åå¨©çæ¯ä¸é¶æ®µäº§éçæ©å¼ åæ¨è¿é½å¾é¡ºçï¼å©´å¿è½ä»¥è¯å¥½çä½ç½®æè½¬å¾ä¸è¿å¥äº§éèåºçï¼çäº§æ´å çèªå¨ï¼éè¿æ¾å¼çä¼é´ï¼èä½¿å¾æ¯äº²çèº«ä½ä¸ä¼äº§çç»ç»ãå¨å®ãèèä¸å¿è¦çæä¼¤ï¼ç±äºå¯ä»¥æè½¬ï¼ç¹å«æ¯ä¸ä¼ä¼¤åå©´å¿çå¤´é¨åèº«ä½ã" \
                    and spo["subject"] == "ä¼é´" == spo["object"][0]:
                spo["object"][0] = "æ¾å¼ç"
            if text == "proav éå¯¹ä¸ä¸å½±é³åºç¨ï¼åæ¬æ¼æ­å®¤è®¾å¤ãè§é¢å¼å³ãå½é³æ£ãå¤§åæå½±ä»ªãæ°å­ä¿¡æ¯åå¸ç³»ç»ãéæå¼å®¶åº­å½±é¢ãæè²åºæåç¤¼æå ç­ï¼adiå¬å¸æ¥æä¸çæé½å¨çicè§£å³æ¹æ¡ã" \
                    and spo["subject"] == "ä¸çæé½å¨ç" == spo["object"][0]:
                spo["subject"] = "icè§£å³æ¹æ¡"
            if text == "ï¼3ï¼ä¼ ç»èæ¥ï¼å¦è±æèï¼æ¸æèï¼ç«¯åèï¼ä¸­ç§èç­ï¼ååæç©¿æ±æåºæ¸¸ï¼æè£ç­ç±å¤èé¨è´è´£èç³»èµå©ï¼ï¼ä¸¾è¡ç¸åºçä¼ ç»ä»ªå¼ï¼å¦ç«¯åç¥­å±åï¼ä¸­ç§ææç­ï¼" \
                    and spo["subject"] == "å¤èé¨" == spo["object"][0]:
                spo["subject"] = "èç³»èµå©"
            if text == "2011å¹´ï¼è£è·âç¬¬äºå±ä¸­å½æºæ¢°å·¥ä¸ä¼ç§ä¼ä¸å®¶âè£èªç§°å·ï¼" \
                    and spo["subject"] == "âç¬¬äºå±ä¸­å½æºæ¢°å·¥ä¸ä¼ç§ä¼ä¸å®¶â" == spo["object"][0]:
                spo["object"][0] = "ç¬¬äºå±"
            if text == "æ è®ºæ¯ççº¯ç´ èãè¤èæè¤ç´ æ­éçèï¼å¯å°åå¤å¥½çä¸»æãéæåä½æå¨é¨ä¸æ¬¡æå¥ï¼å çå¹¶è®¾å®ç¨åºåï¼çèè¿ç¨èªå¨è¿è¡ï¼æ é¡»ä¸äººç¿»çæç§çï¼æ éç»éªï¼ç°å­¦ç°ä¼ã" \
                    and spo["subject"] == "çèè¿ç¨" == spo["object"][0]:
                spo["object"][0] = "_"
            if text == "å¶é ä¸å·¥äººå·¥èµçå¿«éä¸æ¶¨åææå³å¨åçéå¹´åå°ï¼æ­£éæ¸æ¹å30å¹´æ¥ä¸­å½ç»æµèµä»¥é«éå¢é¿çåºç¡ââå¤§éå»ä»·å³å¨åï¼äººå£çº¢å©éæ¸æ¶å¤±çé®é¢æ¯ä¸­å½ä¼ä¸ä¸å¾ä¸ç´è§çä¸ä¸ªä¸¥å³»é®é¢ã" \
                    and spo["subject"] == "äººå£çº¢å©éæ¸æ¶å¤±" == spo["object"][0]:
                spo["object"][0] = "ä¸¥å³»é®é¢"
            if text == "æ»èçå¤å±ï¼æ«éå½¢ï¼è¾¹ç¼æåºç¶ç¼æ¯ï¼å¤å±ç»¿è²ï¼è´¨ç¡¬èå¤å¼¯ï¼åå±ç´«çº¢è²ï¼å¼å±æç´ç«ï¼åç«¯å·å¾®æ¯ï¼" \
                    and spo["subject"] == "å¤å±" == spo["object"][0]:
                spo["object"][0] = "ç»¿è²"
            if text == "å¨2009å¹´1æ18æ¥çé¢é²å®«é¢çç¾çè¾¹åä¸äººæ®æ¥å¤§åå¬çæ´»å¨æ»ç»å¤§ä¼ä¸ï¼èªæ²»åºäººå¤§å¸¸å§ä¼å¯ä¸»ä»»æç§¦çæåºï¼âè¿æ¬¡æ®æ¥æ´»å¨å¨æ°çå»çå«çäºä¸ä¸æ¯å²æ åä¾çï¼å¨å¨å½ä¹èµ°å¨äºåé¢ã" \
                    and spo["subject"] == "æç§¦ç" == spo["object"][0]:
                spo["object"][0] = "èªæ²»åºäººå¤§å¸¸å§ä¼å¯ä¸»ä»»"
                spo["predicate"] = "ISA"
            if text == "æ ¹æ®çäººç¤¾åãè½¬åäººåèµæºç¤¾ä¼ä¿éé¨åå¬åå³äºå½å®¶åºæ¬å»çä¿é©ãå·¥ä¼¤ä¿é©åçè²ä¿é©è¯åç®å½ä¸­é¨åè¯åè¿è¡è°æ´è§èçéç¥ãï¼ç²¤äººç¤¾å½ã2013ã1252å·ï¼è¦æ±ï¼ç»åæå¸å®éå¯¹é¨åè¯åè¿è¡è°æ´è§èï¼æ¯å¦è¥¿è¯é¨åç¬¬1022å·âéç»äººçº¢ç»èçæç´ ï¼éç»äººä¿çº¢ç´ ï¼âä¿®æ¹ä¸ºâéç»äººçº¢ç»èçæç´ ï¼CHOç»èï¼âï¼è±æåç§°ä¿®æ¹ä¸ºâRecombinant Human Erythropoietinï¼CHO cellï¼âã" \
                    and spo["subject"] == "éç»äººçº¢ç»èçæç´ " == spo["object"][0]:
                spo["object"][0] = "éç»äººçº¢ç»èçæç´ ï¼CHOç»èï¼"
                spo["subject"] = "éç»äººçº¢ç»èçæç´ ï¼éç»äººä¿çº¢ç´ ï¼"
            if text == "ç¬¬ååæ¡è¥ä¸æ§è¿è¾è¹è¶åéè¥ä¸æ§è¿è¾è¹è¶ä¸´æ¶ä»äºè¥ä¸æ§è¿è¾ï¼æªæè§å®åçãè¥è¿è¯ãçï¼åãè¥è¿è¯ãè¢«åæ£åä»ç»§ç»­è¥è¿çï¼ç±å¸æ°´è·¯è¿è¾è¡æ¿ä¸»ç®¡é¨é¨æå¸æ°´è·¯è¿è¾è¡æ¿ä¸»ç®¡é¨é¨å§æçå¸æ°´è·¯è¿è¾ç®¡çå¤è´£ä»¤å¶åæ­¢è¥è¿ï¼å¹¶æè¿æ³æå¾å¤ä»¥2åç½æ¬¾ï¼ç½æ¬¾æé«éé¢ä¸å¾è¶è¿3ä¸åã" \
                    and spo["subject"] == "å¸æ°´è·¯è¿è¾è¡æ¿ä¸»ç®¡é¨é¨å§æç" == spo["object"][0]:
                spo["subject"] = "å¸æ°´è·¯è¿è¾ç®¡çå¤"
            if text == "å¼ æ©ç§åæéåè¿ä¸¤ä½ååå»èçä¼ä¸é«å±äººç©é½åFCPAæå³ï¼åºå«ä¹å¤ä»å¨äºï¼å¼ æ©ç§æ¯ä½ä¸ºå¯è½çåè´¿æ¹åå°å¤ç½ï¼èæéååæ¯ä½ä¸ºå¯è½çè¡è´¿æ¹åå°æ©ç½ã" \
                    and spo["subject"] == "ä¸¤ä½ååå»èçä¼ä¸é«å±äººç©" == spo["object"][0]:
                spo["subject"] = "[å¼ æ©ç§|æéå]"
                spo["object"][0] = "å»èçä¼ä¸é«å±äººç©"
            if text == "æäººè¯´ï¼âä¸­å½æ²¡æåå¤ï¼åå¤ä¸å¨ä¸­å½âï¼è½æ¯æ¤æ¨ä¹è¯­ï¼èå¯¹äºåå¤ææåçå°çä½æ°ä»¬ï¼åæä½è¨è¯­è¾©é©³å¢ï¼" \
                    and spo["subject"] == "ä½æ°ä»¬" == spo["object"][0]:
                spo["object"][0] = "åå¤ææåçå°ç"
            if text == "æ²³åæèºåºçç¤¾è¿æ¬¡è¿è¡æ°çå¼æï¼å¿å°ç»æ²³åçåºçäºä¸å¸¦æ¥æ°æï¼å¦æè¿ä½å¾å¥½ï¼ä¹ä¼å¸¦æ¥æåä¸ç»æµçåæçã" \
                    and spo["subject"] == "å¼æ" == spo["object"][0]:
                spo["object"][0] = "æ°ç"
            if text == "èªä»å»å¹´åå¸ä»¥æ¥ï¼è¾è®¯åºåçææ¸¸ãçèè£èãæ¯æå¹³åæ°å¢500ä¸æ¥æ´»è·ç¨æ·ï¼ç®åå·²ç»è¾¾å°5000ä¸ï¼ç­åº¦è¶è¿ä»»å¤©å çãå£è¢å¦æªGoãã" \
                    and spo["subject"] == "ãå£è¢å¦æªGoã" == spo["object"][0]:
                spo["subject"] = "ãçèè£èã"
            if text == "æ³¨ï¼å¶ä¸­ å±¿å¤´ââé»å²© 8å ï¼è½¦ç¨1å°æ¶å·¦å³ï¼ï¼å¯ä½¿ç¨å°å·å¬äº¤ICå¡ã" \
                    and spo["subject"] == "å±¿å¤´ââé»å²©" == spo["object"][0]:
                spo["subject"] = "å±¿å¤´"
                spo["object"][0] = "é»å²©"
                spo["predicate"] = "ââ"
            if text == "ä»¥èº«ç¯é©ç»ä¸´ç»é¡¶ï¼çºæè½è½èå±±ï¼å¹³ç´è¡åéï¼ç¹å¦ä¸å¹æ ç©·å°½çå¤©ç¶ç»å·ï¼å±ç¤ºäºè«è«å¤§å°ã" \
                    and spo["subject"] == "èå±±" == spo["object"][0]:
                spo["object"][0] = "è½è½"
            if text == "ä»¥èº«ç¯é©ç»ä¸´ç»é¡¶ï¼çºæè½è½èå±±ï¼å¹³ç´è¡åéï¼ç¹å¦ä¸å¹æ ç©·å°½çå¤©ç¶ç»å·ï¼å±ç¤ºäºè«è«å¤§å°ã" \
                    and spo["subject"] == "å°åº¦åºå­éåº" == spo["object"][0]:
                spo["object"][0] = "åä½©å°"
            if text == "ä½ä¸ºæ°åºçè¯äººï¼èè½¼å¼è¾äºè±ªæ¾è¯é£ï¼åæ°åºè¯äººè¾å¼ç¾å¹¶ç§°ä¸ºâèè¾âã" \
                    and spo["subject"] == "è¾å¼ç¾" == spo["object"][0]:
                spo["object"][0] = "è¯äºº"
            if text == "åå¼ æ©ç§ç¸åï¼å¨è¢«è¿«å»èä¹åï¼å°æ¹¾äººæéåä¹å¨èªå·±çèä¸çæ¶¯ä¸­è·å¾äºä¸ä¸ªä»¤äººè³ç¾¡çå°ä½ââæè®¯ä¸­å½åºæ»è£ã" \
                    and spo["subject"] == "æéå" == spo["object"][0]:
                spo["object"][0] = "[å°æ¹¾äºº|æè®¯ä¸­å½åºæ»è£]"
            if spo["subject"] == "æ¨èå±æ§æ¯ä¾" and spo["predicate"] == "ææ§=1ï¼4ï¼1ï¼7ï¼2":
                spo["predicate"] = "ï¼"
                spo["object"][0] = "èåãæ ¹éª¨ãèº«æ³ãçµæ§ãææ§=1ï¼4ï¼1ï¼7ï¼2"

        # rm blanks
        sample["natural"] = clean_arg(sample["natural"])

        for spo in sample["logic"]:
            # rm redundant blanks
            for k, v in spo.items():
                if k not in {"object", "objects"}:
                    spo[k] = clean_arg(spo[k])
                    # if len(re.findall("\[", spo[k])) != len(re.findall("\]", spo[k])):
                    #     print(spo)
                    #     print(text)
                    #     print(">>>>>>bad {}>>>>>>>".format(k))

                elif k == "object":
                    new_objs = []
                    for obj in spo[k]:
                        obj = clean_arg(obj)
                        # if len(re.findall("\[", obj)) != len(re.findall("\]", obj)):
                        #     print(spo)
                        #     print(text)
                        #     print(">>>>>>bad object>>>>>>>")

                        new_objs.append(obj)
                    spo[k] = new_objs

    def parse_spe_txt2list(spe_txt, jt=""):
        sep = "\u2E82"
        star = spe_txt.find("[")
        end = -1
        if star != -1:
            stack = []
            for idx in range(star, len(spe_txt)):
                c = spe_txt[idx]
                if c == "[":
                    stack.append(c)
                elif c == "]":
                    stack.pop()
                    if len(stack) == 0:
                        end = idx
                        break

        res = []
        if star != -1 and end != -1:
            pre = spe_txt[:star]
            mid = spe_txt[star + 1:end]
            post = spe_txt[end + 1:]

            mid_sub = mid[:]
            stack = []
            for idx, c in enumerate(mid):
                if c == "[":
                    stack.append(c)
                elif c == "]":
                    stack.pop()
                elif c == "|" and len(stack) == 0:
                    mid_sub = mid_sub[:idx] + sep + mid_sub[idx + 1:]

            mid_segs = mid_sub.split(sep)
            tmp = [jt.join([pre, seg, post]) for seg in mid_segs]
            for txt in tmp:
                res.extend(parse_spe_txt2list(txt))
        else:
            res.append(spe_txt)
        return res

    def get_spe_txt_spans(spe_txt, text, is_pred=False):
        # target_str = re.sub("[\]\[\|]", "", spe_txt)
        # if is_pred:
        #     target_str = re.sub("([^a-zA-Z]|^)[XYZU]([^a-zA-Z]|$)", r"\1\2", target_str)

        # if is_pred:
        #     segs = re.split("[XYZU]", spe_txt)
        # else:
        #     segs = re.split("[\]\[\|]", spe_txt)
        #
        # segs = [s.strip() for s in segs if s.strip() != ""]
        search_str = spe_txt  # "".join(segs)

        candidate_spans, _ = Preprocessor.search_char_spans_fr_txt(search_str, text, "ch")
        spans = candidate_spans[0]
        spans = [(spans[i], spans[i + 1]) for i in range(0, len(spans), 2)]

        preid2c = {}
        pat = "[\]\[\|XYZU]+" if is_pred else "[\]\[\|]+"
        for m in re.finditer(pat, spe_txt):
            if is_pred:
                if spe_txt[m.span()[0]] in set("XYZU") and m.span()[0] - 1 >= 0 and (
                        0 <= ord(spe_txt[m.span()[0] - 1]) - ord("A") <= 25 or 0 <= ord(spe_txt[m.span()[0] - 1]) - ord(
                    "a") <= 25) or \
                        spe_txt[m.span()[1] - 1] in set("XYZU") and m.span()[1] < len(spe_txt) and (
                        0 <= ord(spe_txt[m.span()[1]]) - ord("A") <= 25 or 0 <= ord(spe_txt[m.span()[1]]) - ord(
                    "a") <= 25):
                    continue
            preid2c[m.span()[0] - 1] = m.group()

        start = re.match("[\]\[\|XYZU]+", spe_txt) if is_pred else re.match("[\]\[\|]+", spe_txt)
        spans_str = start.group() if start is not None else ""
        offset = len(spans_str)

        for sp in spans:
            for sp_idx in range(*sp):
                spans_str += "({}, {})".format(sp_idx, sp_idx + 1)
                offset += 1
                if offset - 1 in preid2c:
                    spans_str += preid2c[offset - 1]
                    offset += len(preid2c[offset - 1])

        spans_str_list = []
        for sps_str in parse_spe_txt2list(spans_str):
            sps = [int(s) for s in re.findall("\d+", sps_str)]
            sps = merge_spans(sps)
            spans_str_list.append(sps)
        return spans_str_list

    predefined_p = dict()

    # trans predicate
    for sample in data:
        text = sample["natural"]
        for spo in sample["logic"]:
            predicate = spo["predicate"]
            # re.match("[A-Z=]+$", predicate) and predicate not in text
            if predicate != "_" and re.match("[A-Z=]+$", predicate) and predicate not in text:
                predefined_p[predicate] = predefined_p.get(predicate, 0) + 1

    # predefined_p_map = {"DESC": "æè¿°",
    #                     "ISA": "æ¯ä¸ç§",
    #                     "IN": "ä½äº",
    #                     "BIRTH": "çäº",
    #                     "DEATH": "æ­»äº",
    #                     "=": "ç­äº",
    #                     "NOT": "ä¸",
    #                     }

    predefined_p_set = {"DESC", "ISA", "IN", "BIRTH", "DEATH", "=", "NOT"}
    new_data = []
    bad_spo_list = []
    for sample_id, sample in tqdm(enumerate(data), desc="transform"):
        ori_sample = copy.deepcopy(sample)
        # sample["natural"] = sample["natural"] + "[SEP]" + "ï¼".join(predefined_p_map.values())
        text = sample["natural"]

        sample["logic"] = Preprocessor.unique_list(sample["logic"])
        new_spo_list = []
        for spo in sample["logic"]:
            # fix some wrong samples
            # if spo["object"][0] == "":
            #     print("!!!")

            if spo["predicate"] in predefined_p_set:
                if spo["object"][0] == "" and spo["predicate"] in {"DEATH", "BIRTH"}:
                    if spo["place"] != "":
                        spo["object"][0] = spo["place"]
                        spo["place"] = ""
                    elif spo["time"] != "":
                        spo["object"][0] = spo["time"]
                        spo["time"] = ""

                # # trans predicate
                # new_predicate = re.sub(k, predefined_p_map[k], spo["predicate"])
                # spo["predicate"] = new_predicate

            ori_spo = copy.deepcopy(spo)
            for key in spo:
                if spo[key] == "":
                    spo[key] = []
                elif key != "object" and key != "objects":
                    if re.search(".*\[.*\|.*\].*", spo[key]):  # need to split
                        ori_str = spo[key]
                        try:
                            split_list = parse_spe_txt2list(ori_str)
                            span_list = get_spe_txt_spans(ori_str, text, is_pred=True if key == "predicate" else False)
                        except Exception:
                            print(ori_str)
                            print(key)
                            print(text)
                            print(ori_spo)
                            print("==================error anns=======================")

                        # check spans
                        for idx, sp in enumerate(span_list):
                            extr_txt = Preprocessor.extract_ent_fr_txt_by_char_sp(sp, text, "ch")
                            try:
                                cor_str = re.sub("([^a-zA-Z]|^)[XYZU]([^a-zA-Z]|$)", r"\1\2", split_list[idx]) \
                                    if key == "predicate" else split_list[idx]
                                assert extr_txt == cor_str
                            except Exception:
                                print(text)
                                print(key)
                                print(ori_str)
                                print(extr_txt)
                                print(cor_str)
                                print("==================span search error==================")
                        comb_list = [{"char_span": [sp, ], "text": split_list[idx]} for idx, sp in enumerate(span_list)]
                        spo[key] = comb_list
                    else:
                        if key == "predicate" and spo[key] in predefined_p_set:
                            spo[key] = [
                                {"text": spo[key],
                                 "char_span": [[]],
                                 }, ]
                        else:
                            target_str = spo[key]
                            if key == "predicate":
                                spe_p_map = {
                                    "ä¸XçStarch RX1500ç¸å½": "ä¸çStarch RX1500ç¸å½",
                                    'å°Xè£å¥UNIXæå¡å¨': 'å°è£å¥UNIXæå¡å¨',
                                    'å°Xèå¥å°DKNYçè®¾è®¡å½ä¸­': 'å°èå¥å°DKNYçè®¾è®¡å½ä¸­',
                                    'ä¸Xåä½çäº§SK-1Z02Dæ­£åé²çç»¼åå½äºä»ª': 'ä¸åä½çäº§SK-1Z02Dæ­£åé²çç»¼åå½äºä»ª',
                                    'ç»å¸¸ä»¥Xæ¥ç§°å¼éå£å°çLYF': 'ç»å¸¸ä»¥æ¥ç§°å¼éå£å°çLYF',
                                }
                                if spo[key] in spe_p_map:
                                    target_str = spe_p_map[spo[key]]
                                else:
                                    target_str = re.sub("[XYZU]", "", spo[key])

                            try:
                                char_spans, _ = Preprocessor.search_char_spans_fr_txt(target_str, text, "ch")
                                spo[key] = [
                                    {"text": spo[key],
                                     "char_span": char_spans,
                                     }, ]
                                for ch_sp in char_spans:
                                    extr_txt = Preprocessor.extract_ent_fr_txt_by_char_sp(ch_sp, text, "ch")
                                    assert extr_txt == target_str
                            except Exception:
                                print(target_str)
                                print(key)
                                print(text)
                                print(ori_spo)
                                print("==================error anns=======================")

                elif key == "object":
                    new_objs = []
                    for obj in spo[key]:
                        if re.search(".*\[.*\|.*\].*", obj):  # need to split
                            ori_str = obj

                            try:
                                split_list = parse_spe_txt2list(obj)
                                span_list = get_spe_txt_spans(ori_str, text, is_pred=False)
                            except Exception:
                                print(ori_str)
                                print(key)
                                print(text)
                                print(ori_spo)
                                print("==================error anns=======================")

                            # check spans
                            for idx, sp in enumerate(span_list):
                                extr_txt = Preprocessor.extract_ent_fr_txt_by_char_sp(sp, text, "ch")
                                try:
                                    cor_str = split_list[idx]
                                    assert extr_txt == cor_str
                                except Exception:
                                    print(text)
                                    print(key)
                                    print(ori_str)
                                    print(extr_txt)
                                    print(cor_str)
                                    print("==================span search error==================")

                            comb_list = [{"char_span": [sp, ], "text": split_list[idx]} for idx, sp in
                                         enumerate(span_list)]
                            new_objs.append(comb_list)
                        else:
                            if obj == "":
                                pass
                            else:
                                try:
                                    char_spans, _ = Preprocessor.search_char_spans_fr_txt(obj, text, "ch")
                                except Exception:
                                    print(obj)
                                    print(key)
                                    print(text)
                                    print(ori_spo)
                                    print("==================error anns=======================")

                                new_objs.append([
                                    {"text": obj,
                                     "char_span": char_spans,
                                     }, ])
                    spo[key] = new_objs

            for p in spo["predicate"]:
                if re.search("[XYZU]", p["text"]) is None and len(spo["object"]) > 0 and p[
                    "text"] not in predefined_p_set:
                    p["text"] += "X"

            # align predicate and the corresponding subset of objects (by XYZU)
            ext_spo_list = []
            id_map = {"X": 0, "Y": 1, "Z": 2, "U": 3}
            spe_p_map = {
                "ä¸XçStarch RX1500ç¸å½": "ä¸[OBJ]çStarch RX1500ç¸å½",
                'å°Xè£å¥UNIXæå¡å¨': 'å°[OBJ]è£å¥UNIXæå¡å¨',
                'å°Xèå¥å°DKNYçè®¾è®¡å½ä¸­': 'å°[OBJ]èå¥å°DKNYçè®¾è®¡å½ä¸­',
                'ä¸Xåä½çäº§SK-1Z02Dæ­£åé²çç»¼åå½äºä»ª': 'ä¸[OBJ]åä½çäº§SK-1Z02Dæ­£åé²çç»¼åå½äºä»ª',
                'ç»å¸¸ä»¥Xæ¥ç§°å¼éå£å°çLYF': 'ç»å¸¸ä»¥[OBJ]æ¥ç§°å¼éå£å°çLYF',
            }

            bad_spo = False
            for p in spo["predicate"]:
                sub_objs = []
                if p["text"] in spe_p_map or p["text"] in predefined_p_set:
                    sub_objs.append(spo["object"][0])
                else:
                    for ph in re.findall("[XYZU]", p["text"]):
                        idx = id_map[ph]
                        try:
                            sub_objs.append(spo["object"][idx])
                        except Exception:
                            # print(spo)
                            # print(text)
                            # print(">>>>>>>>>>>>>>>>object list: out of index>>>>>>>>>>>>>>>>>>>>>>>>>")
                            bad_spo = True
                            bad_spo_list.append({
                                "text": text,
                                "bad_spo": ori_spo,
                                "ori_sample": ori_sample,
                            })
                            break

                if bad_spo:
                    break

                # XYZU -> [OBJ]
                if p["text"] in spe_p_map:
                    p["text"] = spe_p_map[p["text"]]
                else:
                    assert len(spo["object"]) <= 4

                    if len(spo["object"]) >= 1:
                        p["text"] = re.sub("X", "[OBJ]", p["text"])
                    if len(spo["object"]) >= 2:
                        p["text"] = re.sub("Y", "[OBJ]", p["text"])
                    if len(spo["object"]) >= 3:
                        p["text"] = re.sub("Z", "[OBJ]", p["text"])
                    if len(spo["object"]) == 4:
                        p["text"] = re.sub("U", "[OBJ]", p["text"])

                new_spo = copy.deepcopy(spo)
                new_spo["predicate"] = [p, ]
                new_spo["object"] = sub_objs
                ext_spo_list.append(new_spo)
            if len(spo["predicate"]) == 0:
                ext_spo_list.append(spo)

            # product
            open_spo_list = []
            for new_spo in ext_spo_list:
                lists4prod = []
                for k, l in new_spo.items():
                    if k in {"object", "objects"} or len(l) == 0:
                        continue
                    lists4prod.append([{"type": k, **i} for i in l])

                for objs in new_spo["object"]:
                    new_objs = []
                    for i in objs:
                        new_objs.append({"type": "object", **i})
                    lists4prod.append(new_objs)

                open_spo_list.extend([list(item) for item in itertools.product(*lists4prod)])

            # choose the best span from candidates
            filtered_open_spo_list = []
            for spo in open_spo_list:
                if any(len(arg["char_span"]) > 1 for arg in spo):
                    new_spo = []
                    for arg_idx_i, arg_i in enumerate(spo):
                        if len(arg_i["char_span"]) > 1:
                            fin_ch_sp = None
                            fin_dis_score = 9999
                            for ch_sp_i in arg_i["char_span"]:
                                dis_score = 0
                                for arg_idx_j, arg_j in enumerate(spo):
                                    if arg_idx_i == arg_idx_j:
                                        continue
                                    for ch_sp_j in arg_j["char_span"]:
                                        if len(ch_sp_j) == 0:
                                            continue
                                        dis_score += min(abs(ch_sp_i[0] - ch_sp_j[1]), abs(ch_sp_j[0] - ch_sp_i[1]))
                                if dis_score < fin_dis_score:
                                    fin_dis_score = dis_score
                                    fin_ch_sp = ch_sp_i

                            arg_cp = copy.deepcopy(arg_i)
                            arg_cp["char_span"] = fin_ch_sp
                            new_spo.append(arg_cp)
                        else:
                            arg_cp = copy.deepcopy(arg_i)
                            arg_cp["char_span"] = arg_cp["char_span"][0]
                            new_spo.append(arg_cp)
                else:
                    new_spo = [{**arg, "char_span": arg["char_span"][0]} for arg in spo]
                filtered_open_spo_list.append(new_spo)

            # clean
            # filter arg with blank span; strip blanks around entity
            fin_open_spo_list = []
            for spo in filtered_open_spo_list:
                if any((len(arg["char_span"]) == 0 and arg["text"] not in predefined_p_set)
                       or arg["text"].strip() == ""
                       for arg in spo):
                    # print(spo)
                    # print(text)
                    # print(">>>>>>>>>>>>>>>>>>>>invalid arg>>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
                    bad_spo_list.append({
                        "text": text,
                        "bad_spo": ori_spo,
                        "ori_sample": ori_sample,
                    })
                    continue

                # strip blanks
                for arg in spo:
                    if arg["text"] in predefined_p_set:
                        # print("predefined predicate")
                        continue
                    utils.strip_entity(arg)

                fin_open_spo_list.append(spo)

            new_spo_list.extend(fin_open_spo_list)
        new_sample = {
            "id": sample_id,
            "text": text,
            "open_spo_list": new_spo_list,
        }
        new_data.append(new_sample)
    return new_data, predefined_p, bad_spo_list


def trans_saoke():
    new_data, predefined_pred_set, bad_spo_list = preprocess_saoke()

    for sample in new_data:
        span_list = []
        for spo in sample["open_spo_list"]:
            for arg in spo:
                span_list.append(arg["char_span"])
        tok_res = ChineseWordTokenizer.tokenize_plus(sample["text"], span_list=span_list)
        sample["word_list"] = tok_res["word_list"]
        sample["word2char_span"] = tok_res["word2char_span"]

    train_data_rate = 0.8
    val_data_rate = 0.1
    train_num = int(len(new_data) * train_data_rate)
    valid_num = int(len(new_data) * val_data_rate)
    test_num = len(new_data) - train_num - valid_num
    random.shuffle(new_data)
    train_data = new_data[:train_num]
    valid_data = new_data[train_num:train_num + valid_num]
    test_data = new_data[-test_num:]

    return train_data, valid_data, test_data


def trans_casie():
    from glob import glob
    data = []
    for file_path in glob("../../data/ori_data/casie_bk/*.json"):
        sample = load_data(file_path)
        file_name = file_path.split("/")[-1]
        idx = re.search("\d+", file_name).group()
        sample["id"] = idx
        data.append(sample)

    new_data = []
    for sample in data:
        if "cyberevent" not in sample:
            continue
        text = sample["content"]
        new_sample = {
            "id": sample["id"],
            "text": text,
            "event_list": []
        }

        span2text = {}
        char_span_list = []
        for hp in sample["cyberevent"]["hopper"]:
            for event in hp["events"]:
                trigger = event["nugget"]["text"]
                trigger_char_span = [event["nugget"]["startOffset"], event["nugget"]["endOffset"]]
                span2text[str(trigger_char_span)] = trigger

                new_event = {
                    "trigger": trigger,
                    "trigger_char_span": trigger_char_span,
                    "realis": event["realis"],
                    "trigger_type": "{}.{}".format(event["type"], event["subtype"]),
                    "argument_list": [],
                }
                if "argument" in event:
                    for arg in event["argument"]:
                        arg_char_span = [arg["startOffset"], arg["endOffset"]]
                        arg_txt = arg["text"]
                        span2text[str(arg_char_span)] = arg_txt

                        new_event["argument_list"].append({
                            "text": arg_txt,
                            "char_span": arg_char_span,
                            "type": arg["role"]["type"],
                        })
                new_sample["event_list"].append(new_event)
        # check spans
        span2text_ = dict(sorted(span2text.items(), key=lambda x: int(re.search("\d+", x[0]).group())))
        for sp_str, txt in span2text_.items():
            sp_se = re.search("\[(\d+), (\d+)\]", sp_str)
            sp = [int(sp_se.group(1)), int(sp_se.group(2))]
            char_span_list.append(sp)

            extr_txt = Preprocessor.extract_ent_fr_txt_by_char_sp(sp, text)
            try:
                assert extr_txt == txt
            except Exception:
                print(sample["sourcefile"])
                print(extr_txt)
                print(txt)
                print("===========================")

        # word2char_span and word list
        tok_res = WhiteWordTokenizer.tokenize_plus(text, span_list=char_span_list)
        new_sample["word_list"] = tok_res["word_list"]
        new_sample["word2char_span"] = tok_res["word2char_span"]
        new_data.append(new_sample)

    random.shuffle(new_data)
    test_data = new_data[-100:]
    save_as_json_lines(test_data, "../../data/ori_data/casie/test_data.json")

    valid_num = int(900 / 8) - 1
    for start_idx in range(0, 900, valid_num):
        end_idx = start_idx + valid_num
        if end_idx > 900:
            break
        valid_data = new_data[start_idx:end_idx]
        train_data = new_data[:start_idx] + new_data[end_idx:-100]
        save_as_json_lines(train_data, "../../data/ori_data/casie/train_data_{}.json".format(start_idx // valid_num))
        save_as_json_lines(valid_data, "../../data/ori_data/casie/valid_data_{}.json".format(start_idx // valid_num))
        print("{}, {}".format(len(train_data), len(valid_data)))


def trans_chfin():
    data_dir = "../../data/ori_data/chfinann_bk"
    save_dir = "../../data/ori_data/chfinann"
    train_data = load_data(os.path.join(data_dir, "train.json"))
    val_data = load_data(os.path.join(data_dir, "dev.json"))
    test_data = load_data(os.path.join(data_dir, "test.json"))

    def trans_chfinann(data):
        new_data = []
        for sample in data:
            text = " ".join(sample[1]["sentences"])

            # mention 2 spans
            all_char_span_list = []
            offset = [0, ]
            for sent in sample[1]["sentences"]:
                offset.append(offset[-1] + len(sent) + 1)
            mention2spans = {m: [[offset[sp[0]] + sp[1], offset[sp[0]] + sp[2]] for sp in sent_spans] for m, sent_spans
                             in sample[1]["ann_mspan2dranges"].items()}
            for m, char_spans in mention2spans.items():
                all_char_span_list.extend(char_spans)
                for sp in char_spans:
                    assert m == text[sp[0]:sp[1]]

            event_list = []
            for event in sample[1]["recguid_eventname_eventdict_list"]:
                event_type = event[1]
                arg_list = []
                for arg_type, mention in event[2].items():
                    if mention is None:
                        continue
                    for m_span in mention2spans[mention]:
                        arg_list.append({
                            "text": mention,
                            "char_span": m_span,
                            "type": arg_type,
                        })
                event_list.append({
                    "event_type": event_type,
                    "argument_list": arg_list,
                })

            tok_res = ChineseWordTokenizer.tokenize_plus(text, span_list=all_char_span_list)
            new_sample = {
                "id": sample[0],
                "text": text,
                **tok_res,
                "event_list": event_list,
            }
            new_data.append(new_sample)
        return new_data

    new_train_data = trans_chfinann(train_data)
    new_valid_data = trans_chfinann(val_data)
    new_test_data = trans_chfinann(test_data)

    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    save_as_json_lines(new_train_data, os.path.join(save_dir, "train_data.json"))
    save_as_json_lines(new_valid_data, os.path.join(save_dir, "valid_data.json"))
    save_as_json_lines(new_test_data, os.path.join(save_dir, "test_data.json"))


def preprocess_duie():
    data_dir = "../../data/ori_data/duie_comp2021_bk"
    save_dir = "../../data/ori_data/duie_comp2021"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    train_data = load_data(os.path.join(data_dir, "train_data.json"))
    valid_data = load_data(os.path.join(data_dir, "valid_data.json"))
    test_data = load_data(os.path.join(data_dir, "test_data_1.json"))

    # fix data
    for sample in tqdm(train_data + valid_data):
        text = sample["text"]

        if text == "2  æ±ç¾é³21967åºçå¨æ±è¥¿åæï¼1989å¹´æ¯ä¸äºæ±è¥¿ç§æå¸èå¤§å­¦å¤è¯­ç³»è±è¯­ä¸ä¸ï¼åæ¥åéè³é¹°æ½­éè·¯ä¸ä¸­ä»»è±è¯­æå¸":
            sample["text"] = "2  æ±ç¾é³ 1967åºçå¨æ±è¥¿åæï¼1989å¹´æ¯ä¸äºæ±è¥¿ç§æå¸èå¤§å­¦å¤è¯­ç³»è±è¯­ä¸ä¸ï¼åæ¥åéè³é¹°æ½­éè·¯ä¸ä¸­ä»»è±è¯­æå¸"
            # sample["postag"] = [{"word": w, } for w in jieba.cut(sample["text"])]
        if text == "äººç©ç®ä»çç2ï¼å¥³ï¼19441ï¼1968å¹´æ¯ä¸äºåäº¬å¤§å­¦ç©çç³»":
            sample["text"] = "äººç©ç®ä»çç2ï¼å¥³ï¼1944ï¼1968å¹´æ¯ä¸äºåäº¬å¤§å­¦ç©çç³»"
            # sample["postag"] = [{"word": w, } for w in jieba.cut(sample["text"])]
        if text == "å½±çä¿¡æ¯çµè§å§å½±çåç§°ï¼èå¨èå å¥ç¬¬äºå­£  å½±çç±»åï¼æ¬§ç¾å§  å½±çè¯­è¨ï¼è±è¯­  ä¸æ å¹´ä»½ï¼20121æ¼åè¡¨å§æä»ç»ç¾å½èå å¥ï¼åäº²å¥³å­©CeCeï¼Bella Thorneé¥°ï¼åéºèRockyï¼Zendaya Colemané¥°ï¼åæ¬åªæ¯ä¸¤ä¸ªç±è·³èçæ®éåä¸­ç":
            sample[
                "text"] = "å½±çä¿¡æ¯çµè§å§å½±çåç§°ï¼èå¨èå å¥ç¬¬äºå­£  å½±çç±»åï¼æ¬§ç¾å§  å½±çè¯­è¨ï¼è±è¯­  ä¸æ å¹´ä»½ï¼2012 æ¼åè¡¨å§æä»ç»ç¾å½èå å¥ï¼åäº²å¥³å­©CeCeï¼Bella Thorneé¥°ï¼åéºèRockyï¼Zendaya Colemané¥°ï¼åæ¬åªæ¯ä¸¤ä¸ªç±è·³èçæ®éåä¸­ç"
            # sample["postag"] = [{"word": w, } for w in jieba.cut(sample["text"])]
        if text == "http://news.sohu.com/20081221/n261333381.shtml 12æ20æ¥ï¼è¥¿åæ¿æ³å¤§å­¦å¨ç©ä¿æ¤æ³ç ç©¶ä¸­å¿æçæç« åæ¶ï¼ç±è¥¿åæ¿æ³å¤§å­¦åä¸­å½ç¤¾ä¼ç§å­¦é¢æ³å­¦ç ç©¶æå±åä¸»åçâä¸­å½ãå¨ç©ä¿æ¤æ³ãç ç©¶é¡¹ç®âæ­£å¼å¯å¨ ããå¾ä¸ºè¥¿åæ¿æ³å¤§å­¦å¨ç©ä¿æ¤æ³ç ç©¶ä¸­å¿ä¸»ä»»å­æ±(å·¦ä¸)ä»è¥¿åæ¿æ³å¤§å­¦æ ¡é¿è´¾å®ææ(å·¦äº)æä¸­æ¥è¿âè¥¿åæ¿æ³å¤§å­¦å¨ç©ä¿æ¤ç ç©¶ä¸­å¿âççå¾":
            sample["text"] = "2008ï¼è¥¿åæ¿æ³å¤§å­¦å¨ç©ä¿æ¤æ³ç ç©¶ä¸­å¿æçæç« åæ¶ï¼ç±è¥¿åæ¿æ³å¤§å­¦åä¸­å½ç¤¾ä¼ç§å­¦é¢æ³å­¦ç ç©¶æå±åä¸»åçâä¸­å½ãå¨ç©ä¿æ¤æ³ãç ç©¶é¡¹ç®âæ­£å¼å¯å¨ ããå¾ä¸ºè¥¿åæ¿æ³å¤§å­¦å¨ç©ä¿æ¤æ³ç ç©¶ä¸­å¿ä¸»ä»»å­æ±(å·¦ä¸)ä»è¥¿åæ¿æ³å¤§å­¦æ ¡é¿è´¾å®ææ(å·¦äº)æä¸­æ¥è¿âè¥¿åæ¿æ³å¤§å­¦å¨ç©ä¿æ¤ç ç©¶ä¸­å¿âççå¾"
        if text in {"äºå«æ¶ï¼1976å¹´åºçäºæ²³åçéè®¸å¿ï¼2013å¹´æºæåæ´ååæ²³åæ¬£èµç½ç»ç§æéå¢ï¼ä»»è¯¥éå¢è£äºé¿ï¼ä¸æ³¨äºæå½å¤§ä¸­å°åä¼ä¸æä¾ä¸ä¸çç½ç»æå¡ï¼å¸¦å¨å¾å¤ä¼ä¸ç½ç»æ¹åçè½¬å",
                    "1962å¹´å¨æçå¨ä¸­å½æ¤ç©ä¿æ¤å­¦ä¼æç«å¤§ä¼ä¸ï¼ä½äºãæå½å®³è«åä¸é²æ²»ç ç©¶ç°ç¶åå±æãçå­¦æ¯æ¥åï¼1963å¹´å¨ãäººæ°æ¥æ¥ãä¸åè¡¨ãç»åèä½é²æ²»å®³è«ãä¸æ"}:
            new_spo_list = [spo for spo in sample["spo_list"] if spo["predicate"] != "æå±ä¸è¾"]
            sample["spo_list"] = new_spo_list

        # if text in {
        #     "è¯¥çè·1988å¹´ç¬¬å«å±ä¸­å½çµå½±éé¸¡å¥æä½³å¥³ä¸»è§å¥ï¼æ½è¹ï¼",
        #             }:
        #     print("2 fix")

        for spo in sample["spo_list"]:
            if text == 'æ¯å¦å¼ èºè°çä¸¤å±å¨å°¼æ¯å½éçµå½±èéç®å¥ ï¼ç¬¬38å±ææå½éçµå½±èéçå¥ ï¼ä¸¤å±è±å½çµå½±å­¦é¢å¥æä½³å¤è¯­ç ï¼ç¬¬55å±å°æ¹¾çµå½±éé©¬å¥æä½³å¯¼æ¼å¥ ï¼ç¬¬05å±ä¸­å½çµå½±åè¡¨å¥æä½³å¯¼æ¼å¥ ï¼2008å½±åä¸çåäººå¤§å¥' and \
                spo["object"]["@value"] == 'ä¸­å½çµå½±åè¡¨å¥æä½³å¯¼æ¼å¥' and spo["object"]["period"] == "5":
                spo["object"]["period"] = "05"
            if text == 'éæè¯ï¼æ¾è·å¾ç¬¬ä¸å±è±å½ä¸åå½éåè¯­çµå½±èä¼ç§ç·éè§å¥ï¼ç¬¬21å±åäº¬å¤§å­¦ççµå½±èæä½³å¯¼æ¼å¤å¥³ä½å¥ï¼ç¬¬23å±åäº¬å¤§å­¦ççµå½±èæä½³ç¼å§å¥ç­å¥é¡¹ï¼åä¸¤å¹´èåä½ä¸½å¨ä¸äºä¹æ¯é¹å¾äººå°½çç¥' \
                    and spo["object"]["@value"] == 'è±å½ä¸åå½éåè¯­çµå½±èä¼ç§ç·éè§å¥' and spo["object"]["period"] == "3":
                spo["object"]["period"] = "ä¸"
            if text == '1982å¹´ï¼è®¸å æå­ãæ©ç»ä¿éãï¼åå¤ºç¬¬ä¸å±é¦æ¸¯çµå½±éåå¥æä½³ç·ä¸»è§å¥' \
                    and spo["object"]["@value"] == 'é¦æ¸¯çµå½±éåå¥æä½³ç·ä¸»è§å¥' and spo["object"]["period"] == "1":
                spo["object"]["period"] = "ä¸"
            if text == 'äºæ´²çµå½±å¤§å¥ç»èº«æå°±å¥:ç¬¬02å±ï¼2008å¹´:å±±ç°æ´æ¬¡ç¬¬04å±ï¼2010å¹´:é¿ç±³è¾¾å·´å½»ç¬¬05å±ï¼2011å¹´:é¹ææç¬¬06å±ï¼2012å¹´:è®¸éåç¬¬08å±ï¼2014å¹´:ä¾¯å­è´¤ç¬¬09å±ï¼2015å¹´:æææ³½ç¬¬10å±ï¼2016å¹´:æ æ¨å¸æ&è¢åå¹³ç¬¬11å±ï¼2017å¹´:å¾åç¬¬12å±ï¼2018å¹´:å¼ è¾åç¬¬13å±ï¼2019å¹´:ææ²§ä¸' \
                    and spo["object"]["@value"] == 'äºæ´²çµå½±å¤§å¥ç»èº«æå°±å¥' and "period" in spo["object"] and spo["object"]["period"] == "9":
                spo["object"]["period"] = "09"
            if text == 'äºæ´²çµå½±å¤§å¥ç»èº«æå°±å¥:ç¬¬02å±ï¼2008å¹´:å±±ç°æ´æ¬¡ç¬¬04å±ï¼2010å¹´:é¿ç±³è¾¾å·´å½»ç¬¬05å±ï¼2011å¹´:é¹ææç¬¬06å±ï¼2012å¹´:è®¸éåç¬¬08å±ï¼2014å¹´:ä¾¯å­è´¤ç¬¬09å±ï¼2015å¹´:æææ³½ç¬¬10å±ï¼2016å¹´:æ æ¨å¸æ&è¢åå¹³ç¬¬11å±ï¼2017å¹´:å¾åç¬¬12å±ï¼2018å¹´:å¼ è¾åç¬¬13å±ï¼2019å¹´:ææ²§ä¸' \
                    and spo["object"]["@value"] == 'äºæ´²çµå½±å¤§å¥ç»èº«æå°±å¥' and "period" in spo["object"] and spo["object"]["period"] == "6":
                spo["object"]["period"] = "06"
            if text == 'ã007ãããè°å½±ééãä½ä¸ºæ¾ç»çç¹å·¥çï¼ççäºæ´ä¸ªå¥½è±åï¼åèè³ä»æ¶è·çº¦70äº¿ç¾åï¼åèè³ä»æ¶è·çº¦11äº¿ç¾åï¼å¨æ¶è·ä¸è²ç¥¨æ¿çåæ¶ï¼ä¹èµ¢å°½äºå£ç¢' \
                    and spo["object"]["@value"] == '70äº¿ç¾å' and spo["subject"] == "7":
                spo["subject"] = "007"
            if text == '1987å¹´ï¼ç±ä½å®¶å¼ å¼¦ç¼å§ãæäºæå¯¼æ¼ççµå½±ãäºãå®æï¼æ½è¹å å¨è¯¥çä¸­æ®æ¼å¾ä¸½èèè·å¾ç¬¬å«å±ä¸­å½çµå½±éé¸¡å¥æä½³å¥³ä¸»è§å¥ãç¬¬18å±æå¤§å©é¶å°ç±³çº³å½éçµå½±èæä½³å¥³ä¸»è§å¥' \
                    and spo["object"]["@value"] == 'ä¸­å½çµå½±éé¸¡å¥æä½³å¥³ä¸»è§' and spo["object"]["period"] == "8":
                spo["object"]["period"] = "å«"
            if text == 'è¯¥çè·1988å¹´ç¬¬å«å±ä¸­å½çµå½±éé¸¡å¥æä½³å¥³ä¸»è§å¥ï¼æ½è¹ï¼' \
                    and spo["object"]["@value"] == 'ä¸­å½çµå½±éé¸¡å¥æä½³å¥³ä¸»è§' and spo["object"]["period"] == "8":
                spo["object"]["period"] = "å«"
            if text == "åºçäº1984å¹´9æ26æ¥ï¼æ²³åéå·ï¼å­¦åï¼æ¬ç§ï¼ç¹é¿ï¼ä¸»æãæè¯µãè¡¨æ¼ãå½ç»ãå¹³é¢æ¨¡ç¹ãç«¥å£°æ¨¡ä»¿ æ¯ä¸é¢æ ¡/ä¸ä¸ï¼ä¸­å½ä¼ åªå¤§å­¦/æ­é³ä¸ä¸»æèºæ¯ä¸ä¸ï¼ç®¡æåï¼ä¸­å¤®çµè§å°ä½è²é¢éï¼CCTV5ï¼ä½è²æ¨æ¥ãå¤©æ°ä½è²ãä¸»æäººï¼åæ¶ï¼è¿ä¸»æä¸­å¤®çµè§å°ç»æµé¢éï¼CCTV2ï¼ãç¬¬ä¸å°è±¡ããä¸­å¤®çµè§å°åä¸é¢éãåä¸æ°è±¡ãåä¸­å½æ°è±¡é¢éãå¤©æ°ç´æ­é´ãç­æ ç®" \
                and spo["object"]["@value"] == "" and spo["predicate"] == "æ¯ä¸é¢æ ¡":
                spo["object"]["@value"] = "ä¸­å½ä¼ åªå¤§å­¦"
            if text == "ãç½è²æ¢¦å¹»ãæ¯ä¸é¨äº1998å¹´1æ1æ¥åºåççµè§å§ï¼ç±å¤ªçº²å¯¼æ¼ï¼ç±ç°å²·ãè®¸äºåãçä¸½ä¸½ åä½æ´ä¸»æ¼ï¼ä¸å±æ20éï¼æ¯é48åé" \
                and spo["object"]["@value"] == "" and spo["predicate"] == "ä¸æ æ¶é´":
                spo["object"]["@value"] = "1998å¹´1æ1æ¥"
                spo["subject"] = "ç½è²æ¢¦å¹»"
            if text == "ãéªç¹è¡å¨ç¬¬äºå­£ãæ¯ä¸é¨ç±ç¼å§Stephanie Morgensternç¼åçä¸é¨å¨ä½å§æçµè§å§ï¼åºåæ¶é´ä¸º2012å¹´09æ20æ¥" \
                and spo["object"]["@value"] == "" and spo["predicate"] == "ä¸æ æ¶é´":
                spo["object"]["@value"] = "2012å¹´09æ20æ¥"
                spo["subject"] = "éªç¹è¡å¨ç¬¬äºå­£"

            if text == "ç¬¬å­å åæ¦ 1982.1.25 æ±èæ·®å® ââ2001å¹´10æ é¦å±ç¾äºå¨å½æ°æå¤§èµæ±èå°åºéæèµä¸ç­å¥ æä½³æ¿æå¥ ææ¥ä¹æç§°å·ï¼2002å¹´6æ ä¸­é©æ°æéç§å¤§èµç¬å±ç»ç¹ç­å¥ï¼2004å¹´6æ ä¸­å¤®çµè§å°ãéå¸¸6+1ãï¼ æ¹åå«è§è¶çº§å¥³çæé½èµåºå10 ï¼ 2011å¹´ ç¬¬ä¸å±åäººæåå¤§éç¬¬ä¸å ç¨çµé­å±æ­çæ­æï¼å±çæ æ°å¬ä¼æµæ³ªï¼ä¸æ§âå°åæ¬¢âï¼å æ­¤æåæå¸å¤§åæ¬¢ 9 S" \
                and spo["predicate"] == "è·å¥" and "period" in spo["object"] and spo["object"]["period"] == "":
                spo["object"]["period"] = "é¦"
            if text == "2010å¹´è·Music Radioä¸­å½Topæè¡æ¦åå°æä½³åä½æ­æå¥ï¼ç¬¬å«å±ä¸åå²çé³ä¹æ¦é¢å¥å¸ç¤¼å²çåå°æä½³å±ä½æ­æå¥ãå²çæä½³ä½æ²äººå¥æå¥ä»¥éåºä¸ºèµ·ç¹ï¼å¼å¯2018-2020âä¸æ­¢ æ¯æå¥âä¸çå·¡åæ¼å±ä¼" \
                and spo["predicate"] == "è·å¥" and "period" in spo["object"] and spo["object"]["period"] == "":
                spo["object"]["period"] = "å«"

            if spo["predicate"] in {"ä¸ä¸ä»£ç ", "é®æ¿ç¼ç "} and text[
                re.search(spo["object"]["@value"], text).span()[0] - 1] == "0":
                spo["object"]["@value"] = "0" + spo["object"]["@value"]

            # strip redundant whitespaces and unknown characters
            spo["subject"] = clean_entity(spo["subject"])
            for k, item in spo["object"].items():
                spo["object"][k] = clean_entity(item)

        new_spo_list = []
        for spo in sample["spo_list"]:
            if spo["subject"].lower() not in text.lower() or spo["object"]["@value"].lower() not in text.lower():
                # drop wrong spo
                continue

            if spo["subject"] not in text:  # if not in, try recover upper case
                m = re.search(re.escape(spo["subject"].lower()), text.lower())
                # print("{}----{}".format(spo["subject"], text[m.span()[0]:m.span()[1]]))
                spo["subject"] = text[m.span()[0]:m.span()[1]]

            if spo["object"]["@value"] not in text:
                m = re.search(re.escape(spo["object"]["@value"].lower()), text.lower())
                # print("{}----{}".format(spo["object"], text[m.span()[0]:m.span()[1]]))
                spo["object"]["@value"] = text[m.span()[0]:m.span()[1]]
            new_spo_list.append(spo)

        filtered_spo_list = []
        for spo in new_spo_list:
            assert spo["subject"] in text

            if spo["subject"].strip() == "":
                continue

            bad_spo = False
            for item in spo["object"].values():
                assert item in text
                if item.strip() == "":
                    bad_spo = True
                    break

            if not bad_spo:
                filtered_spo_list.append(spo)

        sample["spo_list"] = filtered_spo_list

    train_data_path = os.path.join(save_dir, "train_data.json")
    valid_data_path = os.path.join(save_dir, "valid_data.json")
    test_data_path = os.path.join(save_dir, "test_data_1.json")
    save_as_json_lines(train_data, train_data_path)
    save_as_json_lines(valid_data, valid_data_path)
    save_as_json_lines(test_data, test_data_path)


if __name__ == "__main__":
    # preprocess_duie()

    train_data, valid_data, test_data = trans_saoke()
    save_dir = "../../data/ori_data/saoke"

    train_data_path = os.path.join(save_dir, "train_data.json")
    valid_data_path = os.path.join(save_dir, "valid_data.json")
    test_data_path = os.path.join(save_dir, "test_data.json")
    save_as_json_lines(train_data, train_data_path)
    save_as_json_lines(valid_data, valid_data_path)
    save_as_json_lines(test_data, test_data_path)


