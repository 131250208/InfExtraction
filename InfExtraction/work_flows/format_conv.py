import json
import os
from InfExtraction.modules.preprocess import Preprocessor, WhiteWordTokenizer, ChineseWordTokenizer
from InfExtraction.modules.utils import load_data, save_as_json_lines, merge_spans
from tqdm import tqdm
import random
from tqdm import tqdm
from pprint import pprint
import copy
import re
import jieba
import string
from pattern.en import lexeme, lemma
import itertools
import matplotlib.pyplot as plt


def trans_genia():
    data_in_dir = "../../data/ori_data/genia_bk"
    data_out_dir = "../../data/ori_data/genia"
    in_file2out_file = {
        "train_dev.genia.jsonlines": "train_data.json",
        "test.genia.jsonlines": "test_data.json",
    }

    # load data
    filename2data = {}
    for in_filename, out_filename in in_file2out_file.items():
        int_path = os.path.join(data_in_dir, in_filename)
        out_path = os.path.join(data_out_dir, out_filename)
        with open(int_path, "r", encoding="utf-8") as file_in:
            data = [json.loads(line) for line in file_in]
            out_data = []
            for batch in tqdm(data, desc="transforming"):
                ners = batch["ners"]
                sentences = batch["sentences"]
                for idx, words in enumerate(sentences):
                    text = " ".join(words)
                    tok2char_span = WhiteWordTokenizer.get_tok2char_span_map(words)
                    ent_list = []
                    for ent in ners[idx]:
                        ent_text = " ".join(words[ent[0]:ent[1] + 1])
                        char_span_list = tok2char_span[ent[0]:ent[1] + 1]
                        char_span = [char_span_list[0][0], char_span_list[-1][1]]
                        norm_ent = {"text": ent_text,
                                    "type": ent[2],
                                    "char_span": char_span}
                        assert ent_text == text[char_span[0]:char_span[1]]
                        ent_list.append(norm_ent)
                    sample = {
                        "text": text,
                        "word_list": words,
                        "entity_list": ent_list,
                    }
                    out_data.append(sample)

        json.dump(out_data, open(out_path, "w", encoding="utf-8"), ensure_ascii=False)


def clean_entity(ent):
    ent = re.sub("ï¿½", "", ent)
    return ent.strip()


def trans_daixiang_data(path, data_type=None):
    with open(path, "r", encoding="utf-8") as file_in:
        lines = [line.strip("\n") for line in file_in]
        data = []
        for i in range(0, len(lines), 3):
            sample = lines[i: i + 3]
            text = sample[0]
            word_list = text.split(" ")
            annstr = sample[1]
            ent_list = []
            word2char_span = WhiteWordTokenizer.get_tok2char_span_map(word_list)

            # entities
            for ann in annstr.split("|"):
                if ann == "":
                    continue
                offsets, ent_type = ann.split(" ")
                offsets = [int(idx) for idx in offsets.split(",")]
                assert len(offsets) % 2 == 0
                for idx, pos in enumerate(offsets):
                    if idx % 2 != 0:
                        offsets[idx] += 1

                extr_segs = []
                char_span = []
                tok_span = []
                for idx in range(0, len(offsets), 2):
                    wd_sp = [offsets[idx], offsets[idx + 1]]
                    ch_sp_list = word2char_span[wd_sp[0]:wd_sp[1]]
                    ch_sp = [ch_sp_list[0][0], ch_sp_list[-1][1]]

                    seg_wd = " ".join(word_list[wd_sp[0]: wd_sp[1]])
                    seg_ch = text[ch_sp[0]:ch_sp[1]]
                    assert seg_ch == seg_wd

                    char_span.extend(ch_sp)
                    tok_span.extend(wd_sp)
                    extr_segs.append(seg_ch)
                ent_txt_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(char_span, text, "en")
                ent_txt = " ".join(extr_segs)

                assert ent_txt == ent_txt_extr
                ent = {
                    "text": ent_txt,
                    "type": ent_type,
                    "char_span": char_span,
                    "tok_span": tok_span,
                }
                ent_list.append(ent)

            # merge continuous spans
            for ent in ent_list:
                ori_char_span = ent["char_span"]
                merged_span = merge_spans(ori_char_span)
                ent_ori_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(ori_char_span, text, "en")
                ent_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(merged_span, text, "en")
                ent["char_span"] = merged_span
                assert ent_ori_extr == ent_extr == ent["text"]

            new_sample = {
                "text": sample[0],
                "word_list": word_list,
                "word2char_span": word2char_span,
                "entity_list": ent_list,
            }
            if data_type is not None:
                new_sample["id"] = "{}_{}".format(data_type, len(data))

            data.append(new_sample)

    return data


def postprocess_duee():
    res_data_path = "../../data/res_data/test_data.json"
    out_path = "../../data/res_data/duee.json"
    test_data = load_data(res_data_path)
    test_data2submit = []
    for sample in test_data:
        sample2submit = {
            "id": sample["id"],
            "text": sample["text"],
            "event_list": []
        }
        for event in sample["event_list"]:
            event2submit = {
                "event_type": event["trigger_type"],
                "trigger": event["trigger"],
                "trigger_start_index": event["trigger_char_span"][0],
                "arguments": [],
            }
            for arg in event["argument_list"]:
                event2submit["arguments"].append({
                    "argument": arg["text"],
                    "role": arg["type"],
                    "argument_start_index": arg["char_span"][0],
                })
            sample2submit["event_list"].append(event2submit)
        test_data2submit.append(sample2submit)
    save_as_json_lines(test_data2submit, out_path)


def preprocess_oie4():
    data_in_dir = "../../data/ori_data/oie4_bk"
    data_out_dir = "../../data/ori_data/oie4"
    if not os.path.exists(data_out_dir):
        os.makedirs(data_out_dir)

    train_filename = "openie4_labels"
    valid_filename = "dev.tsv"
    test_filename = "test.tsv"
    train_path = os.path.join(data_in_dir, train_filename)
    valid_path = os.path.join(data_in_dir, valid_filename)
    test_path = os.path.join(data_in_dir, test_filename)

    train_save_path = os.path.join(data_out_dir, "train_data.json")
    valid_save_path = os.path.join(data_out_dir, "valid_data.json")
    test_save_path = os.path.join(data_out_dir, "test_data.json")

    train_data = []
    with open(train_path, "r", encoding="utf-8") as file_in:
        text = None
        words = None
        tag_lines = []
        for line in tqdm(file_in, desc="loading data"):
            line = line.strip("\n")
            if re.search("ARG|REL|NONE", line) is not None:
                tag_lines.append(line.split(" "))
            else:
                if text is not None:
                    train_data.append({
                        "text": text,
                        "word_list": words,
                        "tag_lines": tag_lines
                    })
                    tag_lines = []
                text = line
                words = line.split(" ")
        if text is not None:
            train_data.append({
                "text": text,
                "word_list": words,
                "tag_lines": tag_lines
            })

    for sample in tqdm(train_data, desc="transforming data"):
        open_spo_list = []
        word_list = sample["word_list"]
        tok2char_span = WhiteWordTokenizer.get_tok2char_span_map(word_list)
        text = sample["text"]

        for tags in sample["tag_lines"]:
            for tag_id in range(-3, 0):
                if tags[tag_id] != "NONE":
                    assert tags[tag_id] == "REL"
                    tags[tag_id] = "ADD"
            type2indices = {}
            for idx, tag in enumerate(tags):
                if tag == "NONE":
                    continue
                if tag not in type2indices:
                    type2indices[tag] = []
                type2indices[tag].append(idx)

            spo = {"predicate": {"text": "",
                                 "complete": "",
                                 "predefined": False,
                                 "prefix": "",
                                 "suffix": "",
                                 "char_span": [0, 0],
                                 },
                   "subject": {"text": "", "char_span": [0, 0]},
                   "object": {"text": "", "char_span": [0, 0]},
                   "other_args": []}
            add_text = None
            other_args = []
            for type_, ids in type2indices.items():
                wd_spans = []
                pre = -10
                for pos in ids:
                    if pos - 1 != pre:
                        wd_spans.append(pre + 1)
                        wd_spans.append(pos)
                    pre = pos
                wd_spans.append(pre + 1)
                wd_spans = wd_spans[1:]

                ch_spans = Preprocessor.tok_span2char_span(wd_spans, tok2char_span)
                arg_text = Preprocessor.extract_ent_fr_toks(wd_spans, word_list, "en")
                arg_text_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(ch_spans, text, "en")
                assert arg_text_extr == arg_text

                type_map = {
                    "REL": "predicate",
                    "ARG1": "subject",
                    "ARG2": "object",
                    "ADD": "add",
                    "TIME": "time",
                    "LOC": "location",
                }
                if type_ in {"REL", "ARG1", "ARG2"}:
                    spo[type_map[type_]] = {
                        "text": arg_text,
                        "char_span": ch_spans,
                        "word_span": wd_spans,
                    }
                elif type_ in {"TIME", "LOC"}:
                    other_args.append({
                        "type": type_map[type_],
                        "text": arg_text,
                        "char_span": ch_spans,
                    })
                else:
                    add_info_map = {
                        "[unused1]": "be-none",
                        "[unused2]": "be-of",
                        "[unused3]": "be-from",
                    }
                    add_text = add_info_map[arg_text]

            if "predicate" not in spo:
                if add_text == "be-none":
                    spo["predicate"] = {
                        "predefined": True,
                        "text": "be",
                        "complete": "be",
                        "prefix": "",
                        "suffix": "",
                        "char_span": [0, 0]
                    }
                else:
                    spo["predicate"] = {
                        "predefined": True,
                        "text": "",
                        "complete": "DEFAULT",
                        "prefix": "",
                        "suffix": "",
                        "char_span": [0, 0]
                    }
                    raise Exception
            else:
                spo["predicate"]["prefix"] = ""
                spo["predicate"]["suffix"] = ""
                spo["predicate"]["predefined"] = False
                if add_text is not None:
                    spo["predicate"]["prefix"] = "be"
                    if add_text == "be-of":
                        spo["predicate"]["suffix"] = "of"
                    if add_text == "be-from":
                        spo["predicate"]["suffix"] = "from"
                spo["predicate"]["complete"] = " ".join([spo["predicate"]["prefix"],
                                                         spo["predicate"]["text"],
                                                         spo["predicate"]["suffix"]]).strip()
                spo["other_args"] = other_args
                open_spo_list.append(spo)

        word_list = word_list[:-3]
        text = " ".join(word_list)
        sample["word2char_span"] = tok2char_span[:-3]
        sample["text"] = text
        sample["word_list"] = word_list
        sample["open_spo_list"] = open_spo_list
        for spo in open_spo_list:
            for key, val in spo.items():
                if key == "other_args":
                    for arg in spo[key]:
                        arg_text_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(arg["char_span"], text, "en")
                        assert arg_text_extr == arg["text"]
                else:
                    arg_text_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(val["char_span"], text, "en")
                    assert arg_text_extr == val["text"]

    # valid and test
    def get_val_test_data(path):
        fix_map = {
            "the unique limestone limestone of the mountains": "the unique limestone of the mountains",
            "do n't": "don't",
            "did n't": "didn't",
        }

        with open(path, "r", encoding="utf-8") as file_in:
            lines = []
            for line in file_in:
                for key, val in fix_map.items():
                    line = re.sub(key, val, line)
                splits = line.strip("\n").split("\t")
                # puncs = re.escape(re.sub("\.", "", string.punctuation))
                new_splits = []
                # ç¬¦å·ä¸åè¯ç¨ç©ºæ ¼éå¼
                for sp in splits:
                    sp = re.sub("([A-Za-z]+|[0-9\.]+)", r" \1 ", sp)
                    sp = re.sub("\s+", " ", sp)
                    sp = re.sub("n ' t", "n 't", sp)
                    sp = re.sub("' s", "'s", sp)
                    sp = sp.strip()
                    new_splits.append(sp)
                lines.append(new_splits)

            text2anns = {}
            for line in lines:
                text = line[0]
                if text not in text2anns:
                    text2anns[text] = []
                spo = {"predicate": {"text": line[1],
                                     "complete": line[1],
                                     "predefined": False,
                                     "prefix": "",
                                     "suffix": "",
                                     "char_span": [0, 0],
                                     },
                       "subject": {"text": line[2], "char_span": [0, 0]},
                       "object": {"text": "", "char_span": [0, 0]},
                       "other_args": []}

                if len(line) >= 4:
                    if "C :" not in line[3]:
                        spo["object"] = {"text": line[3], }
                if len(line) >= 5:
                    if "C :" not in line[4]:
                        arg = re.sub("T : |L : ", "", line[4])
                        spo["other_args"].append({"text": arg, "type": "time/loc_1"})
                if len(line) == 6:
                    if "C :" not in line[5]:
                        arg = re.sub("T : |L : ", "", line[5])
                        spo["other_args"].append({"text": arg, "type": "time/loc_2"})
                text2anns[text].append(spo)

        data = []
        for text, anns in text2anns.items():
            data.append({
                "text": text,
                "open_spo_list": anns,
            })

        # spans
        predefined_p_set = {"belong to",
                            "come from",
                            "have a", "have",
                            "will be",
                            "exist",
                            "be", "be a", "be in", "be on", "be at", "be of", "be from", "be for", "be with"}
        prefix_set = {
            "be", "will", "will be", "have", "have no", "must", "do not", "that",
        }
        suffix_set = {"in", "by", "of", "to", "from", "at"}
        samples_w_tl = []

        def my_lexeme(ori_word):
            lexeme_ws = lexeme(ori_word)
            lexeme_ws += [ori_word[0].upper() + ori_word[1:]]
            lexeme_ws += [ori_word.lower()]
            lexeme_ws += [ori_word.upper()]

            if ori_word[-2:] == "ly":
                lexeme_ws += [ori_word[:-2]]
            if re.match("[A-Z]", ori_word[0]) is not None:
                lexeme_ws += [ori_word + "'s"]
            if ori_word == "pursued":
                lexeme_ws += ["Pursuit"]
            if ori_word == "approve":
                lexeme_ws += ["approval"]
            if ori_word == "goes":
                lexeme_ws += ["exit onto"]
            return lexeme_ws

        def try_best2get_spans(target_str, text):
            spans, add_text = Preprocessor.search_char_spans_fr_txt(target_str, text, "en")
            fin_spans = None
            if add_text.strip("_ ") == "" and len(spans) != 0:  # if exact match
                fin_spans = spans
            else:
                pre_add_text = add_text
                # find words need to alter
                words2lexeme = re.findall("[^_\s]+", add_text)
                # lexeme all words
                words_list = [my_lexeme(w) for w in words2lexeme]
                # enumerate all possible alternative words
                alt_words_list = [[w] for w in words_list[0]] if len(words_list) == 1 else itertools.product(
                    *words_list)

                match_num2spans = {}
                max_match_num = 0
                for alt_words in alt_words_list:
                    chs = list(target_str)
                    add_text_cp = pre_add_text[:]
                    for wid, alt_w in enumerate(alt_words):
                        # search the span of the word need to alter
                        m4alt = re.search("[^_\s]+", add_text_cp)
                        sp = m4alt.span()
                        if alt_w == m4alt.group():  # same word, skip
                            continue
                        # alter the word
                        chs[sp[0]:sp[1]] = list(alt_w)
                        # mask the positions, will be ignore when getting m4alt next time
                        add_text_cp_ch_list = list(add_text_cp)
                        add_text_cp_ch_list[sp[0]:sp[1]] = ["_"] * len(alt_w)
                        add_text_cp = "".join(add_text_cp_ch_list)
                    # alternative text
                    alt_txt = "".join(chs)

                    # try to get spans
                    spans, add_text = Preprocessor.search_char_spans_fr_txt(alt_txt, text, "en")
                    # cal how many words are matched this time
                    match_num = len(re.findall("_+", add_text)) - len(re.findall("_+", pre_add_text))
                    if match_num > 0:  # some words matched
                        match_num2spans[match_num] = spans
                        max_match_num = max(max_match_num, match_num)
                if max_match_num > 0:  # if there are any successful cases
                    fin_spans = match_num2spans[max_match_num]  # use the longest match

            if fin_spans is None or len(fin_spans) == 0:  # if still can not match, take partial match instead
                spans, add_text = Preprocessor.search_char_spans_fr_txt(target_str, text, "en")
                fin_spans = spans
            return fin_spans

        for sample in tqdm(data, "add char span to val/test"):
            text = sample["text"]
            for spo in sample["open_spo_list"]:
                if len(spo) >= 4:
                    samples_w_tl.append(spo)
                for key, val in spo.items():
                    if key == "predicate":
                        predicate = spo["predicate"]["text"]
                        p_words = predicate.split()
                        p_lemma_words = [lemma(w) for w in p_words]
                        p_lemma = " ".join(p_lemma_words)

                        if p_lemma in predefined_p_set:
                            spo["predicate"]["predefined"] = True
                            spo["predicate"]["text"] = ""
                            spo["predicate"]["complete"] = p_lemma
                            spo["predicate"]["char_span"] = [0, 0]
                            continue

                        spans, add_text = Preprocessor.search_char_spans_fr_txt(predicate, text, "en")
                        if add_text.strip("_ ") == "" and len(spans) != 0:
                            spo["predicate"]["char_span"] = spans
                            continue

                        # take prefix and suffix out
                        if re.search("[A-Za-z0-9]$", add_text):
                            for suffix in sorted(suffix_set, key=lambda a: len(a), reverse=True):
                                if re.search(" {}$".format(suffix), p_lemma):
                                    spo["predicate"]["text"] = " ".join(
                                        spo["predicate"]["text"].split()[:len(p_words) - len(suffix.split())])
                                    spo["predicate"]["suffix"] = suffix
                                    break
                        if re.search("^[A-Za-z0-9]", add_text):
                            for prefix in sorted(prefix_set, key=lambda a: len(a), reverse=True):
                                if re.search("^{} ".format(prefix), p_lemma):
                                    spo["predicate"]["text"] = " ".join(
                                        spo["predicate"]["text"].split()[len(prefix.split()):])
                                    spo["predicate"]["prefix"] = prefix
                                    break

                    elif key != "other_args":
                        arg = spo[key]
                        if arg is not None:
                            arg["char_span"] = try_best2get_spans(arg["text"], text)
                            seg_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(arg["char_span"], text, "en")
                            # if seg_extr != arg["text"]:
                            #     print(sample["text"])
                            #     print("target_seg: {}".format(arg["text"]))
                            #     print("extr_seg: {}".format(seg_extr))
                            #     pprint(spo)
                            #     print("===============")
                    else:
                        for arg in spo[key]:
                            arg["char_span"] = try_best2get_spans(arg["text"], text)
                            seg_extr = Preprocessor.extract_ent_fr_txt_by_char_sp(arg["char_span"], text, "en")
                            # if seg_extr != arg["text"]:
                            #     print(sample["text"])
                            #     print("target_seg: {}".format(arg["text"]))
                            #     print("extr_seg: {}".format(seg_extr))
                            #     pprint(spo)
                            #     print("===============")
        return data

    valid_data = get_val_test_data(valid_path)
    test_data = get_val_test_data(test_path)
    save_as_json_lines(train_data, train_save_path)
    save_as_json_lines(valid_data, valid_save_path)
    save_as_json_lines(test_data, test_save_path)
    return train_data, valid_data, test_data


def trans2dai_dataset():
    '''
    change our data format to daixiang data format
    :return:
    '''
    in_data_dir = "../../data/normal_data/share_14_uncbase"
    out_data_dir = "../../data/ori_data/share_14_uncbase"
    if not os.path.exists(out_data_dir):
        os.makedirs(out_data_dir)

    test_data_path = os.path.join(in_data_dir, "test_data.json")
    train_data_path = os.path.join(in_data_dir, "train_data.json")
    valid_data_path = os.path.join(in_data_dir, "valid_data.json")
    test_out_path = os.path.join(out_data_dir, "test.txt")
    valid_out_path = os.path.join(out_data_dir, "dev.txt")
    train_out_path = os.path.join(out_data_dir, "train.txt")

    def trans2daixiang_subwd(in_path, out_path):
        data = load_data(in_path)
        with open(out_path, "w", encoding="utf-8") as out_file:
            for sample in data:
                ent_list = []
                for ent in sample["entity_list"]:
                    ent_subwd_sp = [str(pos) if idx % 2 == 0 else str(pos - 1) for idx, pos in
                                    enumerate(ent["wd_span"])]
                    ent_list.append(",".join(ent_subwd_sp) + " " + ent["type"])
                text = sample["text"]
                ann_line = "|".join(ent_list)
                out_file.write("{}\n".format(text))
                out_file.write("{}\n".format(ann_line))
                out_file.write("\n")

    trans2daixiang_subwd(test_data_path, test_out_path)
    trans2daixiang_subwd(train_data_path, train_out_path)
    trans2daixiang_subwd(valid_data_path, valid_out_path)


def preprocess_saoke(data_path = "../../data/ori_data/saoke_bk/saoke.json"):
    data = load_data(data_path)
    # fix data
    subj_fix_map = {
        "[BodoL innhoff|BodoL innhoffåäº]": "BodoL innhoff[åäº|]",
        "[å¼ è|å¼ èä¼ä¼´ä»¬]": "å¼ è[ä¼ä¼´ä»¬|]",
        "[æ²é|æ²éçæçå·¨é­]": "æ²é[çæçå·¨é­|]",
        "[æ¾ä¸æ­¦å¨ç[åæ°å£«åµ|å¹³æ°]": "æ¾ä¸æ­¦å¨ç[åæ°å£«åµ|å¹³æ°]",
        "[è±å­æ¯|çµè¯|å¹³é¢çµè§|ä¿é©ç®±|ä¹¦æ¡|ææ°|æ´è¡£å®¤|æ²å|æ¨è´¨/é¶æ¨å°æ¿|è¡£æ/è¡£æ©±|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|æµ´ç¼¸ORæ·æµ´|å¤éæå¡]": "[è±å­æ¯|çµè¯|å¹³é¢çµè§|ä¿é©ç®±|ä¹¦æ¡|ææ°|æ´è¡£å®¤|æ²å|æ¨è´¨/é¶æ¨å°æ¿|è¡£æ/è¡£æ©±|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|æµ´ç¼¸|æ·æµ´|å¤éæå¡]",
        "[ãç·åããå¥³åã|ãè·³æ å¸¸ã]ç[ææ|åºæ¿]]": "[ãç·åã|ãå¥³åã|ãè·³æ å¸¸ã]ç[ææ|åºæ¿]",
        "[ç|å¸|å°]çº§æ°´è¡æ¿ä¸»ç®¡é¨é¨]": "[ç|å¸|å°]çº§æ°´è¡æ¿ä¸»ç®¡é¨é¨",
        "[æ²ä¸é­å ¡äº|ç»æå¨åä»¤ç³»å][æ²ä¸é­å ¡äº|ç»æå¨åä»¤ç³»å]": "[æ²ä¸é­å ¡äº|ç»æå¨åä»¤ç³»å]",
        "[å®¹é|åå¼|ç©ºè½½çµæµ|ç©ºè½½æè|ç­è·¯ï¼è´è½½ï¼æè|é»æçµåå®¹é|åå¼|ç©ºè½½çµæµ|ç©ºè½½æè|ç­è·¯ï¼è´è½½ï¼æè|é»æçµå]": "[å®¹é|åå¼|ç©ºè½½çµæµ|ç©ºè½½æè|ç­è·¯ï¼è´è½½ï¼æè|é»æçµå]",
        "[[ãæ±ç é±¼è°±ã|ãéé±¼åã|ãéé±¼é¥²å»æ³ã]": "[ãæ±ç é±¼è°±ã|ãéé±¼åã|ãéé±¼é¥²å»æ³ã]",
        "[Denise Gimpel|Lyce.jankowski|Lyce.jankowski]": "[Denise Gimpel|Dr.james S.Edgren|Lyce.jankowski]",
        "[å¼ å®¶ååç|ä¸æ°å®åç|èµµåç¶åç|æ¹ç»ªé¾åç|å®¢æ·ä»£è¡¨]|": "[å¼ å®¶ååç|ä¸æ°å®åç|èµµåç¶åç|æ¹ç»ªé¾åç|å®¢æ·ä»£è¡¨]",
        "1å[å¹´é¾è¾å¤§çå¿ç«¥|æäºº]|ä¸å¼ å åºæ¶è´¹": "1å[å¹´é¾è¾å¤§çå¿ç«¥|æäºº]ä¸å¼ å åºæ¶è´¹",
        "[[å¬å¼|çè¯]è°è®ºéè¦è¯¾é¢|ä¸æ­ææèªå·±æèè½å]]": "[[å¬å¼|çè¯]è°è®ºéè¦è¯¾é¢|ä¸æ­ææèªå·±æèè½å]",
        "[ä¸­åæå|æ±æ·®æå|ééµæå|å´æå]": "[ä¸­å|æ±æ·®|ééµ|å´]æå",
        "[éæ­£å®|éè£ç»|éå­é]ç­": "[éæ­£å®|éå­é|éè£ç»]ç­",
        "[éæ­£å®|éè£ç»|éå­é]": "[éæ­£å®|éå­é|éè£ç»]",
        "[æ|é£å ä¸ªå£«åµ]": "[é£å ä¸ªå£«åµ|æ]",
        "[ä»|ä¸­çä»ä¹å©|äºä»£åå]": "[ä»|äºä»£åå|ä¸­çä»ä¹å©]",
        '[[åºé³|å®å|åæ°´|å®å·|æ­£å®]é¾æ°]|[è£åé¾æ°]]': '[[åºé³|å®å|åæ°´|å®å·|æ­£å®]é¾æ°|è£åé¾æ°]',
        "[ææç|ä¹¡ç»ç­¹è´¹|éç»ç­¹è´¹]": "[ææç|[ä¹¡|é]ç»ç­¹è´¹]",
        "[èèå¬ä¸»|æ¨åå½é|ä»åº·ç§æ]]": "[èèå¬ä¸»|æ¨åå½é|ä»åº·ç§æ]",
        "å¶[çµæ´»|æ§ä»·æ¯|æå¡ç»éª|å¯¹æ¬åæ¶è´¹è|å¯¹æ¬åæ¶è´¹èæ´å¯]": "å¶[çµæ´»|æ§ä»·æ¯|æå¡ç»éª|å¯¹æ¬åæ¶è´¹èæ´å¯]",
        "æ²¹ç»[ãçº¢è¡£å°å¥³ã|ãç´«ç½|å°ã]|": "æ²¹ç»[ãçº¢è¡£å°å¥³ã|ãç´«ç½å°ã]",
        "[ç¢³çº¤ç»´ORç¡¼çº¤ç»´å¢å¼ºçç¯æ°§æ èåºå¤åææ|éå±åºå¤åææ]": "[[ç¢³çº¤ç»´|ç¡¼çº¤ç»´]å¢å¼ºç[ç¯æ°§æ èåºå¤åææ|éå±åºå¤åææ]]",
    }
    pred_fix_map = {
        "[è¸ååè¿ç­]åç­é¢": "[è¸å|è¿ç­]åç­é¢",
        "[é«ç¨³äº§åç°å°é¢ç§¯|äººåé«ç¨³äº§åç°å°é¢ç§¯": "[é«ç¨³äº§åç°å°é¢ç§¯|äººåé«ç¨³äº§åç°å°é¢ç§¯]",
        "[åå¼±ORæ¶å¤±]": "[åå¼±|æ¶å¤±]",
        "æ´ä½[è§å|è¥å»º|": "æ´ä½[è§å|è¥å»º]",
        "äºåå¥èµ°|": "äºåå¥èµ°",
        "æè®¢|ç»ç»å®æ½]": "[æè®¢|ç»ç»å®æ½]",
        "è§å®|": "è§å®",
        "ä»¥âè£æ°å¤§å®ç¥ å¾â|": "ä»¥âè£æ°å¤§å®ç¥ å¾â",
        "å¥å¨||": "å¥å¨",
        "è½¬ç§»|å¹è®­": "[è½¬ç§»|å¹è®­]",
        "è¢«å½å¡é¢ç¡®ç¡®å®ä¸º": "è¢«å½å¡é¢ç¡®å®ä¸º",
        "èä¼|": "èä¼",
        "ç±Xç±": "ç±Xåºç",
        "ä»¥Xä»¥": "ä»¥Xä¸ºç®æ ",
        "å¯ä»¥åXå¯ä»¥å": "å¯ä»¥åX",
        "å¯ä½¿Xä½ä¸|": "å¯ä½¿Xä½ä¸",
        "ä¸ç´è´åäºä¸ºXæä¾æå¡]": "ä¸ç´è´åäºä¸ºXæä¾[è®¾æ½|æå¡]",
        "ä»¥è¾¾å°çææXçææ": "ä»¥è¾¾å°Xçææ",
        "ä¸ºX[åå¦ãæ®æ]": "ä¸ºX[åå¦|æ®æ]",
        "æå©Xå¿«é[æå»ºè°æ´]Y": "æå©Xå¿«é[æå»º|è°æ´]Y",
        "ä¾æ³çç£æ£æ¥Xè´¯å½»æ§è¡å®å¨çäº§æ³è§æåµ|è®¾å¤è®¾æ½å®å¨æåµ]": "ä¾æ³çç£æ£æ¥Xè´¯å½»æ§è¡[å®å¨çäº§[æ³å¾|æ³è§]æåµ|å®å¨çäº§æ¡ä»¶|è®¾å¤è®¾æ½å®å¨æåµ]",
        "è´åäºå¸®å©Xè§£å³æè²ãèä¸ç­ç­å·¥ä½æ¹æ¹é¢é¢æéå°çé¾é¢]": "è´åäºå¸®å©Xè§£å³[è¡£|é£|ä½|è¡|å¨±ä¹|ææ|æè²|èä¸]çé¾é¢",
        "ä¸ºXæä¾ä¸ä¸ªçæ§éæ©çå¹³å°|": "ä¸ºXæä¾ä¸ä¸ª[å®¢è§è¯ä¼°|çæ§éæ©]çå¹³å°",
        "ä¼Xå¸¦æ¥[å²å»åå½±å]": "ä¼ç»Xå¸¦æ¥[å²å»|å½±å]",
        "ä»¥Xèµ¢å¾äº[Y]": "ä»¥Xèµ¢å¾äºY",
        "ä¾æ³æå®Xè´è´£Yç[ä¸å®¡|[åºå±æ³é¢|æ£å¯é¢äºå®¡æ¡ä»¶]çå®¡çåæ³å¾çç£å·¥ä½": "ä¾æ³æå®Xè´è´£Yç[ä¸å®¡|åºå±[æ³é¢|æ£å¯é¢]äºå®¡]æ¡ä»¶ç[å®¡ç|æ³å¾çç£å·¥ä½]",
        "å X sockrrlates": "åXsockrrlates",
        "ç¨Xååè¡ä¸­å¿ç³æ¥[ä¸­å¥ç­çº§|æç¥¨äºº[å§å|ä½å|èº«ä»½è¯å·ç ]": "ç¨Xååè¡ä¸­å¿ç³æ¥[ä¸­å¥ç­çº§|æç¥¨äºº[å§å|ä½å|èº«ä»½è¯å·ç ]]",
        "å¯¹Xå¯å®ç°å¨é¨[ä¸»å¤§å¤æ°çå¶çèè´[ä¸»|å¯]æåä½æç[ä¸æ¬¡ææ¾|ä¸æ¬¡å®æY]": "å¯¹Xå¯å®ç°å¨é¨[ä¸»|å¯]æåä½æç[ä¸æ¬¡ææ¾|ä¸æ¬¡å®æY]",
        "ä¸ºXå¥ç®äºäºä¸ä¸ªåä¸ä¸ªè§è²": "ä¸ºXå¥ç®äºä¸ä¸ªåä¸ä¸ªè§è²",
        "æ¯ä¿æ¤Xåé­[[å°é|æ°´ç¾|ç«ç¾]ç­ç¯å¢äºæ]|äººä¸ºæä½[å¤±è¯¯|éè¯¯]|åç§è®¡ç®æºç¯ç½ªè¡ä¸º]å¯¼è´çç ´åè¿ç¨": "æ¯ä¿æ¤Xåé­[[å°é|æ°´ç¾|ç«ç¾]ç­ç¯å¢äºæ|äººä¸ºæä½[å¤±è¯¯|éè¯¯]|åç§è®¡ç®æºç¯ç½ªè¡ä¸º]å¯¼è´çç ´åè¿ç¨",
        "ä»èåè½»|é¿åXçåè¿«": "ä»è[åè½»|é¿å]Xç[æ©æ¦|åè¿«]",
        "è®©Xä¹è½æ¥è§¦[æéè¯æ±ORå¿ å­èä¹æäº]": "è®©Xä¹è½æ¥è§¦[æéè¯æ±|å¿ å­èä¹æäº]",
        "è´è´£å¯¹Xç[å®¡æ¹åæ¾åç®¡ç]": "è´è´£å¯¹Xç[å®¡æ¹åæ¾|ç®¡ç]",
        "åçXç[è°ä»»|è½¬ä»»|å®¡æ¹|å·¥ä½]": "åçXç[è°ä»»|è½¬ä»»|å®¡æ¹å·¥ä½]",
        "èµX[å£°å¦å¥é·ï¼å¿å¦çæ¶ï¼éµå¦è±ªæ­]": "èµX[å£°å¦å¥é·|å¿å¦çæ¶|éµå¦è±ªæ­]",
        "[æ¥èµä¸ºåè´¾|[é©¬è´©|å± å®°]ä¹ç±»]": "æ¥èµä¸º[åè´¾|é©¬è´©|å± å®°]",
        "åçXç[è°ä»»|è½¬ä»»|å®¡æ¹|å·¥ä½": "åçXç[è°ä»»|è½¬ä»»|å®¡æ¹|å·¥ä½]",
        "ä½ç°äº[ç[è±ªå|æ§æ|èé|åé æ§]æ¼ç»çæè´": "ä½ç°äºXç[è±ªå|æ§æ|èé|åé æ§]æ¼ç»çæè´",
        "å¾åäºX[ç[å±±æ°´é£å|äººæåå²|èªç¶å°ç|æ°ä¿é£æ]": "å¾åäºXç[å±±æ°´é£å|äººæåå²|èªç¶å°ç|æ°ä¿é£æ]",
        "[äº[ä¸ç¦|ä¸å¡]ä¹é´ç[çå¯¼|åå|è¯´æ]|[[å¤ºå¶å©|ä¸å¶å©|ä¸å¶å©]çå å¿å©å¯¼çæè²å·¥ä½]": "[äº[ä¸ç¦|ä¸å¡]ä¹é´ç[çå¯¼|åå|è¯´æ]|[å¤ºå¶å©|ä¸å¶å©]çå å¿å©å¯¼çæè²å·¥ä½]",
    }
    obj_fix_map = {
        "[è±å­æ¯|çµè¯|å¹³é¢çµè§|ä¿é©ç®±|ä¹¦æ¡|ææ°|æ´è¡£å®¤|æ²å|æ¨è´¨/é¶æ¨å°æ¿|è¡£æ/è¡£æ©±|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|æµ´ç¼¸ORæ·æµ´|å¤éæå¡]": "[è±å­æ¯|çµè¯|å¹³é¢çµè§|ä¿é©ç®±|ä¹¦æ¡|ææ°|æ´è¡£å®¤|æ²å|æ¨è´¨/é¶æ¨å°æ¿|è¡£æ/è¡£æ©±|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´å®¤|æµ´ç¼¸|æ·æµ´|å¤éæå¡]",
        "[è·å¾[çå­|åå±|å£®å¤§|ä¸ºå¨ç¤¾ä¼æå¡]": "[è·å¾[çå­|åå±|å£®å¤§]|ä¸ºå¨ç¤¾ä¼æå¡]",
        "[[[åºç¨çç©ææ¯|åå·¥|è¯ç©å¶å|]ç¸å³ä¸ä¸å¨æ¥å¶å­¦ç|å¹è®­æºæå­¦ç]": "[[åºç¨çç©ææ¯|åå·¥|è¯ç©å¶å|ç¸å³ä¸ä¸][å¨æ¥å¶å­¦ç|å¹è®­æºæå­¦ç]]",
        "[å¸å§åé¨é¨ä¹é´|æ¿åºåé¨é¨ä¹é´|å¸å§åé¨é¨ä¸æ¿åºåé¨é¨ä¹é´|å¸ç´é¨é¨ä¸ä¹¡ä¹é´|å¸ç´é¨é¨ä¸éä¹é´]çèè´£åå·¥]": "[å¸å§åé¨é¨ä¹é´|æ¿åºåé¨é¨ä¹é´|å¸å§åé¨é¨ä¸æ¿åºåé¨é¨ä¹é´|å¸ç´é¨é¨ä¸[ä¹¡|é]ä¹é´]çèè´£åå·¥",
        "[æ±æ²³|æºªæµ|æ¹æ³|æ°´å¡|æµ·å²¸]ç­æ°´åå²¸è¾¹|å¶æµæ°´å¤]": "[[æ±æ²³|æºªæµ|æ¹æ³|æ°´å¡|æµ·å²¸]ç­æ°´åå²¸è¾¹|å¶æµæ°´å¤]",
        "[æ·±åæ¹é©|æ©å¤§å¼æ¾|å¼è¿å½åå¤[èµé|ææ¯|äººæ]": "[æ·±åæ¹é©|æ©å¤§å¼æ¾|å¼è¿å½åå¤[èµé|ææ¯|äººæ]]",
        "[å¾·å½EVITAââ¡å¨èªå¨å¼å¸æº|CSIâ507SDå¤åæ°çæ¤ä»ª|æ¥æ¬ç¾è½èªå¨çµå¼åº|æ´åææ¯å®¤]|": "[å¾·å½EVITAââ¡å¨èªå¨å¼å¸æº|CSIâ507SDå¤åæ°çæ¤ä»ª|æ¥æ¬ç¾è½èªå¨çµå¼åº|æ´åææ¯å®¤]",
        "[åä¸­|å°å­¦]|æè²": "[åä¸­|å°å­¦]æè²",
        "[éå±ORç¡¬çº¸|å¡æ]": "[éå±|ç¡¬çº¸|å¡æ]",
        "ç¢±[ç¢±2|10|25]mg": "ç¢±[2|10|25]mg",
        "[å¸åº|ç¤¾ä¼çäº§]|": "[å¸åº|ç¤¾ä¼çäº§]",
        "é¿å·è©[ç®å°|åè°]|": "é¿å·è©[ç®å°|åè°]",
        "[æå­|éæ]|[å°è¾¹æ ä¸|[æ¿åå±å|åº­é¢ä¸­]çæ ä¸]": "[æå­|[éæ|å°è¾¹]æ ä¸|[æ¿åå±å|åº­é¢ä¸­]çæ ä¸]",
        "[é©¬åæä¸»ä¹[æ°æè§|å®æè§|åç[æ°æå®æçè®º|æ¿ç­|æ³å¾æ³è§]]": "[é©¬åæä¸»ä¹[æ°æè§|å®æè§]|åç[æ°æå®æçè®º|æ¿ç­|æ³å¾æ³è§]]",
        "åå·¥å§å³äºåç[ææ³|ç»ç»]ä½é£å»ºè®¾|ååæè²è®¡å|ä¸­å¿ç»å­¦ä¹ è®¡å]": "[åå·¥å§å³äºåç[ææ³|ç»ç»]ä½é£å»ºè®¾|ååæè²è®¡å|ä¸­å¿ç»å­¦ä¹ è®¡å]",
        "[[å½å|çå]åå¸[é³å|æèºç]äººå£«": "[å½å|çååå¸][é³å|æèºç]äººå£«",
        "[åè½¬è¤¶æ²|å¹³å§è¤¶æ²ORéæ©æ­å±æ¨è¦æé ä½]": "[åè½¬è¤¶æ²|å¹³å§è¤¶æ²|éæ©æ­å±æ¨è¦æé ä½]",
        "[[ææº|PDA|MP4|ç¬è®°æ¬çµè|å°åæ¥æ¶ç»ç«¯]": "[ææº|PDA|MP4|ç¬è®°æ¬çµè|å°åæ¥æ¶ç»ç«¯]",
        "[å¤§åå¸|ä¸­åå¸|å°åå¸|å°åé]": "[[å¤§|ä¸­|å°]åå¸|å°åé]",
        "[æ°¯æ°°èé¯ç­|ç¹è°±å]æ··åå·é¾]": "[æ°¯æ°°èé¯ç­æè«å|ç¹è°±åæ··åå·é¾]",
        "[æ±æå«çé¢éå»º|æ±æå«çé¢å åºé¡¹ç®|æ±æå°å­¦éå»ºé¡¹ç®|æ±æå¹¼å¿å­éå»ºé¡¹ç®]": "[æ±æå«çé¢[éå»º|å åº]é¡¹ç®|æ±æ[å°å­¦|å¹¼å¿å­]éå»ºé¡¹ç®]",
        "[è´¢æ¿ã[éè|å¶ä»ç»æµ|ç¤¾ä¼åå±]": "[è´¢æ¿|éè|å¶ä»ç»æµ|ç¤¾ä¼åå±]çæåµ",
        "[å¦é¨å¸è¹ä¸åä¼|æ°´è·¯è¿è¾è¡æ¿ä¸»ç®¡é¨é¨ORå¶ä»æå³é¨é¨]": "[å¦é¨å¸è¹ä¸åä¼|æ°´è·¯è¿è¾è¡æ¿ä¸»ç®¡é¨é¨|å¶ä»æå³é¨é¨]",
        "[æ¶è´¹èä¸ä¼ä¸æå³ç[ç»æµ|æ¿æ²»|ç¤¾ä¼|æ¥å¸¸æ´»å¨]èå´åç[è¡ä¸º|éè¦|æåº¦|å¨æº]ç­": "æ¶è´¹èä¸ä¼ä¸æå³ç[ç»æµ|æ¿æ²»|ç¤¾ä¼|æ¥å¸¸]æ´»å¨èå´åç[è¡ä¸º|éè¦|æåº¦|å¨æº]",
        "[[åäº¬|ä¸æµ·|å¤©æ´¥]ç­å°æ¹æ¿åº]|[ä¸­å¤®æ¿åº[æ°æ¿|ç¤¾ä¼ç¦å©|å¤èµç®¡ç]ç­é¨é¨çä¸é¡¹å¨è¯¢é¡¹ç®çé¡¾é®]]": "[[åäº¬|ä¸æµ·|å¤©æ´¥]ç­å°æ¹æ¿åº|ä¸­å¤®æ¿åº][æ°æ¿|ç¤¾ä¼ç¦å©|å¤èµç®¡ç]ç­é¨é¨çä¸é¡¹å¨è¯¢é¡¹ç®çé¡¾é®",
        "åæ¬[ä¸­å½[å¸é¿æµè¯|åå¸å¼ååºæèµç¯å¢è¯ä¼°]ç­å¨åçéè¦é¡¹ç®]]": "ä¸­å½[å¸é¿æµè¯|åå¸å¼ååºæèµç¯å¢è¯ä¼°]ç­éè¦é¡¹ç®",
        "[æºå³è¡æ¿äºå¡ç®¡ç|å¯¹å¤[èç»|åè°|æ¥å¾]|ä¼è®®çç»ç»å®æ|å¯¹å¤ä¿¡æ¯åå¸]]": "[æºå³è¡æ¿äºå¡ç®¡ç|å¯¹å¤[èç»åè°|æ¥å¾]|ä¼è®®çç»ç»å®æ|å¯¹å¤ä¿¡æ¯åå¸]",
        "[DESCåå®¹ä¸°å¯|æ´»å¨å¨é¢]": "[åå®¹ä¸°å¯|æ´»å¨å¨é¢]",
        "[å¦é¨ç¥éå¨æ­¦å¹³çæ´»ç»é¢æ°åå¹´æ¥æ­¦å¹³å¦é¨é´çäº¤æµæ´»å¨|æ°åå¹´æ¥æ­¦å¹³å¦é¨é´çäº¤æµæ´»å¨]": "[å¦é¨ç¥éå¨æ­¦å¹³çæ´»ç»é¢|æ°åå¹´æ¥æ­¦å¹³å¦é¨é´çäº¤æµæ´»å¨]",
        "[å½å[æ­¦æ¯ç|æè²ç]ç[ä¸å®¶|ææ|å­¦è]": "å½å[æ­¦æ¯ç|æè²ç]ç[ä¸å®¶|ææ|å­¦è]",
        "[è°èæ§|æ··åæ§]è¿è§]": "[è°èæ§|æ··åæ§]è¿è§",
        "[åä¸ªä¸åº|ä¸ä¸ç»]ç[éå¥|é¶å¥|éå¥|ä¼ç§èè¹æ°ä½åå¥]å±ä¸åäºä¸ª": "[[ä¸åº|ä¸ä¸ç»]ç[é|é¶|é]å¥|ä¼ç§èè¹æ°ä½åå¥]",
        "[æä½èççç»ç¨åº¦|è¿æä½èççç»ç¨åº¦|è¿éæ¬¡æ°|ç©¿åºéä¸ç©¿åºç¹è¸èåçº¿ä½çéè§åº¦|èºæ°è¿]ç­å ç´ éæ¬¡æ°ãç©¿åºéä¸ç©¿åºç¹è¸èåçº¿ä½çéè§åº¦åèºæ°è¿ç­å ç´ ": "[æä½èççç»ç¨åº¦|è¿éæ¬¡æ°|ç©¿åºéä¸ç©¿åºç¹è¸èåçº¿ä½çéè§åº¦|èºæ°è¿]ç­å ç´ ",
        "ç´æµ1mAçµå[ï¼U1mAï¼|0.75 U1mA|0.75 U1mA]": "[ç´æµ1mAçµåï¼U1mAï¼|0.75 U1mAä¸æ³æ¼çµæµ]",
        "æ®ç¾äººæ¥å[åº·å¤è®­ç»|æºææå»|èä¸å¹è®­|å°±ä¸æå¯¼]|[å¼å±æå|ä½è²æ´»å¨]]": "æ®ç¾äºº[æ¥å[åº·å¤è®­ç»|æºææå»|èä¸å¹è®­|å°±ä¸æå¯¼]|å¼å±[æå|ä½è²]æ´»å¨]",
        "[ç¾åº¦å¬å¸çæç¥è§å|ç¾åº¦å¬å¸çè¿è¥ç®¡ç]": "ç¾åº¦å¬å¸ç[æç¥è§å|è¿è¥ç®¡ç]",
        "[å·éå¬è·¯|æç»µé«éå¬è·¯|ç»µå¹¿é«éå¬è·¯]åæ¥ææéè·¯|ææ¸éè·¯|ææ¸é«éå¬è·¯]": "[å·éå¬è·¯|æç»µé«éå¬è·¯|ç»µå¹¿é«éå¬è·¯|ææéè·¯|ææ¸éè·¯|ææ¸é«éå¬è·¯]",
        "[[åå·¥|å¶é|é¢å|ç®¡é]ç­å¤å¿ä¸å¯å°çåä»¶": "[åå·¥|å¶é|é¢å|ç®¡é]ç­å¤å¿ä¸å¯å°çåä»¶",
        "[å·å¤[ç®¡ç|ç»æµ|æ³å¾|äººåèµæºç®¡ç]ç­æ¹é¢çç¥è¯åè½å]|[è½å¨[äºä¸åä½|æ¿åºé¨é¨]ä»äº[äººåèµæºç®¡ç]|[[æå­¦|ç§ç ]æ¹é¢å·¥ä½ç]å·¥åç®¡çå­¦ç§é«çº§ä¸é¨äººæ": "[å·å¤[ç®¡ç|ç»æµ|æ³å¾|äººåèµæºç®¡ç]ç­æ¹é¢çç¥è¯åè½å|è½å¨[äºä¸åä½|æ¿åºé¨é¨]ä»äº[äººåèµæºç®¡ç|æå­¦|ç§ç ]æ¹é¢å·¥ä½]çå·¥åç®¡çå­¦ç§é«çº§ä¸é¨äººæ",
        "[[éå±±|ä¸­å½é¶è|BBTVç¾è§é|ç§å¤§è®¯é£]ç­åä½ä¼ä¼´]|å¨çè½¯ä»¶å¼åå¤§èµç­è½¯ä»¶å¼åå¹³å°|éå¸å¨ççç åæºæ|æ¶²æ¶é¢æ¿ç­ä¸æ¸¸èµæºä¿é]": "[[éå±±|ä¸­å½é¶è|BBTVç¾è§é|ç§å¤§è®¯é£]ç­åä½ä¼ä¼´|å¨çè½¯ä»¶å¼åå¤§èµç­è½¯ä»¶å¼åå¹³å°|éå¸å¨ççç åæºæ|æ¶²æ¶é¢æ¿ç­ä¸æ¸¸èµæºä¿é]",
        "[[HTTP|TCP|UDPï¼SUDP|RUDPï¼|ç½å³ç©¿éæ¨¡ç»|å¨çIPè¡¨]": "[HTTP|TCP|UDP|SUDP|RUDP|ç½å³ç©¿éæ¨¡ç»|UDPç©¿é|RPNPç©¿é|å¨çIPè¡¨]",
        "å¨å¸[ç»æµ|[èµæº|ç¯å¢]åè°åå±": "å¨å¸[ç»æµ|èµæº|ç¯å¢]åè°åå±",
        "[æ±ç¯|æ±é¶]ç»åçä¹¦æ³ååè½¨è¿¹|[æ±ç¯|æ±é¶|å«åä½]çç¾çé­å]": "[[æ±ç¯|æ±é¶]ç»åçä¹¦æ³ååè½¨è¿¹|[æ±ç¯|æ±é¶|å«åä½]çç¾çé­å]",
        "[å¨å¸åæ¿ç¾¤æºå³ç´å±äºä¸åä½|åé¨é¨æå±äºä¸åä½]ãæºæç¼å¶ç®¡çè¯ã|æ°å¢äººåç[æ§å¶|åç¼]ç­æç»­": "[å¨å¸åæ¿ç¾¤æºå³[ç´å±äºä¸åä½|åé¨é¨æå±äºä¸åä½]ãæºæç¼å¶ç®¡çè¯ã|æ°å¢äººåç[æ§å¶|åç¼]æç»­]",
        "[[âç¨³ç²®|å¢çª|å´ç|æ©ç»|ä¿æ|å¼ºå·¥â]æ»ä½æè·¯": "[ç¨³ç²®|å¢çª|å´ç|æ©ç»|ä¿æ|å¼ºå·¥]æè·¯",
        "[[è½»æ¾|æäº®|åç¡®|åæ¶¦]": "[è½»æ¾|æäº®|åç¡®|åæ¶¦]",
        "[ä¿é©ç®±|ç¨æ|ä¹¦æ¡|ç¨è¡£è®¾å¤|ææ°|æ·æµ´|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´ç¼¸ORæ·æµ´|çµè§|çµè¯|æçº¿é¢é|è¿·ä½ å§|å°ç®±]": "[ä¿é©ç®±|ç¨æ|ä¹¦æ¡|ç¨è¡£è®¾å¤|ææ°|æ·æµ´|å¹é£æº|åè´¹æ´æµ´ç¨å|å«çé´|æµ´ç¼¸|æ·æµ´|çµè§|çµè¯|æçº¿é¢é|è¿·ä½ å§|å°ç®±]",
        "[åå°æµæ¼äºé¡¹|æ¿äº§æµæ¼äºé¡¹|è½¦è¾æµæ¼äºé¡¹|è®¾å¤æµæ¼äºé¡¹]": "[åå°|æ¿äº§|è½¦è¾|è®¾å¤]æµæ¼äºé¡¹",
        "[âä¸èµä¼ä¸â|ææ¸¸å¼åé¡¹ç®æä»¶çå®¡æ¥|æ¥æ¹|åè°æå¡]]": "[âä¸èµä¼ä¸â|ææ¸¸å¼åé¡¹ç®æä»¶ç[å®¡æ¥|æ¥æ¹|åè°æå¡]]",
        "ä¸é¨[åå§|ç±æçORå§æç]": "ä¸é¨[åå§|ç±æç|å§æç]",
        "[å£°é³|æ­å±çåå®¹|æ­å±èçé£åº¦ãä»ªè¡¨ãæ°è´¨æ­å±èçé£åº¦ãä»ªè¡¨ãæ°è´¨]": "[å£°é³|æ­å±çåå®¹|æ­å±èç[é£åº¦|ä»ªè¡¨|æ°è´¨]]",
        "[ä¸å¡å¹è®­||çè®ºè°ç |å®£ä¼ |ä¿¡æ¯|åºå±é¢å¤æ¡ææç[æ¶é|æ¥é|å¤åªä½ç¤ºè¯]å·¥ä½": "[ä¸å¡å¹è®­|çè®ºè°ç |å®£ä¼ |ä¿¡æ¯|åºå±é¢å¤æ¡ææç[æ¶é|æ¥é|å¤åªä½ç¤ºè¯]å·¥ä½]",
        "[éåºè½æºç»æä¸­[æ¸æ´è½æº|å¯åçè½æº|æ°è½æº]": "éåºè½æºç»æä¸­[æ¸æ´è½æº|å¯åçè½æº|æ°è½æº]",
        "[æ§è¡STS - 41Cæ¡|å¨å¤ªç©ºä¸­è®°å½äº168ä¸ªå°æ¶ç]ç": "[æ§è¡STS - 41Cæ¡|å¨å¤ªç©ºä¸­è®°å½äº168ä¸ªå°æ¶ç]",
        "[åºæ¬å·ç |ç¹å«å·ç ]|": "[åºæ¬å·ç |ç¹å«å·ç ]",
        "[[è¯å¥½|æ­£ç¡®]ç[åå£°æ¹æ³|åå£°æå·§]": "[[è¯å¥½|æ­£ç¡®]ç[åå£°æ¹æ³|åå£°æå·§]]",
        "ä¸»æºçææç¨æ·ç[æ³¨åå|çå|æåç»å½æ¶é´|ä½¿ç¨shellç±»å]ç­]": "ä¸»æºçææç¨æ·ç[æ³¨åå|çå|æåç»å½æ¶é´|ä½¿ç¨shellç±»å]ç­",
        "[å½å®¶å¤§æ¿æ¹é|[æ¿æ²»|ç»æµ|ç¤¾ä¼çæ´»]]ä¸­çéè¦é®é¢]": "[å½å®¶å¤§æ¿æ¹é|[æ¿æ²»|ç»æµ|ç¤¾ä¼çæ´»]ä¸­çéè¦é®é¢]",
        "[è®¡ç®æºç§å­¦ä¸ææ¯|è½¯ä»¶å·¥ç¨è®¡ç®æºç§å­¦ä¸ææ¯ãè½¯ä»¶å·¥ç¨ãç½ç»å·¥ç¨ãçµå­ä¿¡æ¯å·¥ç¨ãéä¿¡å·¥ç¨ãèªå¨åãä¿¡æ¯ç®¡çä¸ä¿¡æ¯ç³»ç»ç­7ä¸ªæ¬ç§ä¸ä¸|ç½ç»å·¥ç¨|çµå­ä¿¡æ¯å·¥ç¨|éä¿¡å·¥ç¨|èªå¨å|ä¿¡æ¯ç®¡çä¸ä¿¡æ¯ç³»ç»]ç­7ä¸ªæ¬ç§ä¸ä¸": "[è®¡ç®æºç§å­¦ä¸ææ¯|è½¯ä»¶å·¥ç¨|ç½ç»å·¥ç¨|çµå­ä¿¡æ¯å·¥ç¨|éä¿¡å·¥ç¨|èªå¨å|ä¿¡æ¯ç®¡çä¸ä¿¡æ¯ç³»ç»]",
        "[æå±å°æ°æ°æ|åéå°æ°æ°æ]|å°æ°æ°æ[å¦å¥³|å¿ç«¥]ä¿æ¤ç­æå³äºå®": "[æ£æå±å°æ°æ°æ|åéå°æ°æ°æ|å°æ°æ°æ[å¦å¥³|å¿ç«¥]ä¿æ¤]",
        "[ä¸­åæå|æ±æ·®æå|ééµæå|å´æå]": "[ä¸­å|æ±æ·®|ééµ|å´]æå",
        "å¤§é[åè´¹|æ­¦å¨å¼¹è¯]|": "å¤§é[åè´¹|æ­¦å¨å¼¹è¯]",
        "[è°æ´|ç¼å²ORçº¿æ§åå¤ç]": "[è°æ´|ç¼å²|çº¿æ§åå¤ç]",
        "[é¡¹ç®å»ºè®¾ç[ç¨åº|è´¨é|å®å¨|è¿åº¦|èµéä½¿ç¨ï¼ç»ç®ï¼|å³ç®|ç«£å·¥]ç­å¨è¿ç¨": "é¡¹ç®å»ºè®¾ç[ç¨åº|è´¨é|å®å¨|è¿åº¦|èµéä½¿ç¨ï¼ç»ç®ï¼|å³ç®|ç«£å·¥]ç­å¨è¿ç¨",
        "[æ±æ|éå¿äºº|ä¸­å±åå|[ä¸­å½æå§å®¶åä¼ä¼å|æ²³åçæå§å®¶åä¼çäº|æ²³åçèºæ¯åä½ä¸­å¿ç¹çº¦å¯¼æ¼|ä¸é¨å³¡å¸æå§å®¶åä¼ä¸»å¸­|ä¸é¨å³¡å¸æåå±èºæ¯ç§ç§é¿]": "[æ±æ|éå¿äºº|ä¸­å±åå|ä¸­å½æå§å®¶åä¼ä¼å|æ²³åçæå§å®¶åä¼çäº|æ²³åçèºæ¯åä½ä¸­å¿ç¹çº¦å¯¼æ¼|ä¸é¨å³¡å¸æå§å®¶åä¼ä¸»å¸­|ä¸é¨å³¡å¸æåå±èºæ¯ç§ç§é¿]",
        "å¨çæå¤§è§æ¨¡ç[æç´¢å¼æ[è¥é|ä¼å]ä¸ä¸ä¼è®®æç´¢å¼ææç¥å¤§ä¼": "å¨çæå¤§è§æ¨¡çæç´¢å¼æ[è¥é|ä¼å]ä¸ä¸ä¼è®®æç´¢å¼ææç¥å¤§ä¼",
        "[ãé­æ³å°å¥³å¥å¶StrikerS THE COMICSãç¬¬ä¸å·åè¡æ¬æ¼«ç»ï¼æ¥æï¼|ãé­æ³å°å¥³å¥å¶StrikerS THE COMICSãçç¹ä½ä¸­æç]": "ãé­æ³å°å¥³å¥å¶StrikerS THE COMICSã[ç¬¬ä¸å·åè¡æ¬æ¼«ç»ï¼æ¥æï¼|ç¹ä½ä¸­æç]",
        "[æ°æ[å¤ç±|æç©]ç[æ¢æ|æ¶é|æ´ç|åºçè§å]ç­å·¥ä½": "[æ°æ[å¤ç±|æç©]ç[æ¢æ|æ¶é|æ´ç|åºçè§å]ç­å·¥ä½]",
        "[å¤«å¦»ä¹é´ç¸æ¿¡ä»¥æ²«|[ç¶|æ¯]å¥³ä¹é´è¡èç¸è¢­ç[ç¹ç¹æ»´æ»´|ç»å¸ç»é¢]": "[å¤«å¦»ä¹é´ç¸æ¿¡ä»¥æ²«|[ç¶|æ¯]å¥³ä¹é´è¡èç¸è¢­ç[ç¹ç¹æ»´æ»´|ç»å¸ç»é¢]]",
        "[é´å¹³ã[é´ä¸|é´å»|é´å¥|é³å¹³|é³ä¸|é³å»|é³å¥]": "[é´å¹³|é´ä¸|é´å»|é´å¥|é³å¹³|é³ä¸|é³å»|é³å¥]",
        "[The Patriotic Front | PF]": "[The Patriotic Front|PF]",
        "[åå¬åºæ|ç»è¥åºå°]ç[äº§æè¯æ|ç§æ1å¹´ä»¥ä¸çç§èµåå|": "[åå¬åºæ|ç»è¥åºå°]ç[äº§æè¯æ|ç§æ1å¹´ä»¥ä¸çç§èµåå|åæ³çéªèµè¯æ]",
        "å¨è¾¹[6ä¸ªå¿ï¼å¸ï¼|55ä¸ªä¹¡é|600å¤ä¸ªå¤©ç¶æ]": "å¨è¾¹[6ä¸ªå¿ï¼å¸ï¼|55ä¸ªä¹¡é|500å¤ä¸ªå¤©ç¶æ]",
        "é£ç©ç[åæ°|äºå³|å½ç»|é´é³]å±æ§ç­|äººä½çççå¯åç¸å³ç[çè®º|ç»éª]]": "[é£ç©ç[åæ°|äºå³|å½ç»|é´é³]å±æ§|äººä½çççå¯åç¸å³ç[çè®º|ç»éª]]",
        "å[å·¥å§|ç®¡å§ä¼]ç[å°ç« ç®¡ç|æºè¦ä¿å¯]å·¥ä½]": "å[å·¥å§|ç®¡å§ä¼]ç[å°ç« ç®¡ç|æºè¦ä¿å¯]å·¥ä½",
        "[è¿åæ¼èºç|æåå¤«ç|è·å¾æ´å¤§çä¸ªäººåå±ï¼æä¸ºææçè³åå¤«å·¨æ|è·å¾æ´å¤§çä¸ªäººåå±|æä¸ºææçè³åå¤«å·¨æ]": "[è¿åæ¼èºç|æåå¤«ç|è·å¾æ´å¤§çä¸ªäººåå±|æä¸ºææçè³åå¤«å·¨æ]",
        "[ç±è®¾åºçå¸æ¿åºè¡ä½¿çç»æµç¤¾ä¼ç®¡çæé|[çæ¿åº|çæ¿åºé¨é¨]ä¸æ¾ç»[çè¾å¸æ¿åº|çè¾å¸æ¿åºé¨é¨]çç»æµç¤¾ä¼ç®¡çæé": "[ç±è®¾åºçå¸æ¿åºè¡ä½¿çç»æµç¤¾ä¼ç®¡çæé|[çæ¿åº|çæ¿åºé¨é¨]ä¸æ¾ç»[çè¾å¸æ¿åº|çè¾å¸æ¿åºé¨é¨]çç»æµç¤¾ä¼ç®¡çæé]",
        "[ãç¬¬äºé¡¹ä¿®ç¼ãççè®º|ãç¬¬äºé¡¹ä¿®ç¼ãçå¯æä½æ§]": "ãç¬¬äºé¡¹ä¿®ç¼ãç[çè®º|å¯æä½æ§]",
        "[æºå³|äºä¸åä½äººå|ä¼ä¸ç®¡çäººå|ä¸ä¸ææ¯äººå]ç»è®¡|[æºå³|äºä¸]åä½å·¥èµç»è®¡|äººäºä¿¡æ¯ç®¡ç]å·¥ä½": "[[æºå³|äºä¸åä½äººå|ä¼ä¸ç®¡çäººå|ä¸ä¸ææ¯äººå]ç»è®¡|[æºå³|äºä¸]åä½å·¥èµç»è®¡|äººäºä¿¡æ¯ç®¡çå·¥ä½]",
    }
    place_fix_map = {
        "å¨èå·|[å·¥ä¸å­åº|é«æ°åº]": "å¨èå·[å·¥ä¸å­åº|é«æ°åº]",
    }
    text_fix_map = {
        "ç§æè¾å¯¼åæ´»å¨è¾å¯¼ç»ç»å¹è®­ï¼ä½¿åæ ¡æå¸åçéæµã": "ç§æè¾å¯¼ååæ´»å¨è¾å¯¼ç»ç»å¹è®­ï¼ä½¿åæ ¡æå¸åçéæµã",
    }
    qua_fix_map = {
        "[å¨æ ¡é¢å¯¼çé«åº¦éè§|å¨ä½å¸ççå±ååªå]]": "å¨[æ ¡é¢å¯¼çé«åº¦éè§|å¨ä½å¸ççå±ååªå]ä¸",
        "å¨[é¶ä¸40åº¦é«æ¸©|è¶ä½æ¸©ç¯å¢]ä¸­|å¨[å¹²ç¥|æ½®æ¹¿|é£å°]ç­åä¸ªç¯å¢ä¸­": "[å¨[é¶ä¸40åº¦é«æ¸©|è¶ä½æ¸©ç¯å¢]ä¸­|å¨[å¹²ç¥|æ½®æ¹¿|é£å°]ç­åä¸ªç¯å¢ä¸­]",
        "ä»ç¾å½ç[æ°ä¸»|å®ªæ³]|ç¾å½ç¤¾ä¼çé®é¢|ç¾å½çç§»æ°åå²|ç¾å½äººççæ´»ä¹ æ¯]": "ä»[ç¾å½ç[æ°ä¸»|å®ªæ³]|ç¾å½ç¤¾ä¼çé®é¢|ç¾å½çç§»æ°åå²|ç¾å½äººççæ´»ä¹ æ¯]",
        "ä»¥[æå­|å½©å¾]]": "ä»¥[æå­|å½©å¾]",
    }
    for sample in data:
        text = sample["natural"]
        if text in text_fix_map:
            sample["natural"] = text_fix_map[text]
        for spo in sample["logic"]:
            # fix subject
            if spo["subject"] in subj_fix_map:
                spo["subject"] = subj_fix_map[spo["subject"]]
            # fix predicate
            spo["predicate"] = spo["predicate"].strip()
            spo["predicate"] = re.sub("OR", "|", spo["predicate"])
            if spo["predicate"] in pred_fix_map:
                spo["predicate"] = pred_fix_map[spo["predicate"]]

            # fix object
            new_objs = []
            for obj in spo["object"]:
                if obj in obj_fix_map:
                    new_objs.append(obj_fix_map[obj])
                else:
                    new_objs.append(obj)
            spo["object"] = new_objs

            # fix place
            if spo["place"] in place_fix_map:
                spo["place"] = place_fix_map[spo["place"]]
            # fix qualifier
            if spo["qualifier"] in qua_fix_map:
                spo["qualifier"] = qua_fix_map[spo["qualifier"]]

            # fix by text
            if text == "3ï¼æ­å±ä¸­ç¹å«å¼ºè°æ°æ¯çæ§å¶ï¼å¼ºè°ï¼è¿è´¯æ§ï¼åé³è²çä¼ç¾ï¼è¦æ±æ­å±ä¸­è¯­æ°å¯äºååï¼ææè¡¨è¾¾çæã" and \
                    spo["predicate"] == "è¦æ±X":
                spo["object"] = ["[è¯­æ°å¯äºåå|ææè¡¨è¾¾çæ]"]
            if text == "åç¦»åºç¨å±åé¢åå±æå©äºå¯¹é¢åæ¨¡åçæ½è±¡åä¸æ­ç²¾åï¼ä¹æå©å®æ½äººåå¿«éæå»ºæè°æ´äº§åï¼ä»¥æ»¡è¶³ä¼ä¸åå±ååçç®¡çéè¦ã" and \
                    spo["predicate"] == "æå©Xå¿«é[æå»º|è°æ´]Y":
                spo["object"] = ["å®æ½äººå", "äº§å"]
            if text == "æ±æ­¦å¸ç»æä¹ä¸å¹´ï¼å³å¬åå138å¹´ï¼âæ²³æ°´æº¢äºå¹³åï¼å¤§é¥¥ï¼äººç¸é£âçäºå®ï¼å·²åºç°äºå®æ¹çºªå½ã" and \
                    spo["predicate"] == "=å¬åå138å¹´":
                spo["predicate"] = "="
                spo["object"] = ["å¬åå138å¹´"]
            if text == "åå§å§åãå¯éé¿è°­æå°ï¼" and spo["subject"] == "è°­æå°" and spo["object"][0] == "[åå§å§å|å¯éé¿]":
                spo["predicate"] = "ISA"
            if text == "'èå¯¹äºåç¦æå¹³çäººæ¥è¯´ï¼æ´éè¦åçèèªæ¸¸ç§»çåçï¼å°èä¸ãåèãå°è¹çèèªéä¸­åè£¹å¨è¸é¨ï¼å°å¤§è¿æ ¹é¨ãå¤ä¾§çèèªä¸æåºå®å¨èé¨ï¼åé åºç²çæè´çèº«æã" and \
                    spo["predicate"] == "å°Xéä¸­Y":
                spo["predicate"] = "å°Xéä¸­åè£¹Y"
                spo["object"] = ['[èä¸|åè|å°è¹]çèèª', 'å¨è¸é¨']

    def parse_spe_txt2list(spe_txt, jt=""):
        sep = "\u2E82"
        star = spe_txt.find("[")
        end = -1
        if star != -1:
            stack = []
            for idx in range(star, len(spe_txt)):
                c = spe_txt[idx]
                if c == "[":
                    stack.append(c)
                elif c == "]":
                    stack.pop()
                    if len(stack) == 0:
                        end = idx
                        break

        res = []
        if star != -1 and end != -1:
            pre = spe_txt[:star]
            mid = spe_txt[star + 1:end]
            post = spe_txt[end + 1:]

            mid_sub = mid[:]
            stack = []
            for idx, c in enumerate(mid):
                if c == "[":
                    stack.append(c)
                elif c == "]":
                    stack.pop()
                elif c == "|" and len(stack) == 0:
                    mid_sub = mid_sub[:idx] + sep + mid_sub[idx + 1:]

            mid_segs = mid_sub.split(sep)
            tmp = [jt.join([pre, seg, post]) for seg in mid_segs]
            for txt in tmp:
                res.extend(parse_spe_txt2list(txt))
        else:
            res.append(spe_txt)
        return res

    def get_spe_txt_spans(spe_txt, text, is_pred=False):
        target_str = re.sub("[\]\[\|]", "", spe_txt)
        if is_pred:
            target_str = re.sub("([^a-zA-Z]|^)[XYZU]([^a-zA-Z]|$)", r"\1\2", target_str)

        spans, _ = Preprocessor.search_char_spans_fr_txt(target_str, text, "ch")
        spans = [(spans[i], spans[i + 1]) for i in range(0, len(spans), 2)]

        preid2c = {}
        pat = "[\]\[\|XYZU]+" if is_pred else "[\]\[\|]+"
        for m in re.finditer(pat, spe_txt):
            if is_pred:
                if spe_txt[m.span()[0]] in set("XYZU") and m.span()[0] - 1 >= 0 and (
                        0 <= ord(spe_txt[m.span()[0] - 1]) - ord("A") <= 25 or 0 <= ord(spe_txt[m.span()[0] - 1]) - ord(
                    "a") <= 25) or \
                        spe_txt[m.span()[1] - 1] in set("XYZU") and m.span()[1] < len(spe_txt) and (
                        0 <= ord(spe_txt[m.span()[1]]) - ord("A") <= 25 or 0 <= ord(spe_txt[m.span()[1]]) - ord(
                    "a") <= 25):
                    continue
            preid2c[m.span()[0] - 1] = m.group()

        start = re.match("[\]\[\|XYZU]+", spe_txt) if is_pred else re.match("[\]\[\|]+", spe_txt)
        spans_str = start.group() if start is not None else ""
        offset = len(spans_str)

        for sp in spans:
            for sp_idx in range(*sp):
                spans_str += "({}, {})".format(sp_idx, sp_idx + 1)
                offset += 1
                if offset - 1 in preid2c:
                    spans_str += preid2c[offset - 1]
                    offset += len(preid2c[offset - 1])

        spans_str_list = []
        for sps_str in parse_spe_txt2list(spans_str):
            sps = [int(s) for s in re.findall("\d+", sps_str)]
            sps = merge_spans(sps)
            spans_str_list.append(sps)
        return spans_str_list

    predefined_p = dict()

    # trans predicate
    for sample in data:
        text = sample["natural"]
        for spo in sample["logic"]:
            predicate = spo["predicate"]
            # re.match("[A-Z=]+$", predicate) and predicate not in text
            if predicate != "_" and re.match("[A-Z=]+$", predicate) and predicate not in text:
                predefined_p[predicate] = predefined_p.get(predicate, 0) + 1

    predefined_p_map = {"DESC": "æè¿°",
                        "ISA": "æ¯ä¸ç§",
                        "IN": "ä½äº",
                        "BIRTH": "çäº",
                        "DEATH": "æ­»äº",
                        "=": "ç­äº",
                        "NOT": "ä¸",
                        }

    new_data = []
    bad_spo_list = []
    for sample_id, sample in tqdm(enumerate(data), desc="transform"):
        ori_sample = copy.deepcopy(sample)
        sample["natural"] = sample["natural"] + "[SEP]" + "ï¼".join(predefined_p_map.values())
        text = sample["natural"]

        new_spo_list = []
        for spo in sample["logic"]:
            # trans predicate
            for k in predefined_p_map:
                if k in spo["predicate"]:  # and spo["predicate"] not in text
                    new_predicate = re.sub(k, predefined_p_map[k], spo["predicate"])
                    # print("{} -> {}".format(spo["predicate"], new_predicate))
                    spo["predicate"] = new_predicate

            ori_spo = copy.deepcopy(spo)
            split = False
            for key in spo:
                if spo[key] == "_":
                    spo[key] = []
                elif key != "object" and key != "objects":
                    if re.search(".*\[.*\|.*\].*", spo[key]):  # need to split
                        ori_str = spo[key]
                        split = True
                        split_list = parse_spe_txt2list(ori_str)
                        span_list = get_spe_txt_spans(ori_str, text, is_pred=True if key == "predicate" else False)

                        # check spans
                        for idx, sp in enumerate(span_list):
                            extr_txt = Preprocessor.extract_ent_fr_txt_by_char_sp(sp, text, "ch")
                            try:
                                cor_str = re.sub("([^a-zA-Z]|^)[XYZU]([^a-zA-Z]|$)", r"\1\2", split_list[idx]) \
                                    if key == "predicate" else split_list[idx]
                                assert extr_txt == cor_str
                            except Exception:
                                print(text)
                                print(key)
                                print(ori_str)
                                print(extr_txt)
                                print(cor_str)
                                print("==================span search error==================")
                        comb_list = [{"char_span": sp, "text": split_list[idx]} for idx, sp in enumerate(span_list)]
                        spo[key] = comb_list
                    else:
                        char_sp, _ = Preprocessor.search_char_spans_fr_txt(spo[key], text, "ch")
                        spo[key] = [
                            {"text": spo[key],
                             "char_span": char_sp,
                             }, ]
                elif key == "object":
                    new_objs = []
                    for obj in spo[key]:
                        if re.search(".*\[.*\|.*\].*", obj):  # need to split
                            ori_str = obj
                            split_list = parse_spe_txt2list(obj)
                            span_list = get_spe_txt_spans(ori_str, text, is_pred=False)

                            # check spans
                            for idx, sp in enumerate(span_list):
                                extr_txt = Preprocessor.extract_ent_fr_txt_by_char_sp(sp, text, "ch")
                                try:
                                    cor_str = split_list[idx]
                                    assert extr_txt == cor_str
                                except Exception:
                                    print(text)
                                    print(key)
                                    print(ori_str)
                                    print(extr_txt)
                                    print(cor_str)
                                    print("==================span search error==================")
                            comb_list = [{"char_span": sp, "text": split_list[idx]} for idx, sp in enumerate(span_list)]
                            new_objs.append(comb_list)
                            split = True
                            # print(text)
                            # print(spo)
                            # print(ori_str)
                            # print(split_list)
                            # print("============split {}=============".format(key))
                        else:
                            if obj == "_":
                                pass
                            else:
                                char_sp, _ = Preprocessor.search_char_spans_fr_txt(obj, text, "ch")
                                new_objs.append([
                                    {"text": obj,
                                     "char_span": char_sp,
                                     }, ])
                    spo[key] = new_objs

            for p in spo["predicate"]:
                if re.search("[XYZU]", p["text"]) is None and len(spo["object"]) > 0:
                    p["text"] += "X"

            # align predicate and the corresponding subset of objects (by XYZU)
            ext_spo_list = []
            id_map = {"X": 0, "Y": 1, "Z": 2, "U": 3}

            bad_spo = False
            for p in spo["predicate"]:
                sub_objs = []
                for ph, idx in id_map.items():
                    if ph in p["text"]:
                        if ph == "U" and "UNIX" in p["text"] or \
                                ph == "U" and "MOTOBLUR" in p["text"] or \
                                ph == "Y" and "DKNY" in p["text"] or \
                                ph == "Y" and "LYF" in p["text"] or \
                                ph == "Z" and "SK-1Z02D" in p["text"]:
                            continue
                        try:
                            sub_objs.append(spo["object"][idx])
                        except Exception:
                            bad_spo = True
                            bad_spo_list.append({
                                "text": text,
                                "bad_spo": ori_spo,
                                "ori_sample": ori_sample,
                            })
                if bad_spo:
                    break
                p["text"] = re.sub("[XYZU]", "[OBJ]", p["text"])
                new_spo = copy.deepcopy(spo)
                new_spo["predicate"] = [p, ]
                new_spo["object"] = sub_objs
                ext_spo_list.append(new_spo)
            if len(spo["predicate"]) == 0:
                assert len(spo["object"]) <= 1
                ext_spo_list.append(spo)

            # product
            open_spo_list = []
            for new_spo in ext_spo_list:
                lists4prod = []
                for k, l in new_spo.items():
                    if k in {"object", "objects"} or len(l) == 0:
                        continue
                    # try:
                    lists4prod.append([{"type": k, **i} for i in l])
                    # except Exception:
                    #     # print("!")
                for objs in new_spo["object"]:
                    new_objs = []
                    for i in objs:
                        # try:
                        new_objs.append({"type": "object", **i})
                        # except Exception:
                        #     # print("!")
                    lists4prod.append(new_objs)

                open_spo_list.extend([list(item) for item in itertools.product(*lists4prod)])

            new_spo_list.extend(open_spo_list)
        new_sample = {
            "id": sample_id,
            "text": text,
            "open_spo_list": new_spo_list,
        }
        new_data.append(new_sample)
    return new_data, predefined_p, bad_spo_list

def trans_saoke():
    new_data, predefined_pred_set, bad_spo_list = preprocess_saoke()

    for sample in new_data:
        span_list = []
        for spo in sample["open_spo_list"]:
            for arg in spo:
                span_list.append(arg["char_span"])
        tok_res = ChineseWordTokenizer.tokenize_plus(sample["text"], span_list=span_list)
        sample["word_list"] = tok_res["word_list"]
        sample["word2char_span"] = tok_res["word2char_span"]

    train_data_rate = 0.8
    val_data_rate = 0.1
    train_num = int(len(new_data) * 0.8)
    valid_num = int(len(new_data) * 0.1)
    test_num = len(new_data) - train_num - valid_num
    random.shuffle(new_data)
    train_data = new_data[:train_num]
    valid_data = new_data[train_num:train_num + valid_num]
    test_data = new_data[-test_num:]

    save_as_json_lines(train_data, "../../data/ori_data/saoke/train_data.json")
    save_as_json_lines(valid_data, "../../data/ori_data/saoke/valid_data.json")
    save_as_json_lines(test_data, "../../data/ori_data/saoke/test_data.json")

if __name__ == "__main__":
    pass
